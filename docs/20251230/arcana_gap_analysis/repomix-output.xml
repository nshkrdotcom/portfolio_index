This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where content has been compressed (code blocks are separated by ⋮---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
prompts/
  01_query_processing_pipeline.md
  02_document_management_schemas.md
  03_production_maintenance.md
  04_evaluation_system.md
  05_embedder_enhancements.md
  06_vector_store_enhancements.md
  07_telemetry_standardization.md
  08_collection_selector_self_correction.md
  run_prompts.sh
00_consolidated_summary.md
01_agent_system.md
02_embedder_system.md
03_vector_store.md
04_evaluation_system.md
05_llm_integration.md
06_telemetry.md
07_mix_tasks.md
08_maintenance_and_documents.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="prompts/01_query_processing_pipeline.md">
# Prompt 1: Query Processing Pipeline Implementation

## Target Repositories
- **portfolio_core**: `/home/home/p/g/n/portfolio_core`
- **portfolio_index**: `/home/home/p/g/n/portfolio_index`

## Required Reading Before Implementation

### Reference Implementation (Arcana)
Read these files to understand the reference implementation:
```
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/context.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/rewriter.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/rewriter/llm.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/expander.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/expander/llm.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/decomposer.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/decomposer/llm.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/rewriters.ex
```

### Existing Portfolio Code
Read these files to understand existing patterns:
```
/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/llm.ex
/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/reranker.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/llm/anthropic.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/reranker/llm.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/strategy.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/strategies/agentic.ex
/home/home/p/g/n/portfolio_manager/lib/portfolio_manager/generation.ex
```

### Gap Analysis Documentation
```
/home/home/p/g/n/portfolio_index/docs/20251230/arcana_gap_analysis/01_agent_system.md
/home/home/p/g/n/portfolio_index/docs/20251230/arcana_gap_analysis/05_llm_integration.md
```

---

## Implementation Tasks

### Task 1: Pipeline Context Struct (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/pipeline/context.ex`:

```elixir
defmodule PortfolioIndex.RAG.Pipeline.Context do
  @moduledoc """
  Context struct that flows through the RAG pipeline, tracking all intermediate results.
  Enables functional composition with the pipe operator.
  """

  @type t :: %__MODULE__{
    # Input
    question: String.t() | nil,
    opts: keyword(),

    # Query Processing
    rewritten_query: String.t() | nil,
    expanded_query: String.t() | nil,
    sub_questions: [String.t()],

    # Routing
    selected_indexes: [String.t()],
    selection_reasoning: String.t() | nil,

    # Retrieval
    results: [map()],
    rerank_scores: %{String.t() => float()},

    # Generation
    answer: String.t() | nil,
    context_used: [map()],

    # Self-Correction
    correction_count: non_neg_integer(),
    corrections: [{String.t(), String.t()}],

    # Error Handling
    error: term() | nil,
    halted?: boolean()
  }

  defstruct [
    question: nil,
    opts: [],
    rewritten_query: nil,
    expanded_query: nil,
    sub_questions: [],
    selected_indexes: [],
    selection_reasoning: nil,
    results: [],
    rerank_scores: %{},
    answer: nil,
    context_used: [],
    correction_count: 0,
    corrections: [],
    error: nil,
    halted?: false
  ]

  @doc "Create a new context with the given question and options"
  @spec new(String.t(), keyword()) :: t()
  def new(question, opts \\ [])

  @doc "Mark context as halted with an error"
  @spec halt(t(), term()) :: t()
  def halt(ctx, error)

  @doc "Check if context has an error"
  @spec error?(t()) :: boolean()
  def error?(ctx)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/rag/pipeline/context_test.exs`

---

### Task 2: Query Rewriter Port (portfolio_core)

Create `/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/query_rewriter.ex`:

```elixir
defmodule PortfolioCore.Ports.QueryRewriter do
  @moduledoc """
  Behaviour for query rewriting - transforming conversational input into clean search queries.
  """

  @type rewrite_result :: %{
    original: String.t(),
    rewritten: String.t(),
    changes_made: [String.t()]
  }

  @callback rewrite(query :: String.t(), opts :: keyword()) ::
    {:ok, rewrite_result()} | {:error, term()}

  @optional_callbacks []
end
```

**Test file**: `/home/home/p/g/n/portfolio_core/test/ports/query_rewriter_test.exs`

---

### Task 3: Query Rewriter LLM Adapter (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/query_rewriter/llm.ex`:

```elixir
defmodule PortfolioIndex.Adapters.QueryRewriter.LLM do
  @moduledoc """
  LLM-based query rewriter that cleans conversational input.
  Removes greetings, filler words, and extracts the core question.
  """

  @behaviour PortfolioCore.Ports.QueryRewriter

  @default_prompt """
  Transform the following user query into a clean search query.
  Remove conversational elements like greetings, filler phrases, and politeness markers.
  Keep technical terms and the core question intact.
  Return ONLY the cleaned query, nothing else.

  User query: {query}
  """

  @impl true
  def rewrite(query, opts \\ [])
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/adapters/query_rewriter/llm_test.exs`

---

### Task 4: Query Expander Port (portfolio_core)

Create `/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/query_expander.ex`:

```elixir
defmodule PortfolioCore.Ports.QueryExpander do
  @moduledoc """
  Behaviour for query expansion - adding synonyms and related terms.
  """

  @type expansion_result :: %{
    original: String.t(),
    expanded: String.t(),
    added_terms: [String.t()]
  }

  @callback expand(query :: String.t(), opts :: keyword()) ::
    {:ok, expansion_result()} | {:error, term()}
end
```

**Test file**: `/home/home/p/g/n/portfolio_core/test/ports/query_expander_test.exs`

---

### Task 5: Query Expander LLM Adapter (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/query_expander/llm.ex`:

```elixir
defmodule PortfolioIndex.Adapters.QueryExpander.LLM do
  @moduledoc """
  LLM-based query expander that adds synonyms and related terms.
  Improves recall by including alternative phrasings.
  """

  @behaviour PortfolioCore.Ports.QueryExpander

  @default_prompt """
  Expand the following search query with synonyms and related terms.
  Add abbreviation expansions, alternative phrasings, and technical equivalents.
  Return the original query PLUS the additional terms, space-separated.

  Query: {query}
  """

  @impl true
  def expand(query, opts \\ [])
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/adapters/query_expander/llm_test.exs`

---

### Task 6: Query Decomposer Port (portfolio_core)

Create `/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/query_decomposer.ex`:

```elixir
defmodule PortfolioCore.Ports.QueryDecomposer do
  @moduledoc """
  Behaviour for query decomposition - breaking complex questions into sub-questions.
  """

  @type decomposition_result :: %{
    original: String.t(),
    sub_questions: [String.t()],
    is_complex: boolean()
  }

  @callback decompose(query :: String.t(), opts :: keyword()) ::
    {:ok, decomposition_result()} | {:error, term()}
end
```

**Test file**: `/home/home/p/g/n/portfolio_core/test/ports/query_decomposer_test.exs`

---

### Task 7: Query Decomposer LLM Adapter (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/query_decomposer/llm.ex`:

```elixir
defmodule PortfolioIndex.Adapters.QueryDecomposer.LLM do
  @moduledoc """
  LLM-based query decomposer that breaks complex questions into simpler sub-questions.
  Returns JSON with sub_questions array.
  """

  @behaviour PortfolioCore.Ports.QueryDecomposer

  @default_prompt """
  Analyze the following question. If it is complex and contains multiple parts,
  break it into 2-4 simpler sub-questions that can be answered independently.
  If it is already simple, return it as the only sub-question.

  Return JSON format: {"sub_questions": ["q1", "q2", ...]}

  Question: {query}
  """

  @impl true
  def decompose(query, opts \\ [])
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/adapters/query_decomposer/llm_test.exs`

---

### Task 8: Query Processor Module (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/query_processor.ex`:

```elixir
defmodule PortfolioIndex.RAG.QueryProcessor do
  @moduledoc """
  Unified query processing module that combines rewriting, expansion, and decomposition.
  """

  alias PortfolioIndex.RAG.Pipeline.Context

  @doc "Apply query rewriting to context"
  @spec rewrite(Context.t(), keyword()) :: Context.t()
  def rewrite(ctx, opts \\ [])

  @doc "Apply query expansion to context"
  @spec expand(Context.t(), keyword()) :: Context.t()
  def expand(ctx, opts \\ [])

  @doc "Apply query decomposition to context"
  @spec decompose(Context.t(), keyword()) :: Context.t()
  def decompose(ctx, opts \\ [])

  @doc "Apply all query processing steps in sequence"
  @spec process(Context.t(), keyword()) :: Context.t()
  def process(ctx, opts \\ [])
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/rag/query_processor_test.exs`

---

## TDD Requirements

For each task:

1. **Write tests FIRST** following existing test patterns in the repos
2. Tests must cover:
   - Happy path with valid input
   - Error handling (invalid input, LLM failures)
   - Edge cases (empty query, very long query)
   - Context propagation (for pipeline tests)
3. Run tests continuously: `mix test path/to/test_file.exs`

## Quality Gates

Before considering this prompt complete:

```bash
# In portfolio_core
cd /home/home/p/g/n/portfolio_core
mix test
mix credo --strict
mix dialyzer

# In portfolio_index
cd /home/home/p/g/n/portfolio_index
mix test
mix credo --strict
mix dialyzer
```

All must pass with zero warnings and zero errors.

## Documentation Updates

### portfolio_core
Update `/home/home/p/g/n/portfolio_core/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `PortfolioCore.Ports.QueryRewriter` behaviour for query cleaning
- `PortfolioCore.Ports.QueryExpander` behaviour for query expansion
- `PortfolioCore.Ports.QueryDecomposer` behaviour for multi-hop queries
```

### portfolio_index
Update `/home/home/p/g/n/portfolio_index/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `PortfolioIndex.RAG.Pipeline.Context` struct for pipeline state tracking
- `PortfolioIndex.Adapters.QueryRewriter.LLM` - LLM-based query cleaning
- `PortfolioIndex.Adapters.QueryExpander.LLM` - LLM-based query expansion
- `PortfolioIndex.Adapters.QueryDecomposer.LLM` - LLM-based query decomposition
- `PortfolioIndex.RAG.QueryProcessor` - unified query processing module
```

## Verification Checklist

- [ ] All new files created in correct locations
- [ ] All tests pass
- [ ] No credo warnings
- [ ] No dialyzer errors
- [ ] Changelogs updated
- [ ] Module documentation complete with @moduledoc and @doc
- [ ] Type specifications complete with @type, @spec, @callback
</file>

<file path="prompts/02_document_management_schemas.md">
# Prompt 2: Document Management Schemas Implementation

## Target Repository
- **portfolio_index**: `/home/home/p/g/n/portfolio_index`

## Required Reading Before Implementation

### Reference Implementation (Arcana)
Read these files to understand the reference implementation:
```
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/document.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/chunk.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/collection.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/schemas/document.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/schemas/chunk.ex
/home/home/p/g/n/portfolio_index/arcana/priv/repo/migrations/*_create_arcana_*.exs
```

### Existing Portfolio Code
Read these files to understand existing patterns:
```
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/vector_store/pgvector.ex
/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/vector_store.ex
/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/embedder.ex
```

### Gap Analysis Documentation
```
/home/home/p/g/n/portfolio_index/docs/20251230/arcana_gap_analysis/08_maintenance_and_documents.md
/home/home/p/g/n/portfolio_index/docs/20251230/arcana_gap_analysis/03_vector_store.md
```

---

## Implementation Tasks

### Task 1: Collection Ecto Schema

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/schemas/collection.ex`:

```elixir
defmodule PortfolioIndex.Schemas.Collection do
  @moduledoc """
  Ecto schema for document collections.
  Collections group related documents for organized retrieval and routing.
  """

  use Ecto.Schema
  import Ecto.Changeset

  @type t :: %__MODULE__{
    id: Ecto.UUID.t() | nil,
    name: String.t(),
    description: String.t() | nil,
    metadata: map(),
    document_count: non_neg_integer(),
    inserted_at: DateTime.t() | nil,
    updated_at: DateTime.t() | nil
  }

  @primary_key {:id, :binary_id, autogenerate: true}
  @foreign_key_type :binary_id

  schema "portfolio_collections" do
    field :name, :string
    field :description, :string
    field :metadata, :map, default: %{}
    field :document_count, :integer, default: 0, virtual: true

    has_many :documents, PortfolioIndex.Schemas.Document

    timestamps(type: :utc_datetime)
  end

  @doc "Changeset for creating/updating collections"
  @spec changeset(t(), map()) :: Ecto.Changeset.t()
  def changeset(collection, attrs)

  @doc "Validate name uniqueness"
  @spec validate_unique_name(Ecto.Changeset.t()) :: Ecto.Changeset.t()
  def validate_unique_name(changeset)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/schemas/collection_test.exs`

---

### Task 2: Document Ecto Schema

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/schemas/document.ex`:

```elixir
defmodule PortfolioIndex.Schemas.Document do
  @moduledoc """
  Ecto schema for ingested documents.
  Tracks document metadata, status, and relationship to chunks.
  """

  use Ecto.Schema
  import Ecto.Changeset

  @type status :: :pending | :processing | :completed | :failed | :deleted

  @type t :: %__MODULE__{
    id: Ecto.UUID.t() | nil,
    source_id: String.t() | nil,
    content_hash: String.t() | nil,
    title: String.t() | nil,
    source_path: String.t() | nil,
    metadata: map(),
    status: status(),
    error_message: String.t() | nil,
    chunk_count: non_neg_integer(),
    collection_id: Ecto.UUID.t() | nil,
    inserted_at: DateTime.t() | nil,
    updated_at: DateTime.t() | nil
  }

  @primary_key {:id, :binary_id, autogenerate: true}
  @foreign_key_type :binary_id

  schema "portfolio_documents" do
    field :source_id, :string
    field :content_hash, :string
    field :title, :string
    field :source_path, :string
    field :metadata, :map, default: %{}
    field :status, Ecto.Enum, values: [:pending, :processing, :completed, :failed, :deleted], default: :pending
    field :error_message, :string
    field :chunk_count, :integer, default: 0

    belongs_to :collection, PortfolioIndex.Schemas.Collection
    has_many :chunks, PortfolioIndex.Schemas.Chunk

    timestamps(type: :utc_datetime)
  end

  @doc "Changeset for creating documents"
  @spec changeset(t(), map()) :: Ecto.Changeset.t()
  def changeset(document, attrs)

  @doc "Changeset for updating document status"
  @spec status_changeset(t(), status(), String.t() | nil) :: Ecto.Changeset.t()
  def status_changeset(document, status, error_message \\ nil)

  @doc "Compute content hash for deduplication"
  @spec compute_hash(String.t()) :: String.t()
  def compute_hash(content)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/schemas/document_test.exs`

---

### Task 3: Chunk Ecto Schema with pgvector

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/schemas/chunk.ex`:

```elixir
defmodule PortfolioIndex.Schemas.Chunk do
  @moduledoc """
  Ecto schema for document chunks with vector embeddings.
  Supports pgvector for similarity search.
  """

  use Ecto.Schema
  import Ecto.Changeset

  @type t :: %__MODULE__{
    id: Ecto.UUID.t() | nil,
    content: String.t(),
    embedding: Pgvector.Ecto.Vector.t() | nil,
    chunk_index: non_neg_integer(),
    token_count: non_neg_integer() | nil,
    start_char: non_neg_integer() | nil,
    end_char: non_neg_integer() | nil,
    metadata: map(),
    document_id: Ecto.UUID.t() | nil,
    inserted_at: DateTime.t() | nil,
    updated_at: DateTime.t() | nil
  }

  @primary_key {:id, :binary_id, autogenerate: true}
  @foreign_key_type :binary_id

  schema "portfolio_chunks" do
    field :content, :string
    field :embedding, Pgvector.Ecto.Vector
    field :chunk_index, :integer
    field :token_count, :integer
    field :start_char, :integer
    field :end_char, :integer
    field :metadata, :map, default: %{}

    belongs_to :document, PortfolioIndex.Schemas.Document

    timestamps(type: :utc_datetime)
  end

  @doc "Changeset for creating chunks"
  @spec changeset(t(), map()) :: Ecto.Changeset.t()
  def changeset(chunk, attrs)

  @doc "Changeset for updating embedding"
  @spec embedding_changeset(t(), [float()]) :: Ecto.Changeset.t()
  def embedding_changeset(chunk, embedding)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/schemas/chunk_test.exs`

---

### Task 4: Database Migrations

Create migration file `/home/home/p/g/n/portfolio_index/priv/repo/migrations/YYYYMMDDHHMMSS_create_portfolio_document_schemas.exs`:

```elixir
defmodule PortfolioIndex.Repo.Migrations.CreatePortfolioDocumentSchemas do
  use Ecto.Migration

  def up do
    # Enable pgvector extension
    execute "CREATE EXTENSION IF NOT EXISTS vector"

    # Collections table
    create table(:portfolio_collections, primary_key: false) do
      add :id, :binary_id, primary_key: true
      add :name, :string, null: false
      add :description, :text
      add :metadata, :map, default: %{}

      timestamps(type: :utc_datetime)
    end

    create unique_index(:portfolio_collections, [:name])

    # Documents table
    create table(:portfolio_documents, primary_key: false) do
      add :id, :binary_id, primary_key: true
      add :source_id, :string
      add :content_hash, :string
      add :title, :string
      add :source_path, :string
      add :metadata, :map, default: %{}
      add :status, :string, default: "pending"
      add :error_message, :text
      add :chunk_count, :integer, default: 0
      add :collection_id, references(:portfolio_collections, type: :binary_id, on_delete: :nilify_all)

      timestamps(type: :utc_datetime)
    end

    create index(:portfolio_documents, [:collection_id])
    create index(:portfolio_documents, [:source_id])
    create index(:portfolio_documents, [:status])
    create index(:portfolio_documents, [:content_hash])

    # Chunks table with vector column
    # Note: Vector dimension should be configurable, default 384 for bge-small
    create table(:portfolio_chunks, primary_key: false) do
      add :id, :binary_id, primary_key: true
      add :content, :text, null: false
      add :embedding, :vector, size: 384
      add :chunk_index, :integer, null: false
      add :token_count, :integer
      add :start_char, :integer
      add :end_char, :integer
      add :metadata, :map, default: %{}
      add :document_id, references(:portfolio_documents, type: :binary_id, on_delete: :delete_all), null: false

      timestamps(type: :utc_datetime)
    end

    create index(:portfolio_chunks, [:document_id])

    # HNSW index for fast similarity search
    execute "CREATE INDEX portfolio_chunks_embedding_idx ON portfolio_chunks USING hnsw (embedding vector_cosine_ops)"
  end

  def down do
    drop table(:portfolio_chunks)
    drop table(:portfolio_documents)
    drop table(:portfolio_collections)
  end
end
```

**Note**: Replace `YYYYMMDDHHMMSS` with actual timestamp when creating.

---

### Task 5: Schema Query Helpers

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/schemas/queries.ex`:

```elixir
defmodule PortfolioIndex.Schemas.Queries do
  @moduledoc """
  Query helpers for document management schemas.
  Provides common queries for collections, documents, and chunks.
  """

  import Ecto.Query
  alias PortfolioIndex.Schemas.{Collection, Document, Chunk}

  @doc "Get collection by name"
  @spec get_collection_by_name(Ecto.Repo.t(), String.t()) :: Collection.t() | nil
  def get_collection_by_name(repo, name)

  @doc "Get or create collection by name"
  @spec get_or_create_collection(Ecto.Repo.t(), String.t(), map()) :: {:ok, Collection.t()} | {:error, term()}
  def get_or_create_collection(repo, name, attrs \\ %{})

  @doc "List documents by status"
  @spec list_documents_by_status(Ecto.Repo.t(), Document.status(), keyword()) :: [Document.t()]
  def list_documents_by_status(repo, status, opts \\ [])

  @doc "Get document with chunks preloaded"
  @spec get_document_with_chunks(Ecto.Repo.t(), Ecto.UUID.t()) :: Document.t() | nil
  def get_document_with_chunks(repo, document_id)

  @doc "Find chunks by similarity search"
  @spec similarity_search(Ecto.Repo.t(), [float()], keyword()) :: [Chunk.t()]
  def similarity_search(repo, embedding, opts \\ [])

  @doc "Count chunks needing embedding"
  @spec count_chunks_without_embedding(Ecto.Repo.t()) :: non_neg_integer()
  def count_chunks_without_embedding(repo)

  @doc "Get failed documents for retry"
  @spec get_failed_documents(Ecto.Repo.t(), keyword()) :: [Document.t()]
  def get_failed_documents(repo, opts \\ [])

  @doc "Mark document as deleted (soft delete)"
  @spec soft_delete_document(Ecto.Repo.t(), Document.t()) :: {:ok, Document.t()} | {:error, term()}
  def soft_delete_document(repo, document)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/schemas/queries_test.exs`

---

## TDD Requirements

For each task:

1. **Write tests FIRST** following existing test patterns in the repo
2. Tests must cover:
   - Schema validations (required fields, constraints)
   - Changeset edge cases (missing fields, invalid values)
   - Query functions with fixtures
   - Migration up/down (integration test)
3. Run tests continuously: `mix test path/to/test_file.exs`

## Quality Gates

Before considering this prompt complete:

```bash
cd /home/home/p/g/n/portfolio_index
mix test
mix credo --strict
mix dialyzer
```

All must pass with zero warnings and zero errors.

## Documentation Updates

### portfolio_index
Update `/home/home/p/g/n/portfolio_index/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `PortfolioIndex.Schemas.Collection` - Ecto schema for document collections
- `PortfolioIndex.Schemas.Document` - Ecto schema for ingested documents with status tracking
- `PortfolioIndex.Schemas.Chunk` - Ecto schema for document chunks with pgvector embeddings
- `PortfolioIndex.Schemas.Queries` - Query helpers for schema operations
- Database migration for document management tables with pgvector support
```

## Verification Checklist

- [ ] All schema files created in correct locations
- [ ] Migration file created with correct structure
- [ ] All tests pass
- [ ] No credo warnings
- [ ] No dialyzer errors
- [ ] Changelog updated
- [ ] Module documentation complete with @moduledoc and @doc
- [ ] Type specifications complete with @type and @spec
</file>

<file path="prompts/03_production_maintenance.md">
# Prompt 3: Production Maintenance Implementation

## Target Repositories
- **portfolio_index**: `/home/home/p/g/n/portfolio_index`
- **portfolio_manager**: `/home/home/p/g/n/portfolio_manager`

## Required Reading Before Implementation

### Reference Implementation (Arcana)
Read these files to understand the reference implementation:
```
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/maintenance.ex
/home/home/p/g/n/portfolio_index/arcana/lib/mix/tasks/arcana.install.ex
/home/home/p/g/n/portfolio_index/arcana/lib/mix/tasks/arcana.gen.embedding_migration.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/reembedder.ex
```

### Existing Portfolio Code
Read these files to understand existing patterns:
```
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/embedder/voyage.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/embedder/instructor.ex
/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/embedder.ex
/home/home/p/g/n/portfolio_index/mix.exs
```

### Gap Analysis Documentation
```
/home/home/p/g/n/portfolio_index/docs/20251230/arcana_gap_analysis/07_mix_tasks.md
/home/home/p/g/n/portfolio_index/docs/20251230/arcana_gap_analysis/08_maintenance_and_documents.md
```

---

## Implementation Tasks

### Task 1: Maintenance Module (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/maintenance.ex`:

```elixir
defmodule PortfolioIndex.Maintenance do
  @moduledoc """
  Production maintenance utilities for document and embedding management.
  Provides re-embedding, diagnostics, and batch operations.
  """

  alias PortfolioIndex.Schemas.{Document, Chunk, Collection}

  @type reembed_result :: %{
    total: non_neg_integer(),
    processed: non_neg_integer(),
    failed: non_neg_integer(),
    errors: [%{chunk_id: String.t(), error: term()}]
  }

  @type diagnostics_result :: %{
    collections: non_neg_integer(),
    documents: non_neg_integer(),
    chunks: non_neg_integer(),
    chunks_without_embedding: non_neg_integer(),
    failed_documents: non_neg_integer(),
    storage_bytes: non_neg_integer() | nil
  }

  @doc """
  Re-embed all chunks or a filtered subset.

  Options:
  - `:collection` - Only re-embed chunks in this collection
  - `:document_id` - Only re-embed chunks for this document
  - `:batch_size` - Number of chunks per batch (default 100)
  - `:embedder` - Embedder module to use (default from config)
  - `:on_progress` - Callback function for progress updates
  """
  @spec reembed(Ecto.Repo.t(), keyword()) :: {:ok, reembed_result()} | {:error, term()}
  def reembed(repo, opts \\ [])

  @doc """
  Get system diagnostics including counts and storage usage.
  """
  @spec diagnostics(Ecto.Repo.t()) :: {:ok, diagnostics_result()}
  def diagnostics(repo)

  @doc """
  Retry failed document processing.

  Options:
  - `:limit` - Max documents to retry (default all)
  - `:on_progress` - Callback for progress updates
  """
  @spec retry_failed(Ecto.Repo.t(), keyword()) :: {:ok, map()} | {:error, term()}
  def retry_failed(repo, opts \\ [])

  @doc """
  Clean up soft-deleted documents and their chunks.
  """
  @spec cleanup_deleted(Ecto.Repo.t(), keyword()) :: {:ok, non_neg_integer()}
  def cleanup_deleted(repo, opts \\ [])

  @doc """
  Verify embedding consistency (detect dimension mismatches).
  """
  @spec verify_embeddings(Ecto.Repo.t()) :: {:ok, map()} | {:error, term()}
  def verify_embeddings(repo)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/maintenance_test.exs`

---

### Task 2: Install Mix Task (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/mix/tasks/portfolio.install.ex`:

```elixir
defmodule Mix.Tasks.Portfolio.Install do
  @moduledoc """
  Installs PortfolioIndex into a Phoenix application.

  ## Usage

      mix portfolio.install

  ## What it does

  1. Creates required database migrations
  2. Prints configuration instructions
  3. Provides next steps for setup

  ## Options

  - `--repo` - Ecto repo module (default: inferred from app)
  - `--dimension` - Embedding vector dimension (default: 384)
  - `--no-migrations` - Skip migration generation
  """

  use Mix.Task

  @shortdoc "Install PortfolioIndex in your application"

  @impl Mix.Task
  def run(args)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/mix/tasks/portfolio.install_test.exs`

---

### Task 3: Embedding Migration Generator (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/mix/tasks/portfolio.gen.embedding_migration.ex`:

```elixir
defmodule Mix.Tasks.Portfolio.Gen.EmbeddingMigration do
  @moduledoc """
  Generates a migration for changing embedding dimensions.

  ## Usage

      mix portfolio.gen.embedding_migration --dimension 1536

  This is useful when switching embedding models with different dimensions.

  ## Options

  - `--dimension` - New embedding dimension (required)
  - `--table` - Table name (default: portfolio_chunks)
  - `--column` - Column name (default: embedding)
  """

  use Mix.Task

  @shortdoc "Generate migration to change embedding dimensions"

  @impl Mix.Task
  def run(args)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/mix/tasks/portfolio.gen.embedding_migration_test.exs`

---

### Task 4: Re-embed Mix Task (portfolio_manager)

Create `/home/home/p/g/n/portfolio_manager/lib/mix/tasks/portfolio.reembed.ex`:

```elixir
defmodule Mix.Tasks.Portfolio.Reembed do
  @moduledoc """
  Re-embeds documents using the current embedding configuration.

  ## Usage

      # Re-embed all chunks
      mix portfolio.reembed

      # Re-embed specific collection
      mix portfolio.reembed --collection my_docs

      # Re-embed with progress output
      mix portfolio.reembed --verbose

  ## Options

  - `--collection` - Only re-embed chunks in this collection
  - `--batch-size` - Chunks per batch (default: 100)
  - `--verbose` - Show progress updates
  - `--dry-run` - Show what would be re-embedded without doing it
  """

  use Mix.Task

  @shortdoc "Re-embed documents with current embedding model"

  @impl Mix.Task
  def run(args)
end
```

**Test file**: `/home/home/p/g/n/portfolio_manager/test/mix/tasks/portfolio.reembed_test.exs`

---

### Task 5: Diagnostics Mix Task (portfolio_manager)

Create `/home/home/p/g/n/portfolio_manager/lib/mix/tasks/portfolio.diagnostics.ex`:

```elixir
defmodule Mix.Tasks.Portfolio.Diagnostics do
  @moduledoc """
  Shows diagnostics for the PortfolioIndex system.

  ## Usage

      mix portfolio.diagnostics

  ## Output

  Shows:
  - Collection count and names
  - Document count by status
  - Chunk count and embedding coverage
  - Storage usage estimates
  - Configuration summary
  """

  use Mix.Task

  @shortdoc "Show PortfolioIndex system diagnostics"

  @impl Mix.Task
  def run(args)
end
```

**Test file**: `/home/home/p/g/n/portfolio_manager/test/mix/tasks/portfolio.diagnostics_test.exs`

---

### Task 6: Progress Reporter Module (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/maintenance/progress.ex`:

```elixir
defmodule PortfolioIndex.Maintenance.Progress do
  @moduledoc """
  Progress reporting utilities for maintenance operations.
  Provides callbacks for CLI and programmatic progress tracking.
  """

  @type progress_callback :: (map() -> :ok)

  @type progress_event :: %{
    operation: atom(),
    current: non_neg_integer(),
    total: non_neg_integer(),
    percentage: float(),
    message: String.t() | nil
  }

  @doc "Create a CLI progress reporter that prints to stdout"
  @spec cli_reporter(keyword()) :: progress_callback()
  def cli_reporter(opts \\ [])

  @doc "Create a silent reporter (no-op)"
  @spec silent_reporter() :: progress_callback()
  def silent_reporter()

  @doc "Create a telemetry-emitting reporter"
  @spec telemetry_reporter(list()) :: progress_callback()
  def telemetry_reporter(event_prefix)

  @doc "Report progress event"
  @spec report(progress_callback() | nil, progress_event()) :: :ok
  def report(callback, event)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/maintenance/progress_test.exs`

---

## TDD Requirements

For each task:

1. **Write tests FIRST** following existing test patterns in the repos
2. Tests must cover:
   - Happy path with valid input
   - Error handling (database errors, invalid options)
   - Edge cases (empty database, no embeddings)
   - Progress callback invocation
   - CLI option parsing (for Mix tasks)
3. Run tests continuously: `mix test path/to/test_file.exs`

## Quality Gates

Before considering this prompt complete:

```bash
# In portfolio_index
cd /home/home/p/g/n/portfolio_index
mix test
mix credo --strict
mix dialyzer

# In portfolio_manager
cd /home/home/p/g/n/portfolio_manager
mix test
mix credo --strict
mix dialyzer
```

All must pass with zero warnings and zero errors.

## Documentation Updates

### portfolio_index
Update `/home/home/p/g/n/portfolio_index/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `PortfolioIndex.Maintenance` - Production maintenance utilities (reembed, diagnostics, retry)
- `PortfolioIndex.Maintenance.Progress` - Progress reporting for maintenance operations
- `mix portfolio.install` - Installation task for new projects
- `mix portfolio.gen.embedding_migration` - Generate migration for dimension changes
```

### portfolio_manager
Update `/home/home/p/g/n/portfolio_manager/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `mix portfolio.reembed` - Re-embed documents with current embedding model
- `mix portfolio.diagnostics` - Show system diagnostics and health
```

## Verification Checklist

- [ ] All new files created in correct locations
- [ ] All tests pass in both repos
- [ ] No credo warnings
- [ ] No dialyzer errors
- [ ] Changelogs updated for both repos
- [ ] Module documentation complete with @moduledoc and @doc
- [ ] Type specifications complete with @type and @spec
- [ ] Mix task help text is clear and complete
</file>

<file path="prompts/04_evaluation_system.md">
# Prompt 4: Evaluation System Implementation

## Target Repositories
- **portfolio_core**: `/home/home/p/g/n/portfolio_core`
- **portfolio_index**: `/home/home/p/g/n/portfolio_index`
- **portfolio_manager**: `/home/home/p/g/n/portfolio_manager`

## Required Reading Before Implementation

### Reference Implementation (Arcana)
Read these files to understand the reference implementation:
```
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/evaluation.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/evaluation/test_case.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/evaluation/generator.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/evaluation/metrics.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/evaluation/answer_metrics.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/evaluation/run.ex
/home/home/p/g/n/portfolio_index/arcana/lib/mix/tasks/arcana.eval.generate.ex
/home/home/p/g/n/portfolio_index/arcana/lib/mix/tasks/arcana.eval.run.ex
```

### Existing Portfolio Code
Read these files to understand existing patterns:
```
/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/evaluation.ex
/home/home/p/g/n/portfolio_manager/lib/portfolio_manager/evaluation.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/llm/anthropic.ex
```

### Gap Analysis Documentation
```
/home/home/p/g/n/portfolio_index/docs/20251230/arcana_gap_analysis/04_evaluation_system.md
```

---

## Implementation Tasks

### Task 1: Retrieval Metrics Port (portfolio_core)

Create `/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/retrieval_metrics.ex`:

```elixir
defmodule PortfolioCore.Ports.RetrievalMetrics do
  @moduledoc """
  Behaviour for computing information retrieval quality metrics.
  Measures how well a search system retrieves relevant documents.
  """

  @type metric_result :: %{
    recall_at_k: %{1 => float(), 3 => float(), 5 => float(), 10 => float()},
    precision_at_k: %{1 => float(), 3 => float(), 5 => float(), 10 => float()},
    mrr: float(),
    hit_rate_at_k: %{1 => float(), 3 => float(), 5 => float(), 10 => float()}
  }

  @type test_case_result :: %{
    test_case_id: String.t(),
    question: String.t(),
    expected_ids: [String.t()],
    retrieved_ids: [String.t()],
    metrics: metric_result()
  }

  @doc """
  Compute retrieval metrics for a single test case.

  ## Parameters
  - `expected_ids` - List of relevant document/chunk IDs (ground truth)
  - `retrieved_ids` - List of retrieved document/chunk IDs (in rank order)
  - `opts` - Options including `:k_values` (default [1, 3, 5, 10])
  """
  @callback compute(expected_ids :: [String.t()], retrieved_ids :: [String.t()], opts :: keyword()) ::
    {:ok, metric_result()} | {:error, term()}

  @doc """
  Aggregate metrics across multiple test cases.
  """
  @callback aggregate(results :: [test_case_result()]) :: {:ok, metric_result()} | {:error, term()}
end
```

**Test file**: `/home/home/p/g/n/portfolio_core/test/ports/retrieval_metrics_test.exs`

---

### Task 2: Retrieval Metrics Adapter (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/retrieval_metrics/standard.ex`:

```elixir
defmodule PortfolioIndex.Adapters.RetrievalMetrics.Standard do
  @moduledoc """
  Standard information retrieval metrics implementation.
  Computes Recall@K, Precision@K, MRR, and Hit Rate@K.
  """

  @behaviour PortfolioCore.Ports.RetrievalMetrics

  @default_k_values [1, 3, 5, 10]

  @impl true
  def compute(expected_ids, retrieved_ids, opts \\ [])

  @impl true
  def aggregate(results)

  @doc "Compute Recall@K - fraction of relevant docs in top K"
  @spec recall_at_k([String.t()], [String.t()], pos_integer()) :: float()
  def recall_at_k(expected_ids, retrieved_ids, k)

  @doc "Compute Precision@K - fraction of top K that are relevant"
  @spec precision_at_k([String.t()], [String.t()], pos_integer()) :: float()
  def precision_at_k(expected_ids, retrieved_ids, k)

  @doc "Compute MRR - 1/position of first relevant result"
  @spec mrr([String.t()], [String.t()]) :: float()
  def mrr(expected_ids, retrieved_ids)

  @doc "Compute Hit Rate@K - 1 if any relevant in top K, else 0"
  @spec hit_rate_at_k([String.t()], [String.t()], pos_integer()) :: float()
  def hit_rate_at_k(expected_ids, retrieved_ids, k)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/adapters/retrieval_metrics/standard_test.exs`

---

### Task 3: Test Case Schema (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/schemas/test_case.ex`:

```elixir
defmodule PortfolioIndex.Schemas.TestCase do
  @moduledoc """
  Ecto schema for evaluation test cases.
  Links questions to their expected relevant chunks (ground truth).
  """

  use Ecto.Schema
  import Ecto.Changeset

  @type source :: :synthetic | :manual

  @type t :: %__MODULE__{
    id: Ecto.UUID.t() | nil,
    question: String.t(),
    source: source(),
    collection: String.t() | nil,
    metadata: map(),
    inserted_at: DateTime.t() | nil,
    updated_at: DateTime.t() | nil
  }

  @primary_key {:id, :binary_id, autogenerate: true}
  @foreign_key_type :binary_id

  schema "portfolio_evaluation_test_cases" do
    field :question, :string
    field :source, Ecto.Enum, values: [:synthetic, :manual], default: :manual
    field :collection, :string
    field :metadata, :map, default: %{}

    many_to_many :relevant_chunks, PortfolioIndex.Schemas.Chunk,
      join_through: "portfolio_evaluation_test_case_chunks",
      on_replace: :delete

    timestamps(type: :utc_datetime)
  end

  @doc "Changeset for creating test cases"
  @spec changeset(t(), map()) :: Ecto.Changeset.t()
  def changeset(test_case, attrs)

  @doc "Add relevant chunks to a test case"
  @spec add_relevant_chunks(t(), [PortfolioIndex.Schemas.Chunk.t()]) :: Ecto.Changeset.t()
  def add_relevant_chunks(test_case, chunks)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/schemas/test_case_test.exs`

---

### Task 4: Evaluation Run Schema (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/schemas/evaluation_run.ex`:

```elixir
defmodule PortfolioIndex.Schemas.EvaluationRun do
  @moduledoc """
  Ecto schema for tracking evaluation runs.
  Stores configuration, status, and results for historical comparison.
  """

  use Ecto.Schema
  import Ecto.Changeset

  @type status :: :running | :completed | :failed

  @type t :: %__MODULE__{
    id: Ecto.UUID.t() | nil,
    status: status(),
    config: map(),
    aggregate_metrics: map(),
    per_case_results: [map()],
    error_message: String.t() | nil,
    started_at: DateTime.t() | nil,
    completed_at: DateTime.t() | nil,
    inserted_at: DateTime.t() | nil,
    updated_at: DateTime.t() | nil
  }

  @primary_key {:id, :binary_id, autogenerate: true}

  schema "portfolio_evaluation_runs" do
    field :status, Ecto.Enum, values: [:running, :completed, :failed], default: :running
    field :config, :map, default: %{}
    field :aggregate_metrics, :map, default: %{}
    field :per_case_results, {:array, :map}, default: []
    field :error_message, :string
    field :started_at, :utc_datetime
    field :completed_at, :utc_datetime

    timestamps(type: :utc_datetime)
  end

  @doc "Changeset for creating evaluation runs"
  @spec changeset(t(), map()) :: Ecto.Changeset.t()
  def changeset(run, attrs)

  @doc "Mark run as completed with results"
  @spec complete(t(), map(), [map()]) :: Ecto.Changeset.t()
  def complete(run, aggregate_metrics, per_case_results)

  @doc "Mark run as failed with error"
  @spec fail(t(), String.t()) :: Ecto.Changeset.t()
  def fail(run, error_message)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/schemas/evaluation_run_test.exs`

---

### Task 5: Test Case Generator (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/evaluation/generator.ex`:

```elixir
defmodule PortfolioIndex.Evaluation.Generator do
  @moduledoc """
  LLM-powered synthetic test case generation.
  Creates questions from document chunks for evaluation.
  """

  alias PortfolioIndex.Schemas.{TestCase, Chunk}

  @default_prompt """
  Based on the following text chunk, generate a specific question that could be answered using this content.
  The question should be clear, searchable, and directly related to the information in the chunk.
  Return ONLY the question, nothing else.

  Text chunk:
  {chunk_text}
  """

  @type generate_opts :: [
    sample_size: pos_integer(),
    collection: String.t() | nil,
    prompt: String.t() | (String.t() -> String.t()),
    llm: (String.t() -> {:ok, String.t()} | {:error, term()})
  ]

  @doc """
  Generate synthetic test cases from chunks.

  Options:
  - `:sample_size` - Number of chunks to sample (default: 10)
  - `:collection` - Filter chunks by collection
  - `:prompt` - Custom prompt template with {chunk_text} placeholder
  - `:llm` - LLM function for question generation
  """
  @spec generate(Ecto.Repo.t(), generate_opts()) :: {:ok, [TestCase.t()]} | {:error, term()}
  def generate(repo, opts \\ [])

  @doc "Generate a question for a single chunk"
  @spec generate_question(Chunk.t(), keyword()) :: {:ok, String.t()} | {:error, term()}
  def generate_question(chunk, opts)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/evaluation/generator_test.exs`

---

### Task 6: Evaluation Orchestrator (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/evaluation.ex`:

```elixir
defmodule PortfolioIndex.Evaluation do
  @moduledoc """
  Main entry point for retrieval evaluation.
  Orchestrates test case execution and metrics computation.
  """

  alias PortfolioIndex.Schemas.{TestCase, EvaluationRun}
  alias PortfolioIndex.Adapters.RetrievalMetrics.Standard

  @type run_opts :: [
    mode: :semantic | :fulltext | :hybrid,
    collection: String.t() | nil,
    limit: pos_integer() | nil,
    search_fn: (String.t(), keyword() -> [map()]),
    evaluate_answer: boolean()
  ]

  @doc """
  Run evaluation against all or filtered test cases.

  Options:
  - `:mode` - Search mode (default: :semantic)
  - `:collection` - Filter test cases by collection
  - `:limit` - Max test cases to evaluate
  - `:search_fn` - Custom search function
  - `:evaluate_answer` - Also evaluate answer quality (default: false)
  """
  @spec run(Ecto.Repo.t(), run_opts()) :: {:ok, EvaluationRun.t()} | {:error, term()}
  def run(repo, opts \\ [])

  @doc "List all test cases"
  @spec list_test_cases(Ecto.Repo.t(), keyword()) :: [TestCase.t()]
  def list_test_cases(repo, opts \\ [])

  @doc "Create a manual test case"
  @spec create_test_case(Ecto.Repo.t(), map()) :: {:ok, TestCase.t()} | {:error, term()}
  def create_test_case(repo, attrs)

  @doc "Add relevant chunks to a test case"
  @spec add_ground_truth(Ecto.Repo.t(), TestCase.t(), [String.t()]) :: {:ok, TestCase.t()} | {:error, term()}
  def add_ground_truth(repo, test_case, chunk_ids)

  @doc "Get historical evaluation runs"
  @spec list_runs(Ecto.Repo.t(), keyword()) :: [EvaluationRun.t()]
  def list_runs(repo, opts \\ [])

  @doc "Compare two evaluation runs"
  @spec compare_runs(EvaluationRun.t(), EvaluationRun.t()) :: map()
  def compare_runs(run1, run2)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/evaluation_test.exs`

---

### Task 7: Evaluation Migration (portfolio_index)

Create migration `/home/home/p/g/n/portfolio_index/priv/repo/migrations/YYYYMMDDHHMMSS_create_portfolio_evaluation_tables.exs`:

```elixir
defmodule PortfolioIndex.Repo.Migrations.CreatePortfolioEvaluationTables do
  use Ecto.Migration

  def change do
    # Test cases table
    create table(:portfolio_evaluation_test_cases, primary_key: false) do
      add :id, :binary_id, primary_key: true
      add :question, :text, null: false
      add :source, :string, default: "manual"
      add :collection, :string
      add :metadata, :map, default: %{}

      timestamps(type: :utc_datetime)
    end

    create index(:portfolio_evaluation_test_cases, [:collection])
    create index(:portfolio_evaluation_test_cases, [:source])

    # Join table for test case chunks
    create table(:portfolio_evaluation_test_case_chunks, primary_key: false) do
      add :test_case_id, references(:portfolio_evaluation_test_cases, type: :binary_id, on_delete: :delete_all), null: false
      add :chunk_id, references(:portfolio_chunks, type: :binary_id, on_delete: :delete_all), null: false
    end

    create unique_index(:portfolio_evaluation_test_case_chunks, [:test_case_id, :chunk_id])

    # Evaluation runs table
    create table(:portfolio_evaluation_runs, primary_key: false) do
      add :id, :binary_id, primary_key: true
      add :status, :string, default: "running"
      add :config, :map, default: %{}
      add :aggregate_metrics, :map, default: %{}
      add :per_case_results, {:array, :map}, default: []
      add :error_message, :text
      add :started_at, :utc_datetime
      add :completed_at, :utc_datetime

      timestamps(type: :utc_datetime)
    end

    create index(:portfolio_evaluation_runs, [:status])
  end
end
```

---

### Task 8: Eval Generate Mix Task (portfolio_manager)

Create `/home/home/p/g/n/portfolio_manager/lib/mix/tasks/portfolio.eval.generate.ex`:

```elixir
defmodule Mix.Tasks.Portfolio.Eval.Generate do
  @moduledoc """
  Generate synthetic test cases from document chunks.

  ## Usage

      mix portfolio.eval.generate

  ## Options

  - `--sample-size` - Number of chunks to sample (default: 10)
  - `--collection` - Only sample from this collection
  - `--source-id` - Only sample from documents with this source ID
  """

  use Mix.Task

  @shortdoc "Generate synthetic evaluation test cases"

  @impl Mix.Task
  def run(args)
end
```

**Test file**: `/home/home/p/g/n/portfolio_manager/test/mix/tasks/portfolio.eval.generate_test.exs`

---

### Task 9: Eval Run Mix Task (portfolio_manager)

Create `/home/home/p/g/n/portfolio_manager/lib/mix/tasks/portfolio.eval.run.ex`:

```elixir
defmodule Mix.Tasks.Portfolio.Eval.Run do
  @moduledoc """
  Run evaluation against test cases.

  ## Usage

      mix portfolio.eval.run

  ## Options

  - `--mode` - Search mode: semantic, fulltext, hybrid (default: semantic)
  - `--collection` - Only evaluate test cases for this collection
  - `--generate` - Generate test cases if none exist
  - `--format` - Output format: table, json (default: table)
  - `--fail-under` - Exit with code 1 if recall@5 below threshold
  """

  use Mix.Task

  @shortdoc "Run retrieval evaluation"

  @impl Mix.Task
  def run(args)
end
```

**Test file**: `/home/home/p/g/n/portfolio_manager/test/mix/tasks/portfolio.eval.run_test.exs`

---

## TDD Requirements

For each task:

1. **Write tests FIRST** following existing test patterns in the repos
2. Tests must cover:
   - Metric computations with known values
   - Edge cases (empty lists, no relevant docs)
   - Schema validations
   - Generator with mock LLM
   - CLI option parsing
   - Integration tests with test database
3. Run tests continuously: `mix test path/to/test_file.exs`

## Quality Gates

Before considering this prompt complete:

```bash
# In portfolio_core
cd /home/home/p/g/n/portfolio_core
mix test
mix credo --strict
mix dialyzer

# In portfolio_index
cd /home/home/p/g/n/portfolio_index
mix test
mix credo --strict
mix dialyzer

# In portfolio_manager
cd /home/home/p/g/n/portfolio_manager
mix test
mix credo --strict
mix dialyzer
```

All must pass with zero warnings and zero errors.

## Documentation Updates

### portfolio_core
Update `/home/home/p/g/n/portfolio_core/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `PortfolioCore.Ports.RetrievalMetrics` behaviour for IR quality metrics
```

### portfolio_index
Update `/home/home/p/g/n/portfolio_index/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `PortfolioIndex.Adapters.RetrievalMetrics.Standard` - Recall@K, Precision@K, MRR, Hit Rate
- `PortfolioIndex.Schemas.TestCase` - Ecto schema for evaluation test cases
- `PortfolioIndex.Schemas.EvaluationRun` - Ecto schema for evaluation run tracking
- `PortfolioIndex.Evaluation.Generator` - LLM-powered synthetic test case generation
- `PortfolioIndex.Evaluation` - Evaluation orchestrator
- Database migrations for evaluation tables
```

### portfolio_manager
Update `/home/home/p/g/n/portfolio_manager/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `mix portfolio.eval.generate` - Generate synthetic evaluation test cases
- `mix portfolio.eval.run` - Run retrieval evaluation with metrics output
```

## Verification Checklist

- [ ] All new files created in correct locations across all three repos
- [ ] All tests pass
- [ ] No credo warnings
- [ ] No dialyzer errors
- [ ] Changelogs updated for all three repos
- [ ] Module documentation complete
- [ ] Type specifications complete
</file>

<file path="prompts/05_embedder_enhancements.md">
# Prompt 5: Embedder Enhancements Implementation

## Target Repository
- **portfolio_index**: `/home/home/p/g/n/portfolio_index`

## Required Reading Before Implementation

### Reference Implementation (Arcana)
Read these files to understand the reference implementation:
```
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/embedder.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/embedder/local.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/embedder/openai.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/config.ex
```

### Existing Portfolio Code
Read these files to understand existing patterns:
```
/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/embedder.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/embedder/voyage.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/embedder/instructor.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/embedder/ollama.ex
```

### Gap Analysis Documentation
```
/home/home/p/g/n/portfolio_index/docs/20251230/arcana_gap_analysis/02_embedder_system.md
```

---

## Implementation Tasks

### Task 1: OpenAI Embedder Adapter

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/embedder/openai.ex`:

```elixir
defmodule PortfolioIndex.Adapters.Embedder.OpenAI do
  @moduledoc """
  OpenAI embeddings adapter using the text-embedding API.

  ## Configuration

  Set the API key via environment variable or config:

      config :portfolio_index, PortfolioIndex.Adapters.Embedder.OpenAI,
        api_key: System.get_env("OPENAI_API_KEY"),
        model: "text-embedding-3-small"

  ## Models

  - `text-embedding-3-small` - 1536 dimensions (default)
  - `text-embedding-3-large` - 3072 dimensions
  - `text-embedding-ada-002` - 1536 dimensions (legacy)
  """

  @behaviour PortfolioCore.Ports.Embedder

  @default_model "text-embedding-3-small"
  @api_url "https://api.openai.com/v1/embeddings"

  @model_dimensions %{
    "text-embedding-3-small" => 1536,
    "text-embedding-3-large" => 3072,
    "text-embedding-ada-002" => 1536
  }

  @impl true
  def embed(text, opts \\ [])

  @impl true
  def embed_batch(texts, opts \\ [])

  @impl true
  def dimensions(opts \\ [])

  @impl true
  def supported_models do
    Map.keys(@model_dimensions)
  end

  @doc "Get dimension for a specific model"
  @spec model_dimensions(String.t()) :: pos_integer() | nil
  def model_dimensions(model)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/adapters/embedder/openai_test.exs`

---

### Task 2: Local Bumblebee Embedder Adapter

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/embedder/bumblebee.ex`:

```elixir
defmodule PortfolioIndex.Adapters.Embedder.Bumblebee do
  @moduledoc """
  Local embeddings using Bumblebee and Nx.Serving.
  Runs HuggingFace models locally without API calls.

  ## Configuration

      config :portfolio_index, PortfolioIndex.Adapters.Embedder.Bumblebee,
        model: "BAAI/bge-small-en-v1.5",
        serving_name: PortfolioIndex.EmbeddingServing

  ## Models

  - `BAAI/bge-small-en-v1.5` - 384 dimensions (default, fast)
  - `BAAI/bge-base-en-v1.5` - 768 dimensions
  - `BAAI/bge-large-en-v1.5` - 1024 dimensions
  - `sentence-transformers/all-MiniLM-L6-v2` - 384 dimensions

  ## Setup

  Add to your supervision tree:

      children = [
        {PortfolioIndex.Adapters.Embedder.Bumblebee, name: PortfolioIndex.EmbeddingServing}
      ]
  """

  @behaviour PortfolioCore.Ports.Embedder

  use GenServer

  @default_model "BAAI/bge-small-en-v1.5"

  @model_dimensions %{
    "BAAI/bge-small-en-v1.5" => 384,
    "BAAI/bge-base-en-v1.5" => 768,
    "BAAI/bge-large-en-v1.5" => 1024,
    "sentence-transformers/all-MiniLM-L6-v2" => 384
  }

  # GenServer callbacks for Nx.Serving management
  def start_link(opts)
  def init(opts)
  def child_spec(opts)

  @impl PortfolioCore.Ports.Embedder
  def embed(text, opts \\ [])

  @impl PortfolioCore.Ports.Embedder
  def embed_batch(texts, opts \\ [])

  @impl PortfolioCore.Ports.Embedder
  def dimensions(opts \\ [])

  @impl PortfolioCore.Ports.Embedder
  def supported_models do
    Map.keys(@model_dimensions)
  end

  @doc "Check if the serving is ready"
  @spec ready?(atom()) :: boolean()
  def ready?(serving_name \\ __MODULE__)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/adapters/embedder/bumblebee_test.exs`

---

### Task 3: Custom Function Embedder

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/embedder/function.ex`:

```elixir
defmodule PortfolioIndex.Adapters.Embedder.Function do
  @moduledoc """
  Wrapper adapter that delegates to a custom function.
  Useful for quick integration of custom embedding logic.

  ## Usage

      # With a function
      embedder = PortfolioIndex.Adapters.Embedder.Function.new(
        fn text -> MyEmbedder.embed(text) end,
        dimensions: 768
      )

      # Use in pipeline
      PortfolioIndex.RAG.search(query, embedder: embedder)
  """

  @behaviour PortfolioCore.Ports.Embedder

  @type t :: %__MODULE__{
    embed_fn: (String.t() -> {:ok, [float()]} | {:error, term()}),
    batch_fn: ([String.t()] -> {:ok, [[float()]]} | {:error, term()}) | nil,
    dimensions: pos_integer()
  }

  defstruct [:embed_fn, :batch_fn, :dimensions]

  @doc "Create a new function embedder"
  @spec new((String.t() -> {:ok, [float()]} | {:error, term()}), keyword()) :: t()
  def new(embed_fn, opts \\ [])

  @impl true
  def embed(%__MODULE__{} = embedder, text, opts \\ [])

  @impl true
  def embed_batch(%__MODULE__{} = embedder, texts, opts \\ [])

  @impl true
  def dimensions(%__MODULE__{} = embedder, _opts \\ [])

  @impl true
  def supported_models, do: ["custom"]
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/adapters/embedder/function_test.exs`

---

### Task 4: Model Dimension Registry

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/embedder/registry.ex`:

```elixir
defmodule PortfolioIndex.Embedder.Registry do
  @moduledoc """
  Registry of known embedding models and their dimensions.
  Used for auto-detection and validation.
  """

  @models %{
    # OpenAI
    "text-embedding-3-small" => %{provider: :openai, dimensions: 1536},
    "text-embedding-3-large" => %{provider: :openai, dimensions: 3072},
    "text-embedding-ada-002" => %{provider: :openai, dimensions: 1536},

    # Voyage
    "voyage-3" => %{provider: :voyage, dimensions: 1024},
    "voyage-3-lite" => %{provider: :voyage, dimensions: 512},
    "voyage-code-3" => %{provider: :voyage, dimensions: 1024},

    # Bumblebee/HuggingFace
    "BAAI/bge-small-en-v1.5" => %{provider: :bumblebee, dimensions: 384},
    "BAAI/bge-base-en-v1.5" => %{provider: :bumblebee, dimensions: 768},
    "BAAI/bge-large-en-v1.5" => %{provider: :bumblebee, dimensions: 1024},
    "sentence-transformers/all-MiniLM-L6-v2" => %{provider: :bumblebee, dimensions: 384},

    # Ollama
    "nomic-embed-text" => %{provider: :ollama, dimensions: 768},
    "mxbai-embed-large" => %{provider: :ollama, dimensions: 1024}
  }

  @doc "Get model info by name"
  @spec get(String.t()) :: map() | nil
  def get(model_name)

  @doc "Get dimensions for a model"
  @spec dimensions(String.t()) :: pos_integer() | nil
  def dimensions(model_name)

  @doc "Get provider for a model"
  @spec provider(String.t()) :: atom() | nil
  def provider(model_name)

  @doc "List all known models"
  @spec list() :: [String.t()]
  def list()

  @doc "List models by provider"
  @spec list_by_provider(atom()) :: [String.t()]
  def list_by_provider(provider)

  @doc "Register a custom model"
  @spec register(String.t(), atom(), pos_integer()) :: :ok
  def register(model_name, provider, dimensions)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/embedder/registry_test.exs`

---

### Task 5: Unified Embedder Configuration

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/embedder/config.ex`:

```elixir
defmodule PortfolioIndex.Embedder.Config do
  @moduledoc """
  Unified configuration for embedder selection and initialization.
  Supports shorthand syntax and automatic adapter resolution.

  ## Configuration Examples

      # Shorthand - provider atom
      config :portfolio_index, :embedder, :openai

      # Shorthand with model
      config :portfolio_index, :embedder, {:openai, model: "text-embedding-3-large"}

      # Full module specification
      config :portfolio_index, :embedder, PortfolioIndex.Adapters.Embedder.OpenAI

      # Custom function
      config :portfolio_index, :embedder, fn text -> MyEmbed.embed(text) end
  """

  alias PortfolioIndex.Adapters.Embedder

  @type embedder_config ::
    atom()
    | {atom(), keyword()}
    | module()
    | (String.t() -> {:ok, [float()]} | {:error, term()})

  @provider_modules %{
    openai: Embedder.OpenAI,
    voyage: Embedder.Voyage,
    bumblebee: Embedder.Bumblebee,
    ollama: Embedder.Ollama,
    instructor: Embedder.Instructor
  }

  @doc "Resolve embedder config to a module and options"
  @spec resolve(embedder_config()) :: {:ok, {module(), keyword()}} | {:error, term()}
  def resolve(config)

  @doc "Get the current embedder from application config"
  @spec current() :: {module(), keyword()}
  def current()

  @doc "Get dimensions for current embedder"
  @spec current_dimensions() :: pos_integer()
  def current_dimensions()

  @doc "Validate embedder configuration"
  @spec validate(embedder_config()) :: :ok | {:error, term()}
  def validate(config)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/embedder/config_test.exs`

---

### Task 6: Auto Dimension Detection

Update the existing embedder adapters to support dimension auto-detection:

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/embedder/dimension_detector.ex`:

```elixir
defmodule PortfolioIndex.Embedder.DimensionDetector do
  @moduledoc """
  Utilities for detecting embedding dimensions from various sources.
  """

  alias PortfolioIndex.Embedder.Registry

  @doc """
  Detect dimensions for an embedder configuration.

  Tries in order:
  1. Explicit :dimensions option
  2. Model registry lookup
  3. Probe embedding (embed empty string and measure)
  """
  @spec detect(module() | map(), keyword()) :: {:ok, pos_integer()} | {:error, term()}
  def detect(embedder, opts \\ [])

  @doc """
  Probe an embedder by generating an embedding and measuring dimensions.
  This is a fallback when dimensions aren't known statically.
  """
  @spec probe(module(), keyword()) :: {:ok, pos_integer()} | {:error, term()}
  def probe(embedder, opts \\ [])

  @doc """
  Validate that an embedding has the expected dimensions.
  """
  @spec validate_dimensions([float()], pos_integer()) :: :ok | {:error, term()}
  def validate_dimensions(embedding, expected_dimensions)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/embedder/dimension_detector_test.exs`

---

## TDD Requirements

For each task:

1. **Write tests FIRST** following existing test patterns in the repo
2. Tests must cover:
   - Happy path embedding
   - Batch embedding
   - Error handling (API errors, invalid input)
   - Dimension detection and validation
   - Configuration resolution
   - Mock external APIs in tests
3. Run tests continuously: `mix test path/to/test_file.exs`

## Quality Gates

Before considering this prompt complete:

```bash
cd /home/home/p/g/n/portfolio_index
mix test
mix credo --strict
mix dialyzer
```

All must pass with zero warnings and zero errors.

## Documentation Updates

### portfolio_index
Update `/home/home/p/g/n/portfolio_index/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `PortfolioIndex.Adapters.Embedder.OpenAI` - OpenAI text-embedding API adapter
- `PortfolioIndex.Adapters.Embedder.Bumblebee` - Local Bumblebee/Nx.Serving embeddings
- `PortfolioIndex.Adapters.Embedder.Function` - Custom function wrapper adapter
- `PortfolioIndex.Embedder.Registry` - Model dimension registry
- `PortfolioIndex.Embedder.Config` - Unified embedder configuration
- `PortfolioIndex.Embedder.DimensionDetector` - Automatic dimension detection
```

## Verification Checklist

- [ ] All new files created in correct locations
- [ ] All tests pass
- [ ] No credo warnings
- [ ] No dialyzer errors
- [ ] Changelog updated
- [ ] Module documentation complete with @moduledoc and @doc
- [ ] Type specifications complete with @type and @spec
- [ ] External API calls properly mocked in tests
</file>

<file path="prompts/06_vector_store_enhancements.md">
# Prompt 6: Vector Store Enhancements Implementation

## Target Repository
- **portfolio_index**: `/home/home/p/g/n/portfolio_index`

## Required Reading Before Implementation

### Reference Implementation (Arcana)
Read these files to understand the reference implementation:
```
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/vector_store.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/vector_store/pgvector.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/vector_store/hnswlib.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/search.ex
```

### Existing Portfolio Code
Read these files to understand existing patterns:
```
/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/vector_store.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/vector_store/pgvector.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/vector_store/qdrant.ex
```

### Gap Analysis Documentation
```
/home/home/p/g/n/portfolio_index/docs/20251230/arcana_gap_analysis/03_vector_store.md
```

---

## Implementation Tasks

### Task 1: In-Memory HNSWLib Adapter

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/vector_store/memory.ex`:

```elixir
defmodule PortfolioIndex.Adapters.VectorStore.Memory do
  @moduledoc """
  In-memory vector store using HNSWLib for fast similarity search.
  Useful for testing, development, and small datasets.

  ## Configuration

      config :portfolio_index, PortfolioIndex.Adapters.VectorStore.Memory,
        dimensions: 384,
        max_elements: 10_000,
        ef_construction: 200,
        m: 16

  ## Usage

  Add to supervision tree:

      children = [
        {PortfolioIndex.Adapters.VectorStore.Memory, name: :my_index, dimensions: 384}
      ]

  ## Notes

  - Data is not persisted across restarts
  - Supports optional file-based persistence via save/load
  - Thread-safe for concurrent reads and writes
  """

  @behaviour PortfolioCore.Ports.VectorStore

  use GenServer

  @default_opts [
    max_elements: 10_000,
    ef_construction: 200,
    m: 16,
    space: :cosine
  ]

  # GenServer callbacks
  def start_link(opts)
  def init(opts)
  def child_spec(opts)

  @impl PortfolioCore.Ports.VectorStore
  def insert(store, id, embedding, metadata \\ %{}, opts \\ [])

  @impl PortfolioCore.Ports.VectorStore
  def insert_batch(store, items, opts \\ [])

  @impl PortfolioCore.Ports.VectorStore
  def search(store, embedding, opts \\ [])

  @impl PortfolioCore.Ports.VectorStore
  def delete(store, id, opts \\ [])

  @impl PortfolioCore.Ports.VectorStore
  def get(store, id, opts \\ [])

  @doc "Save index to file"
  @spec save(GenServer.server(), String.t()) :: :ok | {:error, term()}
  def save(store, path)

  @doc "Load index from file"
  @spec load(GenServer.server(), String.t()) :: :ok | {:error, term()}
  def load(store, path)

  @doc "Get index statistics"
  @spec stats(GenServer.server()) :: map()
  def stats(store)

  @doc "Clear all data"
  @spec clear(GenServer.server()) :: :ok
  def clear(store)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/adapters/vector_store/memory_test.exs`

---

### Task 2: Backend Override Pattern

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/vector_store/backend.ex`:

```elixir
defmodule PortfolioIndex.VectorStore.Backend do
  @moduledoc """
  Backend resolution and override utilities for vector store operations.
  Allows per-call backend switching without global configuration changes.

  ## Usage

      # Use default backend from config
      Backend.search(embedding, limit: 5)

      # Override backend for this call
      Backend.search(embedding, limit: 5, backend: :memory)

      # Use specific backend module
      Backend.search(embedding, backend: PortfolioIndex.Adapters.VectorStore.Qdrant)
  """

  alias PortfolioIndex.Adapters.VectorStore

  @type backend_spec :: atom() | module() | {module(), keyword()}

  @backend_aliases %{
    pgvector: VectorStore.Pgvector,
    qdrant: VectorStore.Qdrant,
    memory: VectorStore.Memory
  }

  @doc "Resolve backend specification to module and options"
  @spec resolve(backend_spec() | nil) :: {module(), keyword()}
  def resolve(backend_spec \\ nil)

  @doc "Get the default backend from configuration"
  @spec default() :: {module(), keyword()}
  def default()

  @doc "Execute search with backend resolution"
  @spec search([float()], keyword()) :: {:ok, [map()]} | {:error, term()}
  def search(embedding, opts \\ [])

  @doc "Execute insert with backend resolution"
  @spec insert(String.t(), [float()], map(), keyword()) :: :ok | {:error, term()}
  def insert(id, embedding, metadata, opts \\ [])

  @doc "Execute batch insert with backend resolution"
  @spec insert_batch([map()], keyword()) :: :ok | {:error, term()}
  def insert_batch(items, opts \\ [])

  @doc "Execute delete with backend resolution"
  @spec delete(String.t(), keyword()) :: :ok | {:error, term()}
  def delete(id, opts \\ [])

  @doc "Get item by ID with backend resolution"
  @spec get(String.t(), keyword()) :: {:ok, map()} | {:error, :not_found} | {:error, term()}
  def get(id, opts \\ [])
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/vector_store/backend_test.exs`

---

### Task 3: Auto Index Creation

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/vector_store/index_manager.ex`:

```elixir
defmodule PortfolioIndex.VectorStore.IndexManager do
  @moduledoc """
  Manages vector store indexes including auto-creation and configuration.
  """

  @doc """
  Ensure index exists, creating if necessary.

  Options vary by backend:
  - pgvector: Creates HNSW index on embedding column
  - qdrant: Creates collection with specified config
  - memory: Initializes HNSWLib index
  """
  @spec ensure_index(module(), keyword()) :: :ok | {:error, term()}
  def ensure_index(backend, opts \\ [])

  @doc """
  Check if index exists.
  """
  @spec index_exists?(module(), keyword()) :: boolean()
  def index_exists?(backend, opts \\ [])

  @doc """
  Get index statistics.
  """
  @spec index_stats(module(), keyword()) :: {:ok, map()} | {:error, term()}
  def index_stats(backend, opts \\ [])

  @doc """
  Rebuild index (useful after bulk inserts).
  """
  @spec rebuild_index(module(), keyword()) :: :ok | {:error, term()}
  def rebuild_index(backend, opts \\ [])

  @doc """
  Drop index.
  """
  @spec drop_index(module(), keyword()) :: :ok | {:error, term()}
  def drop_index(backend, opts \\ [])
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/vector_store/index_manager_test.exs`

---

### Task 4: Collection Organization

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/vector_store/collections.ex`:

```elixir
defmodule PortfolioIndex.VectorStore.Collections do
  @moduledoc """
  Collection-based organization for vector store operations.
  Provides logical grouping of vectors without separate physical stores.
  """

  @doc """
  Search within a specific collection.
  """
  @spec search_collection(String.t(), [float()], keyword()) :: {:ok, [map()]} | {:error, term()}
  def search_collection(collection, embedding, opts \\ [])

  @doc """
  Insert into a specific collection.
  """
  @spec insert_to_collection(String.t(), String.t(), [float()], map(), keyword()) :: :ok | {:error, term()}
  def insert_to_collection(collection, id, embedding, metadata, opts \\ [])

  @doc """
  List all collections.
  """
  @spec list_collections(keyword()) :: {:ok, [String.t()]} | {:error, term()}
  def list_collections(opts \\ [])

  @doc """
  Get collection statistics.
  """
  @spec collection_stats(String.t(), keyword()) :: {:ok, map()} | {:error, term()}
  def collection_stats(collection, opts \\ [])

  @doc """
  Delete all vectors in a collection.
  """
  @spec clear_collection(String.t(), keyword()) :: :ok | {:error, term()}
  def clear_collection(collection, opts \\ [])

  @doc """
  Check if collection exists.
  """
  @spec collection_exists?(String.t(), keyword()) :: boolean()
  def collection_exists?(collection, opts \\ [])
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/vector_store/collections_test.exs`

---

### Task 5: Soft Deletion Support

Update the pgvector adapter to support soft deletion:

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/vector_store/soft_delete.ex`:

```elixir
defmodule PortfolioIndex.VectorStore.SoftDelete do
  @moduledoc """
  Soft deletion utilities for vector store items.
  Marks items as deleted without removing from storage.
  """

  @doc """
  Soft delete an item by ID.
  """
  @spec soft_delete(Ecto.Repo.t(), String.t()) :: :ok | {:error, term()}
  def soft_delete(repo, id)

  @doc """
  Soft delete items matching filter.
  """
  @spec soft_delete_where(Ecto.Repo.t(), keyword()) :: {:ok, non_neg_integer()} | {:error, term()}
  def soft_delete_where(repo, filters)

  @doc """
  Restore a soft-deleted item.
  """
  @spec restore(Ecto.Repo.t(), String.t()) :: :ok | {:error, term()}
  def restore(repo, id)

  @doc """
  Permanently delete soft-deleted items older than threshold.
  """
  @spec purge_deleted(Ecto.Repo.t(), keyword()) :: {:ok, non_neg_integer()} | {:error, term()}
  def purge_deleted(repo, opts \\ [])

  @doc """
  Count soft-deleted items.
  """
  @spec count_deleted(Ecto.Repo.t()) :: non_neg_integer()
  def count_deleted(repo)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/vector_store/soft_delete_test.exs`

---

### Task 6: Enhanced Search Options

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/vector_store/search.ex`:

```elixir
defmodule PortfolioIndex.VectorStore.Search do
  @moduledoc """
  Enhanced search utilities with filtering, scoring, and result processing.
  """

  @type search_opts :: [
    limit: pos_integer(),
    threshold: float(),
    collection: String.t(),
    filters: keyword(),
    include_deleted: boolean(),
    include_metadata: boolean(),
    distance_metric: :cosine | :euclidean | :dot_product
  ]

  @doc """
  Execute similarity search with enhanced options.
  """
  @spec similarity_search([float()], search_opts()) :: {:ok, [map()]} | {:error, term()}
  def similarity_search(embedding, opts \\ [])

  @doc """
  Execute hybrid search combining vector and keyword search.
  Uses Reciprocal Rank Fusion (RRF) for result merging.
  """
  @spec hybrid_search([float()], String.t(), search_opts()) :: {:ok, [map()]} | {:error, term()}
  def hybrid_search(embedding, query_text, opts \\ [])

  @doc """
  Apply metadata filters to search results.
  """
  @spec filter_results([map()], keyword()) :: [map()]
  def filter_results(results, filters)

  @doc """
  Normalize similarity scores to 0-1 range.
  """
  @spec normalize_scores([map()], atom()) :: [map()]
  def normalize_scores(results, distance_metric)

  @doc """
  Deduplicate results by content hash or ID.
  """
  @spec deduplicate([map()], atom()) :: [map()]
  def deduplicate(results, key \\ :id)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/vector_store/search_test.exs`

---

## TDD Requirements

For each task:

1. **Write tests FIRST** following existing test patterns in the repo
2. Tests must cover:
   - CRUD operations (insert, search, delete, get)
   - Batch operations
   - Collection filtering
   - Backend override functionality
   - Edge cases (empty results, not found)
   - Concurrent access (for memory store)
3. Run tests continuously: `mix test path/to/test_file.exs`

## Quality Gates

Before considering this prompt complete:

```bash
cd /home/home/p/g/n/portfolio_index
mix test
mix credo --strict
mix dialyzer
```

All must pass with zero warnings and zero errors.

## Documentation Updates

### portfolio_index
Update `/home/home/p/g/n/portfolio_index/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `PortfolioIndex.Adapters.VectorStore.Memory` - In-memory HNSWLib vector store
- `PortfolioIndex.VectorStore.Backend` - Backend resolution with per-call override
- `PortfolioIndex.VectorStore.IndexManager` - Index auto-creation and management
- `PortfolioIndex.VectorStore.Collections` - Collection-based organization
- `PortfolioIndex.VectorStore.SoftDelete` - Soft deletion support
- `PortfolioIndex.VectorStore.Search` - Enhanced search with hybrid support
```

## Verification Checklist

- [ ] All new files created in correct locations
- [ ] All tests pass
- [ ] No credo warnings
- [ ] No dialyzer errors
- [ ] Changelog updated
- [ ] Module documentation complete with @moduledoc and @doc
- [ ] Type specifications complete with @type and @spec
- [ ] Memory store properly handles concurrent access
</file>

<file path="prompts/07_telemetry_standardization.md">
# Prompt 7: Telemetry Standardization Implementation

## Target Repositories
- **portfolio_core**: `/home/home/p/g/n/portfolio_core`
- **portfolio_index**: `/home/home/p/g/n/portfolio_index`

## Required Reading Before Implementation

### Reference Implementation (Arcana)
Read these files to understand the reference implementation:
```
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/telemetry.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/telemetry/logger.ex
```

### Existing Portfolio Code
Read these files to understand existing patterns:
```
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/embedder/voyage.ex (search for :telemetry)
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/vector_store/pgvector.ex (search for :telemetry)
/home/home/p/g/n/portfolio_manager/lib/portfolio_manager/evaluation.ex (search for :telemetry)
```

### Gap Analysis Documentation
```
/home/home/p/g/n/portfolio_index/docs/20251230/arcana_gap_analysis/06_telemetry.md
```

---

## Implementation Tasks

### Task 1: Telemetry Event Definitions (portfolio_core)

Create `/home/home/p/g/n/portfolio_core/lib/portfolio_core/telemetry.ex`:

```elixir
defmodule PortfolioCore.Telemetry do
  @moduledoc """
  Telemetry event definitions and utilities for the Portfolio libraries.

  ## Event Naming Convention

  All events follow the pattern: `[:portfolio, :component, :operation]`

  ## Standard Events

  ### Embedding Events
  - `[:portfolio, :embedder, :embed, :start]`
  - `[:portfolio, :embedder, :embed, :stop]`
  - `[:portfolio, :embedder, :embed, :exception]`

  ### Vector Store Events
  - `[:portfolio, :vector_store, :search, :start]`
  - `[:portfolio, :vector_store, :search, :stop]`
  - `[:portfolio, :vector_store, :insert, :start]`
  - `[:portfolio, :vector_store, :insert, :stop]`

  ### LLM Events
  - `[:portfolio, :llm, :complete, :start]`
  - `[:portfolio, :llm, :complete, :stop]`
  - `[:portfolio, :llm, :complete, :exception]`

  ### RAG Pipeline Events
  - `[:portfolio, :rag, :rewrite, :start/:stop/:exception]`
  - `[:portfolio, :rag, :expand, :start/:stop/:exception]`
  - `[:portfolio, :rag, :decompose, :start/:stop/:exception]`
  - `[:portfolio, :rag, :select, :start/:stop/:exception]`
  - `[:portfolio, :rag, :search, :start/:stop/:exception]`
  - `[:portfolio, :rag, :rerank, :start/:stop/:exception]`
  - `[:portfolio, :rag, :answer, :start/:stop/:exception]`

  ### Evaluation Events
  - `[:portfolio, :evaluation, :run, :start/:stop/:exception]`
  - `[:portfolio, :evaluation, :test_case, :start/:stop]`
  """

  @type event_name :: [atom()]
  @type measurements :: map()
  @type metadata :: map()

  @doc """
  Execute a function wrapped in telemetry span.
  Emits start, stop, and exception events automatically.
  """
  @spec span(event_name(), metadata(), (-> result)) :: result when result: any()
  def span(event, metadata, fun)

  @doc """
  Emit a telemetry event.
  """
  @spec emit(event_name(), measurements(), metadata()) :: :ok
  def emit(event, measurements, metadata)

  @doc """
  Get all defined event names for documentation/attachment.
  """
  @spec events() :: [event_name()]
  def events()

  @doc """
  Get events for a specific component.
  """
  @spec events_for(atom()) :: [event_name()]
  def events_for(component)
end
```

**Test file**: `/home/home/p/g/n/portfolio_core/test/telemetry_test.exs`

---

### Task 2: Telemetry Logger (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/telemetry/logger.ex`:

```elixir
defmodule PortfolioIndex.Telemetry.Logger do
  @moduledoc """
  Human-readable telemetry logger for Portfolio events.
  Provides one-line setup for development and debugging.

  ## Usage

      # In application.ex or iex
      PortfolioIndex.Telemetry.Logger.attach()

      # With options
      PortfolioIndex.Telemetry.Logger.attach(
        level: :info,
        events: [:embedder, :vector_store],
        format: :json
      )

      # Detach when done
      PortfolioIndex.Telemetry.Logger.detach()
  """

  require Logger

  @type log_level :: :debug | :info | :warning | :error
  @type format :: :text | :json

  @type opts :: [
    level: log_level(),
    events: [atom()] | :all,
    format: format(),
    handler_id: atom()
  ]

  @default_handler_id :portfolio_telemetry_logger

  @doc """
  Attach the telemetry logger to all Portfolio events.
  """
  @spec attach(opts()) :: :ok | {:error, term()}
  def attach(opts \\ [])

  @doc """
  Detach the telemetry logger.
  """
  @spec detach(atom()) :: :ok | {:error, term()}
  def detach(handler_id \\ @default_handler_id)

  @doc """
  Check if logger is attached.
  """
  @spec attached?(atom()) :: boolean()
  def attached?(handler_id \\ @default_handler_id)

  @doc """
  Format an event for logging.
  """
  @spec format_event([atom()], map(), map(), format()) :: String.t()
  def format_event(event, measurements, metadata, format)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/telemetry/logger_test.exs`

---

### Task 3: LLM Telemetry Enrichment (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/telemetry/llm.ex`:

```elixir
defmodule PortfolioIndex.Telemetry.LLM do
  @moduledoc """
  LLM-specific telemetry utilities with enriched metadata.
  """

  @doc """
  Wrap an LLM call with telemetry, including token tracking.

  Metadata includes:
  - `:model` - Model identifier
  - `:prompt_length` - Character count of prompt
  - `:prompt_tokens` - Estimated token count (if available)
  - `:response_length` - Character count of response
  - `:response_tokens` - Estimated token count (if available)
  - `:provider` - LLM provider (openai, anthropic, etc.)
  """
  @spec span(keyword(), (-> result)) :: result when result: any()
  def span(metadata, fun)

  @doc """
  Estimate token count for text.
  Uses simple heuristic: ~4 chars per token for English.
  """
  @spec estimate_tokens(String.t()) :: non_neg_integer()
  def estimate_tokens(text)

  @doc """
  Extract token usage from LLM response if available.
  """
  @spec extract_usage(map()) :: map()
  def extract_usage(response)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/telemetry/llm_test.exs`

---

### Task 4: Embedder Telemetry (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/telemetry/embedder.ex`:

```elixir
defmodule PortfolioIndex.Telemetry.Embedder do
  @moduledoc """
  Embedder-specific telemetry utilities.
  """

  @doc """
  Wrap an embedding call with telemetry.

  Metadata includes:
  - `:model` - Embedding model identifier
  - `:dimensions` - Embedding vector dimensions
  - `:text_length` - Character count of input
  - `:batch_size` - Number of texts (for batch operations)
  - `:provider` - Embedder provider
  """
  @spec span(keyword(), (-> result)) :: result when result: any()
  def span(metadata, fun)

  @doc """
  Wrap a batch embedding call with telemetry.
  """
  @spec batch_span(keyword(), (-> result)) :: result when result: any()
  def batch_span(metadata, fun)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/telemetry/embedder_test.exs`

---

### Task 5: RAG Pipeline Telemetry (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/telemetry/rag.ex`:

```elixir
defmodule PortfolioIndex.Telemetry.RAG do
  @moduledoc """
  RAG pipeline telemetry for tracking each step.
  """

  alias PortfolioIndex.RAG.Pipeline.Context

  @pipeline_steps [:rewrite, :expand, :decompose, :select, :search, :rerank, :answer]

  @doc """
  Wrap a pipeline step with telemetry.

  Metadata includes:
  - `:step` - Pipeline step name
  - `:question` - Original question
  - `:context_state` - Summary of context state
  """
  @spec step_span(atom(), Context.t(), (Context.t() -> Context.t())) :: Context.t()
  def step_span(step, ctx, fun)

  @doc """
  Emit search-specific telemetry.

  Additional metadata:
  - `:result_count` - Number of results
  - `:collections` - Collections searched
  - `:mode` - Search mode (semantic, fulltext, hybrid)
  """
  @spec search_span(Context.t(), keyword(), (-> result)) :: result when result: any()
  def search_span(ctx, opts, fun)

  @doc """
  Emit rerank-specific telemetry.

  Additional metadata:
  - `:input_count` - Chunks before reranking
  - `:output_count` - Chunks after reranking
  - `:threshold` - Score threshold used
  """
  @spec rerank_span(Context.t(), keyword(), (-> result)) :: result when result: any()
  def rerank_span(ctx, opts, fun)

  @doc """
  Emit self-correction telemetry.

  Additional metadata:
  - `:correction_count` - Number of corrections
  - `:reason` - Reason for correction
  """
  @spec correction_event(Context.t(), String.t()) :: :ok
  def correction_event(ctx, reason)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/telemetry/rag_test.exs`

---

### Task 6: Vector Store Telemetry (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/telemetry/vector_store.ex`:

```elixir
defmodule PortfolioIndex.Telemetry.VectorStore do
  @moduledoc """
  Vector store telemetry utilities.
  """

  @doc """
  Wrap a search operation with telemetry.

  Metadata includes:
  - `:backend` - Vector store backend
  - `:collection` - Collection name (if applicable)
  - `:limit` - Requested result limit
  - `:result_count` - Actual results returned
  - `:mode` - Search mode (semantic, hybrid)
  """
  @spec search_span(keyword(), (-> result)) :: result when result: any()
  def search_span(metadata, fun)

  @doc """
  Wrap an insert operation with telemetry.
  """
  @spec insert_span(keyword(), (-> result)) :: result when result: any()
  def insert_span(metadata, fun)

  @doc """
  Wrap a batch insert operation with telemetry.
  """
  @spec batch_insert_span(keyword(), (-> result)) :: result when result: any()
  def batch_insert_span(metadata, fun)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/telemetry/vector_store_test.exs`

---

### Task 7: Integrate Telemetry into Existing Adapters

Update existing adapter files to use the new telemetry utilities. This involves modifying:

1. `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/embedder/*.ex`
   - Wrap `embed/2` and `embed_batch/2` with `Telemetry.Embedder.span/2`

2. `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/vector_store/*.ex`
   - Wrap `search/3` with `Telemetry.VectorStore.search_span/2`
   - Wrap `insert/4` with `Telemetry.VectorStore.insert_span/2`

3. `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/llm/*.ex`
   - Wrap `complete/2` with `Telemetry.LLM.span/2`

4. `/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/query_processor.ex` (from Prompt 1)
   - Wrap each step with `Telemetry.RAG.step_span/3`

---

## TDD Requirements

For each task:

1. **Write tests FIRST** following existing test patterns in the repos
2. Tests must cover:
   - Event emission with correct names
   - Metadata correctness
   - Exception handling in spans
   - Logger output format
   - Handler attachment/detachment
3. Run tests continuously: `mix test path/to/test_file.exs`

## Quality Gates

Before considering this prompt complete:

```bash
# In portfolio_core
cd /home/home/p/g/n/portfolio_core
mix test
mix credo --strict
mix dialyzer

# In portfolio_index
cd /home/home/p/g/n/portfolio_index
mix test
mix credo --strict
mix dialyzer
```

All must pass with zero warnings and zero errors.

## Documentation Updates

### portfolio_core
Update `/home/home/p/g/n/portfolio_core/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `PortfolioCore.Telemetry` - Telemetry event definitions and span utilities
```

### portfolio_index
Update `/home/home/p/g/n/portfolio_index/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `PortfolioIndex.Telemetry.Logger` - Human-readable telemetry logger with one-line attach
- `PortfolioIndex.Telemetry.LLM` - LLM-specific telemetry with token tracking
- `PortfolioIndex.Telemetry.Embedder` - Embedder telemetry utilities
- `PortfolioIndex.Telemetry.RAG` - RAG pipeline step telemetry
- `PortfolioIndex.Telemetry.VectorStore` - Vector store operation telemetry

### Changed
- All adapters now emit standardized telemetry events
```

## Verification Checklist

- [ ] All new files created in correct locations
- [ ] All tests pass in both repos
- [ ] No credo warnings
- [ ] No dialyzer errors
- [ ] Changelogs updated for both repos
- [ ] Module documentation complete
- [ ] Type specifications complete
- [ ] Existing adapters updated with telemetry
- [ ] Logger produces readable output
</file>

<file path="prompts/08_collection_selector_self_correction.md">
# Prompt 8: Collection Selector & Self-Correction Implementation

## Target Repositories
- **portfolio_core**: `/home/home/p/g/n/portfolio_core`
- **portfolio_index**: `/home/home/p/g/n/portfolio_index`

## Required Reading Before Implementation

### Reference Implementation (Arcana)
Read these files to understand the reference implementation:
```
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/context.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/selector.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/selector/llm.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/searcher.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/searcher/arcana.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/answerer.ex
/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent/answerer/llm.ex
```

### Existing Portfolio Code
Read these files to understand existing patterns:
```
/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/strategy.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/strategies/agentic.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/strategies/self_rag.ex
/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/reranker/llm.ex
/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/reranker.ex
```

### Gap Analysis Documentation
```
/home/home/p/g/n/portfolio_index/docs/20251230/arcana_gap_analysis/01_agent_system.md
/home/home/p/g/n/portfolio_index/docs/20251230/arcana_gap_analysis/05_llm_integration.md
```

### Prerequisite - Pipeline Context (from Prompt 1)
This prompt depends on the Pipeline Context from Prompt 1:
```
/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/pipeline/context.ex
```

---

## Implementation Tasks

### Task 1: Collection Selector Port (portfolio_core)

Create `/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/collection_selector.ex`:

```elixir
defmodule PortfolioCore.Ports.CollectionSelector do
  @moduledoc """
  Behaviour for selecting relevant collections/indexes to search.
  Enables intelligent routing of queries to appropriate data sources.
  """

  @type selection_result :: %{
    selected: [String.t()],
    reasoning: String.t() | nil,
    confidence: float() | nil
  }

  @type collection_info :: %{
    name: String.t(),
    description: String.t() | nil,
    document_count: non_neg_integer() | nil
  }

  @doc """
  Select relevant collections for a query.

  ## Parameters
  - `query` - The search query
  - `available_collections` - List of available collection info
  - `opts` - Options including `:max_collections`, `:llm`
  """
  @callback select(query :: String.t(), available_collections :: [collection_info()], opts :: keyword()) ::
    {:ok, selection_result()} | {:error, term()}
end
```

**Test file**: `/home/home/p/g/n/portfolio_core/test/ports/collection_selector_test.exs`

---

### Task 2: Collection Selector LLM Adapter (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/collection_selector/llm.ex`:

```elixir
defmodule PortfolioIndex.Adapters.CollectionSelector.LLM do
  @moduledoc """
  LLM-based collection selector that routes queries to relevant collections.
  Uses collection descriptions to determine relevance.
  """

  @behaviour PortfolioCore.Ports.CollectionSelector

  @default_prompt """
  You are a query router. Given a user query and available document collections,
  select the most relevant collections to search.

  User query: {query}

  Available collections:
  {collections}

  Return a JSON object with:
  - "selected": array of collection names to search (1-3 collections)
  - "reasoning": brief explanation of why these collections were selected

  Return ONLY the JSON, nothing else.
  """

  @impl true
  def select(query, available_collections, opts \\ [])

  @doc "Format collection info for prompt"
  @spec format_collections([map()]) :: String.t()
  def format_collections(collections)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/adapters/collection_selector/llm_test.exs`

---

### Task 3: Deterministic Collection Selector (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/collection_selector/rule_based.ex`:

```elixir
defmodule PortfolioIndex.Adapters.CollectionSelector.RuleBased do
  @moduledoc """
  Rule-based collection selector using keyword matching.
  Useful when LLM routing is not needed or for deterministic behavior.

  ## Configuration

      rules = [
        %{
          collection: "api_docs",
          keywords: ["api", "endpoint", "request", "response"],
          boost: 2.0
        },
        %{
          collection: "tutorials",
          keywords: ["how to", "guide", "tutorial", "example"],
          boost: 1.5
        }
      ]

      RuleBased.select(query, collections, rules: rules)
  """

  @behaviour PortfolioCore.Ports.CollectionSelector

  @impl true
  def select(query, available_collections, opts \\ [])

  @doc "Score a query against rules"
  @spec score_query(String.t(), [map()]) :: [{String.t(), float()}]
  def score_query(query, rules)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/adapters/collection_selector/rule_based_test.exs`

---

### Task 4: Self-Correcting Search Module (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/self_correcting_search.ex`:

```elixir
defmodule PortfolioIndex.RAG.SelfCorrectingSearch do
  @moduledoc """
  Search with self-correction loop that evaluates result sufficiency
  and rewrites queries when needed.

  ## Flow

  1. Execute initial search
  2. Evaluate if results are sufficient for answering the question
  3. If insufficient:
     a. Ask LLM to suggest a better query
     b. Execute new search
     c. Repeat until sufficient or max iterations
  4. Return results with correction history
  """

  alias PortfolioIndex.RAG.Pipeline.Context

  @type search_opts :: [
    max_iterations: pos_integer(),
    min_results: pos_integer(),
    sufficiency_prompt: String.t() | (String.t(), [map()] -> String.t()),
    rewrite_prompt: String.t() | (String.t(), [map()] -> String.t()),
    llm: (String.t() -> {:ok, String.t()} | {:error, term()}),
    search_fn: (String.t(), keyword() -> {:ok, [map()]} | {:error, term()})
  ]

  @doc """
  Execute self-correcting search.

  Returns context with results and correction history.
  """
  @spec search(Context.t(), search_opts()) :: Context.t()
  def search(ctx, opts \\ [])

  @doc """
  Evaluate if search results are sufficient.
  """
  @spec evaluate_sufficiency(String.t(), [map()], keyword()) :: {:ok, boolean(), String.t()} | {:error, term()}
  def evaluate_sufficiency(question, results, opts)

  @doc """
  Generate a rewritten query based on feedback.
  """
  @spec rewrite_query(String.t(), [map()], String.t(), keyword()) :: {:ok, String.t()} | {:error, term()}
  def rewrite_query(original_query, results, feedback, opts)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/rag/self_correcting_search_test.exs`

---

### Task 5: Self-Correcting Answer Module (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/self_correcting_answer.ex`:

```elixir
defmodule PortfolioIndex.RAG.SelfCorrectingAnswer do
  @moduledoc """
  Answer generation with grounding evaluation and correction loop.

  ## Flow

  1. Generate initial answer from context
  2. Evaluate if answer is grounded in the provided context
  3. If not grounded:
     a. Identify ungrounded claims
     b. Generate corrected answer
     c. Repeat until grounded or max iterations
  4. Return answer with correction history
  """

  alias PortfolioIndex.RAG.Pipeline.Context

  @type answer_opts :: [
    max_corrections: pos_integer(),
    grounding_threshold: float(),
    grounding_prompt: String.t() | (String.t(), String.t(), [map()] -> String.t()),
    correction_prompt: String.t() | (String.t(), String.t(), String.t() -> String.t()),
    llm: (String.t() -> {:ok, String.t()} | {:error, term()})
  ]

  @type grounding_result :: %{
    grounded: boolean(),
    score: float(),
    ungrounded_claims: [String.t()],
    feedback: String.t()
  }

  @doc """
  Generate answer with self-correction.
  """
  @spec answer(Context.t(), answer_opts()) :: Context.t()
  def answer(ctx, opts \\ [])

  @doc """
  Generate initial answer from context.
  """
  @spec generate_answer(String.t(), [map()], keyword()) :: {:ok, String.t()} | {:error, term()}
  def generate_answer(question, context_chunks, opts)

  @doc """
  Evaluate if answer is grounded in context.
  """
  @spec evaluate_grounding(String.t(), String.t(), [map()], keyword()) :: {:ok, grounding_result()} | {:error, term()}
  def evaluate_grounding(question, answer, context_chunks, opts)

  @doc """
  Generate corrected answer based on grounding feedback.
  """
  @spec correct_answer(String.t(), String.t(), grounding_result(), [map()], keyword()) :: {:ok, String.t()} | {:error, term()}
  def correct_answer(question, original_answer, grounding_result, context_chunks, opts)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/rag/self_correcting_answer_test.exs`

---

### Task 6: Reranker Integration Module (portfolio_index)

Create `/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/reranker.ex`:

```elixir
defmodule PortfolioIndex.RAG.Reranker do
  @moduledoc """
  Reranking utilities for RAG pipeline integration.
  Wraps the existing Reranker.LLM adapter with pipeline-aware functionality.
  """

  alias PortfolioIndex.RAG.Pipeline.Context
  alias PortfolioIndex.Adapters.Reranker.LLM, as: RerankerLLM

  @type rerank_opts :: [
    threshold: float(),
    limit: pos_integer(),
    reranker: module() | (String.t(), [map()] -> {:ok, [map()]} | {:error, term()}),
    track_scores: boolean()
  ]

  @doc """
  Rerank search results in pipeline context.

  Updates context with:
  - Reranked and filtered results
  - Rerank scores (if track_scores: true)
  """
  @spec rerank(Context.t(), rerank_opts()) :: Context.t()
  def rerank(ctx, opts \\ [])

  @doc """
  Rerank a list of chunks directly.
  """
  @spec rerank_chunks(String.t(), [map()], rerank_opts()) :: {:ok, [map()]} | {:error, term()}
  def rerank_chunks(question, chunks, opts \\ [])

  @doc """
  Deduplicate chunks by content or ID.
  """
  @spec deduplicate([map()], atom()) :: [map()]
  def deduplicate(chunks, key \\ :id)
end
```

**Test file**: `/home/home/p/g/n/portfolio_index/test/rag/reranker_test.exs`

---

### Task 7: Enhanced Agentic Strategy (portfolio_index)

Update `/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/strategies/agentic.ex` to integrate all new components:

```elixir
# Add these new functions to the existing Agentic strategy module:

@doc """
Execute full agentic pipeline with all enhancements.

Pipeline steps:
1. Query rewriting (clean conversational input)
2. Query expansion (add synonyms)
3. Query decomposition (break complex questions)
4. Collection selection (route to relevant collections)
5. Self-correcting search (iterate until sufficient)
6. Reranking (score and filter results)
7. Self-correcting answer (ensure grounding)
"""
@spec execute_pipeline(String.t(), keyword()) :: {:ok, map()} | {:error, term()}
def execute_pipeline(question, opts \\ [])

@doc """
Execute pipeline with Context struct.
Enables functional composition with pipe operator.
"""
@spec with_context(Context.t()) :: Context.t()
def with_context(ctx)
```

**Test file**: Update `/home/home/p/g/n/portfolio_index/test/rag/strategies/agentic_test.exs`

---

## TDD Requirements

For each task:

1. **Write tests FIRST** following existing test patterns in the repos
2. Tests must cover:
   - Happy path with valid input
   - LLM mock responses
   - Error handling (LLM failures, empty results)
   - Iteration limits
   - Correction history tracking
   - Integration with Pipeline Context
3. Run tests continuously: `mix test path/to/test_file.exs`

## Quality Gates

Before considering this prompt complete:

```bash
# In portfolio_core
cd /home/home/p/g/n/portfolio_core
mix test
mix credo --strict
mix dialyzer

# In portfolio_index
cd /home/home/p/g/n/portfolio_index
mix test
mix credo --strict
mix dialyzer
```

All must pass with zero warnings and zero errors.

## Documentation Updates

### portfolio_core
Update `/home/home/p/g/n/portfolio_core/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `PortfolioCore.Ports.CollectionSelector` behaviour for query routing
```

### portfolio_index
Update `/home/home/p/g/n/portfolio_index/CHANGELOG.md` - add entry to version 0.3.1:
```markdown
### Added
- `PortfolioIndex.Adapters.CollectionSelector.LLM` - LLM-based collection routing
- `PortfolioIndex.Adapters.CollectionSelector.RuleBased` - Rule-based collection routing
- `PortfolioIndex.RAG.SelfCorrectingSearch` - Search with sufficiency evaluation and query rewriting
- `PortfolioIndex.RAG.SelfCorrectingAnswer` - Answer generation with grounding evaluation
- `PortfolioIndex.RAG.Reranker` - Pipeline-integrated reranking utilities

### Changed
- `PortfolioIndex.RAG.Strategies.Agentic` - Added full pipeline execution with all enhancements
```

## Verification Checklist

- [ ] All new files created in correct locations
- [ ] All tests pass in both repos
- [ ] No credo warnings
- [ ] No dialyzer errors
- [ ] Changelogs updated for both repos
- [ ] Module documentation complete
- [ ] Type specifications complete
- [ ] Integration with Pipeline Context verified
- [ ] Agentic strategy enhanced with new components
</file>

<file path="prompts/run_prompts.sh">
#!/usr/bin/env bash
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../../../../.." && pwd)"
PROMPTS_DIR="$SCRIPT_DIR"
LOG_DIR="$PROJECT_ROOT/logs/arcana-gap-impl-$(date +%Y%m%d-%H%M%S)"

START_NUM=1
END_NUM=8

if ! command -v claude >/dev/null 2>&1; then
  echo "[ERROR] claude is not on PATH"
  exit 1
fi

mkdir -p "$LOG_DIR"
cd "$PROJECT_ROOT"

echo "Arcana Gap Analysis Implementation Runner"
echo "Project root: $PROJECT_ROOT"
echo "Prompts dir:  $PROMPTS_DIR"
echo "Log dir:      $LOG_DIR"
echo "Running prompts: $(printf '%02d' $START_NUM) through $(printf '%02d' $END_NUM)"
echo "Mode: AUTONOMOUS (--dangerously-skip-permissions)"
echo ""

SUCCESSFUL=()
FAILED=()

run_prompt() {
  local prompt_file="$1"
  local prompt_name
  local log_file

  prompt_name="$(basename "$prompt_file" .md)"
  log_file="$LOG_DIR/${prompt_name}.log"

  echo ""
  echo "------------------------------------------------------------"
  echo "[$(date '+%H:%M:%S')] Starting: $prompt_name"
  echo "------------------------------------------------------------"
  echo ""
  echo "[Starting Claude Code for $prompt_name...]"
  echo ""

  if command -v stdbuf >/dev/null 2>&1; then
    if cat "$prompt_file" | stdbuf -oL -eL claude --dangerously-skip-permissions --verbose -p --output-format stream-json 2>&1 | tee "$log_file"; then
      SUCCESSFUL+=("$prompt_name")
      return 0
    else
      FAILED+=("$prompt_name")
      return 1
    fi
  else
    if cat "$prompt_file" | claude --dangerously-skip-permissions --verbose -p --output-format stream-json 2>&1 | tee "$log_file"; then
      SUCCESSFUL+=("$prompt_name")
      return 0
    else
      FAILED+=("$prompt_name")
      return 1
    fi
  fi
}

for i in $(seq -f "%02g" "$START_NUM" "$END_NUM"); do
  prompt_file="$PROMPTS_DIR/${i}_"*.md

  if ! ls $prompt_file >/dev/null 2>&1; then
    echo "[WARN] Prompt ${i} not found, skipping"
    continue
  fi

  prompt_file=$(ls $prompt_file)
  if run_prompt "$prompt_file"; then
    echo ""
    echo "[$(date '+%H:%M:%S')] Completed: $(basename "$prompt_file" .md)"
  else
    echo ""
    echo "[ERROR] $(basename "$prompt_file") failed (see log in $LOG_DIR)"
  fi
done

echo ""
echo "Summary - $(date)"

if [ ${#SUCCESSFUL[@]} -gt 0 ]; then
  echo "Successful (${#SUCCESSFUL[@]}):"
  for name in "${SUCCESSFUL[@]}"; do
    echo "  - $name"
  done
fi

if [ ${#FAILED[@]} -gt 0 ]; then
  echo ""
  echo "Failed (${#FAILED[@]}):"
  for name in "${FAILED[@]}"; do
    echo "  - $name"
  done
fi

echo ""
echo "All logs saved to: $LOG_DIR"

if [ ${#FAILED[@]} -gt 0 ]; then
  exit 1
fi

exit 0
</file>

<file path="00_consolidated_summary.md">
# Arcana Gap Analysis - Consolidated Summary

## Overview

This document consolidates all findings from the Arcana vs Portfolio libraries gap analysis across 8 analysis areas. It identifies **~72 individual gaps** across agent systems, embedders, vector stores, evaluation, LLM integration, telemetry, mix tasks, and maintenance/documents.

## Analysis Documents

1. **01_agent_system.md** - Agent pipeline, query processing, behaviours (10 gaps)
2. **02_embedder_system.md** - Embedding providers, local models, configuration (10 gaps)
3. **03_vector_store.md** - Memory backend, collections, auto-creation (15 gaps, 4 reverse)
4. **04_evaluation_system.md** - IR metrics, test cases, synthetic generation (12 gaps, 4 reverse)
5. **05_llm_integration.md** - Query rewriting, context handling, prompts (10 gaps)
6. **06_telemetry.md** - Spans, logging, pipeline observability (12 gaps)
7. **07_mix_tasks.md** - Installation, re-embedding, evaluation CLI (8 gaps)
8. **08_maintenance_and_documents.md** - Schemas, status tracking, parser (14 gaps)

---

## Priority 1: High-Impact, Foundation Gaps

These gaps should be implemented first as they provide the foundation for other features.

### P1-1: Pipeline Context Object
- **From**: 01_agent_system.md (Gap 7)
- **Description**: `Context` struct that flows through RAG pipeline tracking all intermediate results
- **Repo**: portfolio_index
- **Complexity**: Medium
- **Enables**: Query processing, observability, debugging

### P1-2: Query Rewriting
- **From**: 01_agent_system.md (Gap 1), 05_llm_integration.md (Gap 1)
- **Description**: LLM-based cleaning of conversational input into search queries
- **Repo**: portfolio_index, portfolio_core (port)
- **Complexity**: Low
- **Enables**: Better search quality

### P1-3: Query Expansion
- **From**: 01_agent_system.md (Gap 2), 05_llm_integration.md (Gap 2)
- **Description**: Adding synonyms, related terms for better retrieval recall
- **Repo**: portfolio_index, portfolio_core (port)
- **Complexity**: Low
- **Enables**: Improved recall

### P1-4: Query Decomposition
- **From**: 01_agent_system.md (Gap 3), 05_llm_integration.md (Gap 3)
- **Description**: Breaking complex questions into simpler sub-questions
- **Repo**: portfolio_index, portfolio_core (port)
- **Complexity**: Medium
- **Enables**: Multi-hop retrieval

### P1-5: Document/Chunk Ecto Schemas
- **From**: 08_maintenance_and_documents.md (Gaps 2, 3, 4)
- **Description**: Ecto schemas for documents, collections, chunks with status tracking
- **Repo**: portfolio_index
- **Complexity**: Medium
- **Enables**: Maintenance, retry logic, status tracking

### P1-6: Production Maintenance Utilities
- **From**: 08_maintenance_and_documents.md (Gap 1), 07_mix_tasks.md (Gaps 1, 2, 3)
- **Description**: Re-embed, embedding migration, installation tasks
- **Repo**: portfolio_index, portfolio_manager
- **Complexity**: Medium
- **Enables**: Production operations

---

## Priority 2: Core RAG Enhancements

### P2-1: Retrieval Quality Metrics (IR Metrics)
- **From**: 04_evaluation_system.md (Gap 3)
- **Description**: Recall@K, Precision@K, MRR, Hit Rate@K for test cases
- **Repo**: portfolio_index, portfolio_core (port)
- **Complexity**: Low-Medium

### P2-2: Test Case Persistence & Generation
- **From**: 04_evaluation_system.md (Gaps 1, 2)
- **Description**: Ecto schemas for test cases, LLM-based synthetic generation
- **Repo**: portfolio_index
- **Complexity**: Medium

### P2-3: Self-Correcting Search
- **From**: 01_agent_system.md (Gap 5)
- **Description**: Search with sufficiency evaluation and query rewriting loop
- **Repo**: portfolio_index
- **Complexity**: Medium

### P2-4: Collection/Index Selection (Routing)
- **From**: 01_agent_system.md (Gap 4), 05_llm_integration.md (Gap 4)
- **Description**: LLM-based routing to relevant collections
- **Repo**: portfolio_index
- **Complexity**: Medium

### P2-5: Local Bumblebee Embeddings
- **From**: 02_embedder_system.md (Gap 1)
- **Description**: Nx.Serving + Bumblebee for local HuggingFace models
- **Repo**: portfolio_index
- **Complexity**: High

### P2-6: OpenAI Embeddings Implementation
- **From**: 02_embedder_system.md (Gap 2)
- **Description**: Complete the placeholder OpenAI adapter
- **Repo**: portfolio_index
- **Complexity**: Low

### P2-7: In-Memory Vector Store
- **From**: 03_vector_store.md (Gap 1)
- **Description**: HNSWLib-based in-memory store for testing
- **Repo**: portfolio_index
- **Complexity**: Medium

---

## Priority 3: Observability & Developer Experience

### P3-1: Built-in Telemetry Logger
- **From**: 06_telemetry.md (Gap 2)
- **Description**: Human-readable telemetry logging with one-line attach
- **Repo**: portfolio_index
- **Complexity**: Low

### P3-2: Span-Based Telemetry
- **From**: 06_telemetry.md (Gap 1)
- **Description**: Use :telemetry.span for all operations
- **Repo**: portfolio_index
- **Complexity**: Medium

### P3-3: LLM Telemetry Enrichment
- **From**: 06_telemetry.md (Gap 3)
- **Description**: Track model, prompt length, response length in events
- **Repo**: portfolio_index
- **Complexity**: Low

### P3-4: Agent Pipeline Telemetry
- **From**: 06_telemetry.md (Gap 4)
- **Description**: Events for each RAG pipeline step
- **Repo**: portfolio_index
- **Complexity**: Medium

### P3-5: Exception Event Consistency
- **From**: 06_telemetry.md (Gap 8)
- **Description**: All operations emit exception events
- **Repo**: portfolio_index
- **Complexity**: Medium

### P3-6: Evaluation CLI Tasks
- **From**: 07_mix_tasks.md (Gaps 4, 5), 04_evaluation_system.md (Gap 5)
- **Description**: mix tasks for eval generate and eval run
- **Repo**: portfolio_manager
- **Complexity**: Low

---

## Priority 4: Extended Features

### P4-1: Reranker Integration in Strategies
- **From**: 01_agent_system.md (Gap 8)
- **Description**: Wire up existing Reranker.LLM into RAG strategies
- **Repo**: portfolio_index
- **Complexity**: Low

### P4-2: Self-Correcting Answers
- **From**: 01_agent_system.md (Gap 6)
- **Description**: Grounding evaluation with correction loop
- **Repo**: portfolio_index
- **Complexity**: Medium

### P4-3: Customizable Prompt Functions
- **From**: 05_llm_integration.md (Gap 9)
- **Description**: :prompt option for all LLM components
- **Repo**: portfolio_index
- **Complexity**: Low

### P4-4: Unified Pipeline Context (Extension)
- **From**: 05_llm_integration.md (Gap 7)
- **Description**: Extend Generation struct with query transformations
- **Repo**: portfolio_manager
- **Complexity**: Medium

### P4-5: File Parser with PDF Support
- **From**: 08_maintenance_and_documents.md (Gap 6)
- **Description**: Parse text/markdown/PDF with format detection
- **Repo**: portfolio_index
- **Complexity**: Low

### P4-6: Backend Override at Runtime
- **From**: 03_vector_store.md (Gap 3)
- **Description**: Per-call backend switching for vector store
- **Repo**: portfolio_index
- **Complexity**: Medium

---

## Grouped Implementation Prompts

Based on the gaps, the following prompt files should be created:

### Prompt 1: Query Processing Pipeline
- Query Rewriter port + adapter
- Query Expander port + adapter
- Query Decomposer port + adapter
- Pipeline Context struct
- **Repos**: portfolio_core, portfolio_index

### Prompt 2: Document Management Schemas
- Document Ecto schema
- Collection Ecto schema
- Chunk Ecto schema with pgvector
- Migrations
- **Repo**: portfolio_index

### Prompt 3: Production Maintenance
- Maintenance module (reembed, diagnostics)
- mix portfolio.install task
- mix portfolio.gen.embedding_migration task
- mix portfolio.reembed task
- **Repos**: portfolio_index, portfolio_manager

### Prompt 4: Evaluation System
- Retrieval metrics port + adapter
- Test case schema and generation
- Evaluation run tracking
- mix portfolio.eval.generate/run tasks
- **Repos**: portfolio_core, portfolio_index, portfolio_manager

### Prompt 5: Embedder Enhancements
- OpenAI adapter implementation
- Local Bumblebee adapter
- Custom function embedder
- Model dimension registry
- **Repo**: portfolio_index

### Prompt 6: Vector Store Enhancements
- In-memory HNSWLib adapter
- Backend override pattern
- Auto index creation
- **Repo**: portfolio_index

### Prompt 7: Telemetry Standardization
- Telemetry.Logger module
- Span-based instrumentation
- LLM telemetry enrichment
- Agent pipeline events
- **Repos**: portfolio_index, portfolio_core

### Prompt 8: Collection Selector & Self-Correction
- Collection selector port + adapter
- Self-correcting search
- Self-correcting answers
- Reranker integration in strategies
- **Repo**: portfolio_index

---

## Gap Count by Area

| Analysis Area | Forward Gaps | Reverse Gaps | Total |
|---------------|-------------|--------------|-------|
| Agent System | 10 | 0 | 10 |
| Embedder System | 8 | 2 | 10 |
| Vector Store | 9 | 6 | 15 |
| Evaluation System | 12 | 4 | 16 |
| LLM Integration | 10 | 0 | 10 |
| Telemetry | 12 | 0 | 12 |
| Mix Tasks | 8 | 0 | 8 |
| Maintenance/Documents | 14 | 0 | 14 |
| **TOTAL** | **83** | **12** | **95** |

*Note: Forward gaps = Arcana features missing from Portfolio. Reverse gaps = Portfolio features missing from Arcana.*

---

## Recommended Implementation Order

1. **Sprint 1**: Query Processing (P1-1 through P1-4)
2. **Sprint 2**: Document Schemas + Maintenance (P1-5, P1-6)
3. **Sprint 3**: Evaluation System (P2-1, P2-2)
4. **Sprint 4**: Embedder Enhancements (P2-5, P2-6)
5. **Sprint 5**: Telemetry + Observability (P3-1 through P3-5)
6. **Sprint 6**: Vector Store + Collection Selection (P2-4, P2-7, P4-6)
7. **Sprint 7**: Self-Correction Features (P2-3, P4-2)
8. **Sprint 8**: Extended Features (P4-1, P4-3, P4-4, P4-5)
</file>

<file path="01_agent_system.md">
# Agent System Gap Analysis

## Overview

This document compares the Arcana RAG agent system against the portfolio libraries (portfolio_core, portfolio_manager, portfolio_index) to identify gaps in RAG pipeline functionality.

## Arcana Agent Capabilities

### Architecture Overview

Arcana implements a **pipeline-based agentic RAG** system where a `Context` struct flows through each transformation step. The architecture follows a functional, composable pattern:

```elixir
Arcana.Agent.new(question, llm: llm_fn)
|> Arcana.Agent.rewrite()      # Clean conversational input
|> Arcana.Agent.expand()       # Add synonyms/related terms
|> Arcana.Agent.decompose()    # Break into sub-questions
|> Arcana.Agent.select()       # Choose collections to search
|> Arcana.Agent.search()       # Execute search with self-correction
|> Arcana.Agent.rerank()       # Re-score and filter results
|> Arcana.Agent.answer()       # Generate answer with self-correction
```

### Component Details

#### 1. Context (`Arcana.Agent.Context`)
- Central data structure that flows through pipeline
- Tracks all intermediate results: rewritten_query, expanded_query, sub_questions, collections, results, rerank_scores, answer, context_used
- Includes error handling and correction history

#### 2. Query Rewriter (`Arcana.Agent.Rewriter`)
- Behaviour + LLM implementation (`Arcana.Agent.Rewriter.LLM`)
- Removes conversational noise (greetings, filler phrases)
- Extracts core question while preserving technical terms
- Custom prompt support via `:prompt` option

#### 3. Query Expander (`Arcana.Agent.Expander`)
- Behaviour + LLM implementation (`Arcana.Agent.Expander.LLM`)
- Adds synonyms, related terms, abbreviation expansions
- Example: "ML models" -> "ML machine learning models"
- Custom expander modules or inline functions supported

#### 4. Query Decomposer (`Arcana.Agent.Decomposer`)
- Behaviour + LLM implementation (`Arcana.Agent.Decomposer.LLM`)
- Breaks complex questions into 2-4 simpler sub-questions
- Each sub-question searched independently
- Improves retrieval for multi-faceted queries

#### 5. Collection Selector (`Arcana.Agent.Selector`)
- Behaviour + LLM implementation (`Arcana.Agent.Selector.LLM`)
- LLM-based routing to select relevant collections
- Fetches collection descriptions from database for context
- Supports deterministic routing via custom selectors
- Returns reasoning for selection decisions

#### 6. Searcher (`Arcana.Agent.Searcher`)
- Behaviour + Arcana implementation (`Arcana.Agent.Searcher.Arcana`)
- Self-correcting search with configurable iterations
- Asks LLM if results are sufficient
- Rewrites query and retries if insufficient
- Supports multiple collections and sub-questions

#### 7. Reranker (`Arcana.Agent.Reranker`)
- Behaviour + LLM implementation (`Arcana.Agent.Reranker.LLM`)
- Scores each chunk 0-10 based on relevance
- Filters by threshold (default 7)
- Re-sorts by score
- Tracks scores for observability

#### 8. Answerer (`Arcana.Agent.Answerer`)
- Behaviour + LLM implementation (`Arcana.Agent.Answerer.LLM`)
- Generates answers from retrieved context
- Self-correcting answers with grounding evaluation
- Tracks correction history and count
- Custom prompt support

### Key Features

1. **Telemetry Integration**: All steps emit telemetry events via `:telemetry.span`
2. **Behaviour-based Extensibility**: Each component has a behaviour for custom implementations
3. **Error Propagation**: Context carries errors through pipeline, short-circuiting subsequent steps
4. **Self-Correction Loops**: Both search and answer steps support iterative improvement

---

## Portfolio Libraries Current State

### portfolio_core

**File**: `/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/agent.ex`

Defines the `PortfolioCore.Ports.Agent` behaviour with:
- `run/2` - Execute agent task
- `available_tools/0` - List tool specifications
- `execute_tool/1` - Execute a tool call
- `max_iterations/0` - Return max iterations
- `get_state/0` - Get current agent state (optional)
- `process/3` - Process input within session
- `process_with_tools/4` - Process with tool execution

**Type definitions** for:
- tool_spec, parameter_spec, tool_call, tool_result
- agent_state, message, session

### portfolio_manager

**File**: `/home/home/p/g/n/portfolio_manager/lib/portfolio_manager/agent.ex`

Implements a **tool-using agent** for code analysis:
- Session-based conversation management
- Tool execution loop with max iterations
- Built-in tools: `search_code`, `read_file`, `list_files`, `get_graph_context`
- JSON parsing for tool calls and final answers
- Memory tracking across iterations

**File**: `/home/home/p/g/n/portfolio_manager/lib/portfolio_manager/agent/session.ex`

Session management with:
- Message history with timestamps
- Tool results tracking
- Context and metadata storage
- LLM message formatting
- Token estimation

**File**: `/home/home/p/g/n/portfolio_manager/lib/portfolio_manager/agent/tool.ex`

Tool framework with:
- Tool behaviour definition
- Built-in tools (search_code, read_file, list_files, get_graph_context)
- Argument validation
- LLM prompt formatting

### portfolio_index RAG Strategies

**File**: `/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/strategies/hybrid.ex`
- Combines vector and keyword search
- Reciprocal Rank Fusion (RRF) for result merging
- No query preprocessing

**File**: `/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/strategies/graph_rag.ex`
- Entity extraction from queries
- Graph traversal for related entities
- Local/Global/Hybrid search modes
- Community-based search

**File**: `/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/strategies/agentic.ex`
- Tool-based iterative retrieval
- Tools: semantic_search, keyword_search, get_context
- Agent loop with max iterations
- Basic tool call parsing

**File**: `/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/strategies/self_rag.ex`
- Retrieval need assessment
- Self-critique with Relevance/Support/Completeness scores
- Answer refinement based on critique

### portfolio_index Reranker Adapters

**File**: `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/reranker/llm.ex`
- LLM-based document reranking
- Scores 1-10, normalizes to 0-1
- Custom prompt templates
- Fallback to passthrough on failure

**File**: `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/reranker/passthrough.ex`
- No-op reranker for testing

---

## Identified Gaps

### Gap 1: Query Rewriting

- **Arcana Feature**: Dedicated `Rewriter` behaviour with LLM implementation that removes conversational noise while preserving technical terms. Configurable prompts, telemetry integration, inline function support.
- **Missing From**: `portfolio_index` RAG strategies
- **Implementation Complexity**: Low
- **Technical Details**:
  - Create `PortfolioCore.Ports.QueryRewriter` behaviour
  - Implement `PortfolioIndex.Adapters.QueryRewriter.LLM` adapter
  - Add rewrite step to RAG pipeline before embedding

### Gap 2: Query Expansion

- **Arcana Feature**: `Expander` behaviour with LLM implementation that adds synonyms, related terms, and expands abbreviations/acronyms.
- **Missing From**: `portfolio_index` RAG strategies
- **Implementation Complexity**: Low
- **Technical Details**:
  - Create `PortfolioCore.Ports.QueryExpander` behaviour
  - Implement `PortfolioIndex.Adapters.QueryExpander.LLM` adapter
  - Can also add thesaurus-based or embedding-based expansion

### Gap 3: Query Decomposition

- **Arcana Feature**: `Decomposer` behaviour that breaks complex questions into 2-4 simpler sub-questions. Each sub-question is searched independently, improving retrieval for multi-faceted queries.
- **Missing From**: `portfolio_index` RAG strategies
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Create `PortfolioCore.Ports.QueryDecomposer` behaviour
  - Implement `PortfolioIndex.Adapters.QueryDecomposer.LLM` adapter
  - Modify search to handle multiple queries and merge results
  - GraphRAG already extracts entities but doesn't decompose to sub-questions

### Gap 4: Collection/Index Selection (LLM-based Routing)

- **Arcana Feature**: `Selector` behaviour with LLM implementation that:
  - Fetches collection descriptions from database
  - Uses LLM to select most relevant collections
  - Returns reasoning for decisions
  - Supports deterministic routing via custom selectors
- **Missing From**: All portfolio libraries
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Create `PortfolioCore.Ports.IndexSelector` behaviour
  - Implement `PortfolioIndex.Adapters.IndexSelector.LLM` adapter
  - Add collection/index metadata storage
  - Integrate with RAG strategies for multi-index scenarios

### Gap 5: Self-Correcting Search

- **Arcana Feature**: Search with self-correction loop:
  1. Execute search
  2. LLM evaluates if results are sufficient
  3. If not, LLM rewrites query
  4. Repeat until sufficient or max iterations
- **Missing From**: `portfolio_index` Agentic strategy has iteration but no sufficiency check or query rewriting
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Add sufficiency evaluation to search loop
  - Implement query rewriting on insufficient results
  - Track iteration count and final query used
  - Configurable max_iterations and custom prompts

### Gap 6: Self-Correcting Answers (Grounding Evaluation)

- **Arcana Feature**: Answer generation with grounding evaluation:
  1. Generate answer
  2. LLM evaluates if answer is grounded in context
  3. If not, LLM provides feedback
  4. Generate corrected answer based on feedback
  5. Track correction history
- **Missing From**: `portfolio_index/self_rag.ex` has self-critique but not grounding-based correction
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Current SelfRAG has Relevance/Support/Completeness scores
  - Add explicit grounding evaluation (claims vs context)
  - Add correction loop with feedback-based regeneration
  - Track correction history for debugging

### Gap 7: Pipeline Context Object

- **Arcana Feature**: `Context` struct that flows through entire pipeline, accumulating:
  - Original question
  - Rewritten query
  - Expanded query
  - Sub-questions
  - Selected collections
  - Search results
  - Rerank scores
  - Final answer
  - Context used
  - Correction history
  - Errors
- **Missing From**: Portfolio strategies return flat result maps
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Create `PortfolioIndex.RAG.Context` struct
  - Modify strategies to work with context
  - Enable pipeline composition with `|>` operator
  - Add error propagation through pipeline

### Gap 8: Unified Reranker Integration in Agent Pipeline

- **Arcana Feature**: Reranker is a first-class pipeline step with:
  - Threshold filtering (default 7/10)
  - Score tracking in context
  - Telemetry integration
  - Chunk deduplication by ID
- **Missing From**: `portfolio_index` has `Reranker.LLM` adapter but it's not integrated into strategies
- **Implementation Complexity**: Low
- **Technical Details**:
  - Add rerank step to RAG strategies
  - Wire up existing `Reranker.LLM` adapter
  - Add threshold configuration
  - Track scores in result metadata

### Gap 9: Behaviour-based Component Extensibility

- **Arcana Feature**: Every pipeline component (Rewriter, Expander, Decomposer, Selector, Searcher, Reranker, Answerer) has:
  - A behaviour definition
  - A default LLM implementation
  - Support for custom modules
  - Support for inline functions
- **Missing From**: Portfolio adapters exist but not as composable pipeline components
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Create behaviours for each component in `portfolio_core/ports/`
  - Implement LLM-based adapters in `portfolio_index/adapters/`
  - Add inline function support for quick customization
  - Enable mix-and-match of implementations

### Gap 10: Telemetry Spans for Pipeline Steps

- **Arcana Feature**: Each step wrapped in `:telemetry.span` with:
  - Start metadata (question, component name)
  - Stop metadata (results, counts)
  - Consistent event naming ([:arcana, :agent, :step_name])
- **Missing From**: Some portfolio strategies emit telemetry but inconsistently
- **Implementation Complexity**: Low
- **Technical Details**:
  - Standardize telemetry event names across portfolio libraries
  - Use `:telemetry.span` for all pipeline steps
  - Include duration, token usage, result counts

---

## Implementation Priority

### Priority 1 - High Value, Low Complexity
1. **Query Rewriting** - Immediate improvement to query quality
2. **Query Expansion** - Better recall without complex changes
3. **Reranker Integration** - Already implemented, needs wiring

### Priority 2 - High Value, Medium Complexity
4. **Pipeline Context Object** - Enables composition and debugging
5. **Self-Correcting Search** - Significant quality improvement
6. **Collection/Index Selection** - Essential for multi-collection scenarios

### Priority 3 - Medium Value, Medium Complexity
7. **Query Decomposition** - Helps with complex queries
8. **Self-Correcting Answers** - Quality improvement for answers
9. **Behaviour-based Extensibility** - Long-term maintainability

### Priority 4 - Foundation/Infrastructure
10. **Telemetry Spans** - Observability, can be added incrementally

---

## Technical Dependencies

### Dependency Graph

```
Telemetry Spans (standalone)
    |
Pipeline Context Object
    |
    +-- Query Rewriting
    +-- Query Expansion
    +-- Query Decomposition
    +-- Collection Selection
    +-- Self-Correcting Search
    +-- Reranker Integration
    +-- Self-Correcting Answers
    |
Behaviour-based Extensibility (refactoring)
```

### Implementation Order

1. **Phase 1: Foundation**
   - Create `PortfolioIndex.RAG.Pipeline.Context` struct
   - Add telemetry span wrapper utility

2. **Phase 2: Query Processing**
   - Implement Query Rewriter port + LLM adapter
   - Implement Query Expander port + LLM adapter
   - Implement Query Decomposer port + LLM adapter

3. **Phase 3: Search Enhancement**
   - Implement Collection Selector port + LLM adapter
   - Add self-correcting search loop to Agentic strategy
   - Integrate Reranker into strategy pipelines

4. **Phase 4: Answer Quality**
   - Add grounding evaluation to SelfRAG
   - Implement answer correction loop

5. **Phase 5: Refactoring**
   - Standardize all components as behaviours
   - Add inline function support throughout
   - Unified telemetry across all libraries

---

## Summary

The Arcana agent system provides a mature, well-architected RAG pipeline with:
- 7 distinct pipeline components, each with behaviour + implementation
- Self-correcting loops for both search and answer generation
- LLM-based query preprocessing (rewrite, expand, decompose)
- LLM-based collection routing
- Comprehensive telemetry
- Error propagation through pipeline context

The portfolio libraries have foundational pieces:
- Tool-based agents in portfolio_manager
- RAG strategies in portfolio_index (Hybrid, GraphRAG, Agentic, SelfRAG)
- Reranker adapters (LLM, Passthrough)
- Port behaviours in portfolio_core

**10 gaps identified**, with the most impactful being:
1. Query preprocessing (rewrite, expand, decompose)
2. Self-correcting search
3. Collection selection/routing
4. Pipeline context for composition

These gaps can be addressed incrementally, with low-complexity items providing immediate value while building toward the full pipeline architecture.
</file>

<file path="02_embedder_system.md">
# Embedder System Gap Analysis

## Arcana Embedder Capabilities

Arcana provides a comprehensive embedding system with the following components:

### Core Behaviour (`Arcana.Embedder`)
- **Callback-based architecture**: Defines `embed/2`, `embed_batch/2`, and `dimensions/1` callbacks
- **Optional batch callback**: `embed_batch/2` is optional with automatic sequential fallback
- **Unified dispatch**: Helper functions `embed/2`, `embed_batch/2`, and `dimensions/1` that accept `{module, opts}` tuples
- **Flexible configuration**: Supports atoms (`:local`, `:openai`), tuples with options, custom functions, and custom modules

### Local Embedding Provider (`Arcana.Embedder.Local`)
- **Bumblebee/EXLA integration**: Uses Nx.Serving for batched inference
- **HuggingFace model support**: Loads models directly from HuggingFace Hub
- **Pre-configured model dimensions**: Knows dimensions for BGE, E5, GTE, and MiniLM models
- **Automatic dimension detection**: Falls back to runtime detection for unknown models
- **Supervision tree integration**: Provides `child_spec/1` for supervised serving
- **Configurable compilation**: Batch size 32, sequence length 512, EXLA compiler
- **Telemetry instrumentation**: `[:arcana, :embed]` span with text, model, and dimensions metadata

### OpenAI Provider (`Arcana.Embedder.OpenAI`)
- **ReqLLM integration**: Uses ReqLLM for API calls
- **Lazy dependency loading**: Checks for ReqLLM at runtime with helpful error messages
- **Model dimension mapping**: text-embedding-3-small (1536), text-embedding-3-large (3072), ada-002 (1536)
- **Automatic dimension detection**: Runtime detection for unknown models
- **Telemetry instrumentation**: Same span pattern as local embedder

### Custom Provider (`Arcana.Embedder.Custom`)
- **Function wrapper**: Wraps user-provided functions to implement the behaviour
- **Flexible function signature**: Accepts functions returning `{:ok, embedding}` or `{:error, reason}`
- **Optional dimension specification**: Can provide dimensions via opts or detect at runtime
- **Error handling**: Validates embedding result format

### Legacy Serving (`Arcana.Embeddings.Serving`)
- **Standalone Nx.Serving**: Simplified serving without behaviour pattern
- **Fixed model**: BAAI/bge-small-en-v1.5 (384 dimensions)
- **Direct API**: `embed/1` and `embed_batch/1` functions
- **Telemetry**: Same instrumentation pattern

## Portfolio Libraries Current State

### PortfolioCore Port (`PortfolioCore.Ports.Embedder`)
- **Behaviour definition**: Defines `embed/2`, `embed_batch/2`, `dimensions/1`, `supported_models/0`
- **Rich return types**: Returns structured maps with vector, model, dimensions, and token_count
- **Batch result type**: Includes total_tokens aggregation
- **Documentation**: Well-documented with examples and model suggestions

### PortfolioIndex Adapters

#### Gemini Adapter (`PortfolioIndex.Adapters.Embedder.Gemini`)
- **Full implementation**: Complete working adapter using gemini_ex
- **Configurable dimensions**: Supports 128-3072 dimension output
- **Automatic normalization**: Normalizes when required by model
- **Token estimation**: Estimates tokens from text length
- **Telemetry**: Emits `[:portfolio_index, :embedder, :embed]` and `[:portfolio_index, :embedder, :embed_batch]`
- **Model resolution**: Handles atoms, strings, and registry defaults

#### OpenAI Adapter (`PortfolioIndex.Adapters.Embedder.OpenAI`)
- **Placeholder only**: Returns `{:error, :not_implemented}` for all operations
- **No actual functionality**: Only defines dimensions and supported_models

## Identified Gaps

### Gap 1: Local Bumblebee/Nx.Serving Embedding Support
- **Arcana Feature**: Complete local embedding using Bumblebee with HuggingFace models, Nx.Serving supervision, EXLA compilation, and support for multiple model families (BGE, E5, GTE, MiniLM)
- **Missing From**: PortfolioIndex (no local embedding adapter exists)
- **Implementation Complexity**: High
- **Technical Details**:
  - Requires Bumblebee, EXLA, and Nx dependencies
  - Need to implement `child_spec/1` for supervision tree integration
  - Must handle model loading from HuggingFace Hub
  - Should maintain dimension mapping for common models
  - Needs batched inference configuration (batch_size, sequence_length)
  - Must integrate with PortfolioCore.Ports.Embedder behaviour (returning structured maps)

### Gap 2: OpenAI Embeddings Implementation
- **Arcana Feature**: Working OpenAI embeddings via ReqLLM with proper model handling and telemetry
- **Missing From**: PortfolioIndex.Adapters.Embedder.OpenAI (exists but not implemented)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana uses ReqLLM with model spec format "openai:#{model}"
  - Portfolio could use ReqLLM or direct Req calls
  - Need to implement actual API calls instead of returning `:not_implemented`
  - Must return structured result matching PortfolioCore.Ports.Embedder types
  - Add token counting from API response (OpenAI returns usage info)

### Gap 3: Custom Function Embedder
- **Arcana Feature**: Wraps arbitrary user functions as embedders, allowing `fn text -> {:ok, embedding} end` configuration
- **Missing From**: PortfolioIndex (no custom/function-based adapter)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Simple wrapper module that implements the behaviour
  - Accepts function via options and invokes it
  - Validates return type `{:ok, [float()]}` or `{:error, term()}`
  - Optional dimension specification or runtime detection
  - Useful for testing and ad-hoc integrations

### Gap 4: Unified Configuration System
- **Arcana Feature**: Single `:arcana, embedder:` config that accepts atoms, tuples, functions, or modules, with automatic resolution to `{module, opts}` tuple
- **Missing From**: PortfolioIndex lacks a unified embedder configuration resolver
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Config parsing logic to handle multiple formats:
    - `:local` -> `{Arcana.Embedder.Local, []}`
    - `{:local, model: "..."}` -> `{Arcana.Embedder.Local, [model: "..."]}`
    - `:openai` -> `{Arcana.Embedder.OpenAI, []}`
    - `fn text -> ... end` -> `{Arcana.Embedder.Custom, [fun: fn]}`
    - `MyModule` -> `{MyModule, []}`
    - `{MyModule, opts}` -> `{MyModule, opts}`
  - Currently PortfolioIndex uses explicit adapter modules without unified dispatch

### Gap 5: Automatic Dimension Detection
- **Arcana Feature**: All embedders can detect dimensions at runtime by embedding a test string when dimensions are not preconfigured
- **Missing From**: PortfolioIndex (Gemini relies on config, OpenAI is hardcoded)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Fall back to embedding "test" string and measuring result length
  - Cache detected dimensions to avoid repeated API calls
  - Useful when adding support for new models

### Gap 6: Embedder Model Dimension Registry
- **Arcana Feature**: `@model_dimensions` map containing known dimensions for popular models (BGE, E5, GTE, MiniLM families)
- **Missing From**: PortfolioIndex (no central dimension registry)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Central registry of model -> dimension mappings
  - BGE: small=384, base=768, large=1024
  - E5: small=384, base=768, large=1024
  - GTE: small=384, base=768, large=1024
  - OpenAI: text-embedding-3-small=1536, text-embedding-3-large=3072
  - Reduces need for runtime detection

### Gap 7: Lazy Dependency Validation
- **Arcana Feature**: Checks `Code.ensure_loaded?(ReqLLM)` at runtime with helpful error messages
- **Missing From**: PortfolioIndex adapters assume dependencies are available
- **Implementation Complexity**: Low
- **Technical Details**:
  - Runtime check for optional dependencies before use
  - Raise clear error with installation instructions
  - Allows adapters to exist without requiring all dependencies

### Gap 8: Serving Child Spec for Supervision
- **Arcana Feature**: Local embedder provides `child_spec/1` for direct supervision tree integration
- **Missing From**: PortfolioIndex has no supervised serving pattern
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Implement `child_spec/1` returning proper spec with id, start, type
  - Allow multiple servings with different models via unique names
  - Use `Module.concat(__MODULE__, model)` pattern for naming
  - Enables hot model swapping and graceful restarts

### Gap 9: Token Count in Return Types
- **Arcana Feature**: N/A (Arcana returns raw embedding without token count)
- **Portfolio Feature Exists**: PortfolioCore.Ports.Embedder specifies `token_count` in result
- **Status**: Portfolio is MORE complete here - Arcana lacks token counting
- **Note**: This is a reverse gap - portfolio has this, Arcana doesn't

### Gap 10: Supported Models Callback
- **Portfolio Feature**: `supported_models/0` callback exists in PortfolioCore.Ports.Embedder
- **Arcana Lacks**: No equivalent callback in Arcana.Embedder behaviour
- **Status**: Portfolio is MORE complete here
- **Note**: This is a reverse gap - portfolio has this, Arcana doesn't

## Implementation Priority

1. **OpenAI Embeddings Implementation** (Low complexity, High value)
   - Complete the existing placeholder adapter
   - Immediate unblocking for OpenAI-based workflows

2. **Local Bumblebee/Nx.Serving Adapter** (High complexity, High value)
   - Enable fully local RAG without API costs
   - Critical for offline/private deployments

3. **Custom Function Embedder** (Low complexity, Medium value)
   - Useful for testing and prototyping
   - Enables quick integrations without full adapter

4. **Model Dimension Registry** (Low complexity, Medium value)
   - Reduces configuration overhead
   - Supports automatic model detection

5. **Unified Configuration System** (Medium complexity, Medium value)
   - Improves developer experience
   - Reduces boilerplate in application config

6. **Automatic Dimension Detection** (Low complexity, Low value)
   - Nice fallback for unknown models
   - Lower priority since registry covers common cases

7. **Lazy Dependency Validation** (Low complexity, Low value)
   - Defensive programming improvement
   - Can be added incrementally to adapters

8. **Serving Child Spec Pattern** (Medium complexity, Medium value)
   - Only needed if implementing local embedder
   - Bundled with Local adapter implementation

## Technical Dependencies

### For Local Bumblebee Adapter
- `bumblebee` - Model loading and serving
- `exla` - XLA compiler backend for Nx
- `nx` - Numerical computing (Nx.Serving)
- HuggingFace Hub access for model download

### For OpenAI Adapter Completion
- `req` - HTTP client (likely already present)
- Optional: `req_llm` - Higher-level LLM client wrapper
- OpenAI API key configuration

### For Custom Function Embedder
- No additional dependencies
- Pure Elixir implementation

### For Configuration System
- No additional dependencies
- May need Application config access patterns

### Shared Dependencies
- `:telemetry` - Already used in portfolio libs
- `Logger` - Standard library

## Summary

The primary gaps are:
1. **Local embedding** - Arcana has full Bumblebee/Nx.Serving support; Portfolio has none
2. **OpenAI implementation** - Arcana works; Portfolio is a placeholder
3. **Configuration flexibility** - Arcana supports functions/atoms/modules; Portfolio is rigid

Portfolio has advantages in:
1. **Richer return types** - Structured maps with token counts
2. **Supported models callback** - Explicit model enumeration
3. **Gemini support** - Working Gemini adapter that Arcana lacks
</file>

<file path="03_vector_store.md">
# Vector Store System Gap Analysis

## Executive Summary

This document analyzes the vector store systems in Arcana and compares them against the Portfolio libraries (PortfolioCore and PortfolioIndex). The analysis identifies key architectural differences, missing features, and prioritized implementation gaps for RAG functionality.

---

## Arcana Vector Store Capabilities

### Architecture Overview

Arcana implements a **dispatch-based vector store** with a behaviour pattern that supports:

1. **Multiple Backend Support**
   - `:pgvector` - PostgreSQL with pgvector extension (default)
   - `:memory` - In-memory storage using HNSWLib
   - Custom modules implementing the `Arcana.VectorStore` behaviour

2. **Core Operations**
   - `store/5` - Store vectors with metadata
   - `search/3` - Semantic similarity search
   - `search_text/3` - Full-text search
   - `delete/3` - Remove vectors by ID
   - `clear/2` - Clear entire collections

3. **Backend Override Pattern**
   - Runtime backend switching via options
   - Per-call backend configuration: `vector_store: {:memory, pid: pid}`
   - Supports custom module backends

### Memory Backend Features (`Arcana.VectorStore.Memory`)

- **GenServer-based** in-memory store
- **HNSWLib integration** for approximate nearest neighbor (ANN) search
- **Cosine similarity** for semantic search
- **TF-IDF-like scoring** for text search
- **Soft deletion** via MapSet tracking
- **Collection isolation** - multiple collections per server
- **Max elements configuration** (default: 10,000)
- **Automatic dimension detection** on first vector

### Pgvector Backend Features (`Arcana.VectorStore.Pgvector`)

- **Ecto-based** PostgreSQL integration
- **Collection/Document/Chunk schema** integration
- **Full-text search** via PostgreSQL ts_rank
- **Cosine distance** (`<=>` operator)
- **Score threshold filtering**
- **Source ID filtering** for document-level queries
- **Auto document creation** for standalone vector storage

---

## Portfolio Libraries Current State

### PortfolioCore Port Specification (`PortfolioCore.Ports.VectorStore`)

A comprehensive **behaviour specification** with:

1. **Index Lifecycle Management**
   - `create_index/2` - Create index with configuration
   - `delete_index/1` - Remove index
   - `index_exists?/1` - Check existence
   - `index_stats/1` - Get statistics

2. **Vector Operations**
   - `store/4` - Single vector storage
   - `store_batch/2` - Batch vector storage
   - `search/4` - k-NN search with options
   - `delete/2` - Remove by ID
   - `fulltext_search/4` - Optional text search

3. **Rich Type Specifications**
   - Multiple distance metrics: `:cosine`, `:euclidean`, `:dot_product`
   - Index types: IVF, HNSW, Flat
   - Configurable index options
   - Search result includes optional vector return

### PortfolioCore Hybrid Behavior (`PortfolioCore.Ports.VectorStore.Hybrid`)

- Separate behaviour for hybrid search capability
- `hybrid_search/6` helper function
- Integrates with RRF scoring module

### PortfolioCore RRF Module (`PortfolioCore.VectorStore.RRF`)

- **Reciprocal Rank Fusion** implementation
- Configurable k parameter (default: 60)
- Separate weights for semantic/fulltext
- Result merging with vector preservation

### PortfolioIndex Pgvector Adapter (`PortfolioIndex.Adapters.VectorStore.Pgvector`)

1. **Dynamic Table Management**
   - Per-index table creation (`vectors_{index_id}`)
   - Index registry table for metadata
   - Support for IVFFlat and HNSW index types

2. **All Three Distance Metrics**
   - Cosine (`<=>`)
   - Euclidean (`<->`)
   - Dot Product (`<#>`)

3. **Advanced Features**
   - Metadata filtering in searches
   - Min score thresholds
   - Optional vector inclusion in results
   - Telemetry integration
   - Transaction-based batch operations

4. **Full-Text Search Module** (`Pgvector.FullText`)
   - tsvector-based PostgreSQL search
   - Language-aware stemming
   - Phrase matching support
   - GIN index creation

---

## Identified Gaps

### Gap 1: In-Memory Vector Store Adapter

- **Arcana Feature**: `Arcana.VectorStore.Memory` - GenServer-based in-memory store using HNSWLib for ANN search with soft deletion and collection management
- **Missing From**: PortfolioIndex (no memory adapter exists)
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Portfolio currently only has pgvector adapter
  - HNSWLib dependency would need to be added
  - Useful for testing, development, and small datasets
  - Should implement `PortfolioCore.Ports.VectorStore` behaviour
  - Consider adding index statistics tracking

### Gap 2: Collection-Based Organization

- **Arcana Feature**: First-class collection abstraction with `collection` parameter in all operations, allowing logical grouping without separate table management
- **Missing From**: PortfolioIndex (uses per-index tables)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana uses collection names mapped to collection_id
  - Portfolio uses index_id which maps to individual tables
  - Different architectural approaches - both valid
  - Portfolio approach may have better isolation
  - Could add collection abstraction layer on top of current approach

### Gap 3: Backend Override at Runtime

- **Arcana Feature**: Per-call backend switching via `vector_store: {:memory, pid: pid}` or `vector_store: {:pgvector, repo: MyRepo}` options
- **Missing From**: PortfolioCore/PortfolioIndex
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Arcana allows mixing backends in same application
  - Portfolio uses static adapter configuration
  - Useful for testing (use memory in tests, pgvector in prod)
  - Could add optional `:adapter` key to search/store options
  - Requires dispatcher module similar to Arcana's dispatch pattern

### Gap 4: Automatic Document/Collection Creation

- **Arcana Feature**: `Collection.get_or_create/2` and auto-document creation for standalone vector storage in pgvector backend
- **Missing From**: PortfolioIndex (requires explicit index creation)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana creates minimal documents when storing vectors without explicit document
  - Reduces boilerplate for simple use cases
  - Portfolio requires `create_index/2` before storing vectors
  - Could add auto-create option to `store/4`

### Gap 5: Soft Deletion in Memory Backend

- **Arcana Feature**: Memory backend uses MapSet for tracking deleted indices, avoiding index rebuilds
- **Missing From**: N/A (no memory backend in Portfolio)
- **Implementation Complexity**: Low (when implementing memory adapter)
- **Technical Details**:
  - HNSWLib doesn't support true deletion
  - Soft deletion allows marking as deleted without rebuild
  - Filter deleted entries in search results
  - Increases memory usage but improves performance

### Gap 6: Dimension Auto-Detection

- **Arcana Feature**: Memory backend auto-detects vector dimensions from first stored vector
- **Missing From**: PortfolioIndex (requires explicit dimension in config)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana: `ensure_dimensions/2` sets dims on first store
  - Portfolio: dimensions must be specified in `create_index/2`
  - Auto-detection simplifies API but can mask configuration errors
  - Consider optional auto-detect with explicit override

### Gap 7: Simple Text Search in Memory

- **Arcana Feature**: `search_text/4` in memory backend with TF-IDF-like scoring using tokenization
- **Missing From**: PortfolioIndex memory adapter (doesn't exist yet)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Simple term matching without database
  - Tokenization: lowercase, remove punctuation, split on whitespace
  - Score: `(matching_terms / query_terms) * length_normalization`
  - Useful for testing hybrid search without PostgreSQL

### Gap 8: Document-Level Filtering in Vector Search

- **Arcana Feature**: `source_id` filtering allows searching within specific document scope
- **Missing From**: PortfolioIndex (has metadata filtering but not document-level)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana: `Pgvector.search/3` accepts `:source_id` option
  - Portfolio: Uses generic metadata filter `{:filter, %{key: value}}`
  - Could add explicit document_id/source_id filter shorthand
  - Or document as convention in metadata

### Gap 9: Integrated Document/Chunk Schema

- **Arcana Feature**: Tight integration with `Arcana.Document` and `Arcana.Chunk` schemas, including document status tracking
- **Missing From**: PortfolioIndex (schema-agnostic approach)
- **Implementation Complexity**: High
- **Technical Details**:
  - Arcana stores chunks with document_id foreign key
  - Document has status (`:pending`, `:completed`)
  - Collection has associated documents
  - Portfolio uses standalone vector tables without document hierarchy
  - Different design philosophy - Portfolio favors flexibility

### Gap 10: Score Threshold in Search

- **Arcana Feature**: `threshold` option in pgvector search to filter low-similarity results
- **Missing From**: Partially exists - Portfolio has `:min_score` option
- **Implementation Complexity**: N/A (already implemented differently)
- **Technical Details**:
  - Arcana: `threshold: 0.0` - filter in WHERE clause
  - Portfolio: `:min_score` - same functionality
  - Naming difference only, no implementation gap

### Gap 11: Hybrid Search Convenience Function

- **Arcana Feature**: No built-in hybrid search - relies on caller to combine results
- **Missing From**: N/A - Portfolio has better implementation
- **Implementation Complexity**: N/A
- **Technical Details**:
  - Portfolio has `PortfolioCore.Ports.VectorStore.Hybrid.hybrid_search/6`
  - Portfolio has RRF scoring module
  - Arcana requires manual result merging
  - **Portfolio is ahead here**

### Gap 12: Euclidean and Dot Product Distance Metrics

- **Arcana Feature**: Only cosine similarity supported in search
- **Missing From**: N/A - Portfolio has better implementation
- **Implementation Complexity**: N/A
- **Technical Details**:
  - Arcana pgvector uses only `<=>` (cosine)
  - Arcana memory uses only `:cosine` in HNSWLib
  - Portfolio supports all three: cosine, euclidean, dot_product
  - **Portfolio is ahead here**

### Gap 13: Index Statistics

- **Arcana Feature**: No index statistics API
- **Missing From**: N/A - Portfolio has this
- **Implementation Complexity**: N/A
- **Technical Details**:
  - Portfolio: `index_stats/1` returns count, dimensions, metric, size_bytes
  - Arcana has no equivalent
  - **Portfolio is ahead here**

### Gap 14: Batch Store Operations

- **Arcana Feature**: No batch store API - only single vector store
- **Missing From**: N/A - Portfolio has this
- **Implementation Complexity**: N/A
- **Technical Details**:
  - Portfolio: `store_batch/2` with transaction support
  - Arcana requires loop of individual `store/5` calls
  - **Portfolio is ahead here**

### Gap 15: Max Elements Configuration (Memory)

- **Arcana Feature**: Configurable `max_elements` for HNSWLib index (default: 10,000)
- **Missing From**: PortfolioIndex (no memory adapter)
- **Implementation Complexity**: Low (when implementing memory adapter)
- **Technical Details**:
  - HNSWLib requires max elements at index creation
  - Arcana passes this via GenServer options
  - Portfolio memory adapter should include this configuration

---

## Implementation Priority

### Priority 1 - High Value, Low Effort

1. **Backend Override at Runtime** (Gap 3)
   - Enable testing with memory backend
   - Production flexibility
   - Estimated: 2-3 hours

2. **Auto Index/Collection Creation** (Gap 4)
   - Reduce boilerplate
   - Better developer experience
   - Estimated: 1-2 hours

### Priority 2 - High Value, Medium Effort

3. **In-Memory Vector Store Adapter** (Gaps 1, 5, 7, 15)
   - Testing without PostgreSQL
   - Development workflows
   - Small dataset optimization
   - Estimated: 4-6 hours

4. **Document-Level Filtering** (Gap 8)
   - Scoped searches
   - Multi-tenant use cases
   - Estimated: 1-2 hours

### Priority 3 - Medium Value, Low Effort

5. **Dimension Auto-Detection** (Gap 6)
   - Convenience feature
   - Can be optional
   - Estimated: 1 hour

### Priority 4 - Low Priority

6. **Collection Abstraction** (Gap 2)
   - Different design philosophy
   - Current approach is valid
   - Consider only if specific need arises

7. **Integrated Document Schema** (Gap 9)
   - Major architectural change
   - Portfolio's flexible approach may be preferred
   - Consider domain-specific wrappers instead

---

## Technical Dependencies

### For In-Memory Adapter (Priority 2)

```elixir
# mix.exs
{:hnswlib, "~> 0.1", optional: true}
{:nx, "~> 0.7"}  # Already likely present
```

### For Backend Override (Priority 1)

- No new dependencies
- Requires dispatcher pattern in PortfolioIndex
- Update `search/4` and `store/4` signatures

### For Auto-Creation (Priority 1)

- No new dependencies
- Add `get_or_create_index/2` helper
- Update `store/4` to optionally auto-create

---

## Portfolio Advantages Over Arcana

The following features exist in Portfolio but not in Arcana:

1. **Multiple Distance Metrics** - Cosine, Euclidean, Dot Product
2. **Index Type Configuration** - IVFFlat, HNSW, Flat
3. **Batch Operations** - `store_batch/2` for efficient bulk ingestion
4. **Index Statistics** - `index_stats/1` for monitoring
5. **Index Lifecycle** - `create_index/2`, `delete_index/1`, `index_exists?/1`
6. **RRF Hybrid Search** - Built-in RRF scoring with configurable weights
7. **Hybrid Behaviour** - Separate behaviour for hybrid-capable stores
8. **Advanced Full-Text** - Phrase matching, language support, GIN indices
9. **Telemetry Integration** - Performance monitoring out of the box
10. **Dynamic Table Management** - Per-index tables with registry

---

## Summary

The Portfolio libraries have a more mature and feature-rich vector store implementation compared to Arcana. The primary gaps are:

1. **In-memory adapter** for testing and development
2. **Runtime backend switching** for flexibility
3. **Convenience features** like auto-creation and dimension detection

The recommended approach is to implement the in-memory adapter and backend switching pattern from Arcana while preserving Portfolio's superior features in distance metrics, batch operations, and hybrid search.
</file>

<file path="04_evaluation_system.md">
# Evaluation System Gap Analysis

## Executive Summary

This document analyzes the evaluation capabilities in Arcana's RAG framework compared to the portfolio libraries (PortfolioCore and PortfolioManager). While both systems address RAG evaluation, they take fundamentally different approaches: **Arcana focuses on retrieval quality metrics with test case management**, while **portfolio libraries focus on generation quality assessment using the RAG Triad framework**. These are complementary systems with significant gaps in both directions.

---

## Arcana Evaluation Capabilities

### 1. Core Evaluation Architecture

Arcana provides a comprehensive retrieval evaluation framework with the following components:

#### 1.1 Evaluation Module (`Arcana.Evaluation`)
- **Main entry point** for all evaluation operations
- Coordinates test case generation, evaluation runs, and metrics aggregation
- Supports multiple search modes: `:semantic`, `:fulltext`, `:hybrid`
- Optional answer evaluation with faithfulness scoring

#### 1.2 Test Case System (`Arcana.Evaluation.TestCase`)
- Ecto schema for persisting test cases in database
- Tracks `question` with linked `relevant_chunks` (ground truth)
- Two sources: `:synthetic` (LLM-generated) or `:manual` (user-created)
- Many-to-many relationship with chunks via join table
- Enables reproducible evaluation across runs

#### 1.3 Synthetic Test Case Generator (`Arcana.Evaluation.Generator`)
- LLM-powered question generation from document chunks
- Random sampling with configurable `sample_size`
- Filtering by `source_id` or `collection`
- Customizable prompt templates with `{chunk_text}` placeholder
- Default prompt ensures questions are specific and searchable

#### 1.4 Retrieval Metrics (`Arcana.Evaluation.Metrics`)
Standard IR (Information Retrieval) metrics at K=[1, 3, 5, 10]:
- **Recall@K**: Fraction of relevant docs in top K results
- **Precision@K**: Fraction of top K that are relevant
- **MRR (Mean Reciprocal Rank)**: 1/position of first relevant result
- **Hit Rate@K**: Binary indicator of any relevant doc in top K

Aggregation across test cases with automatic averaging.

#### 1.5 Answer Quality Metrics (`Arcana.Evaluation.AnswerMetrics`)
- **Faithfulness Scoring**: LLM-as-judge evaluation (0-10 scale)
- Evaluates if generated answers are grounded in retrieved context
- Customizable evaluation prompts
- Returns score with reasoning explanation

#### 1.6 Evaluation Run Persistence (`Arcana.Evaluation.Run`)
- Ecto schema for storing evaluation runs
- Tracks status: `:running`, `:completed`, `:failed`
- Stores configuration, aggregate metrics, and per-case results
- Enables historical comparison and trend analysis

#### 1.7 Mix Tasks (CLI Interface)
**`mix arcana.eval.generate`**:
- Generates synthetic test cases from chunks
- Options: `--sample-size`, `--source-id`, `--collection`

**`mix arcana.eval.run`**:
- Executes evaluation against test cases
- Options: `--mode`, `--source-id`, `--generate`, `--format`, `--fail-under`
- Supports JSON output for CI integration
- Threshold-based CI failure (`--fail-under` for recall@5)

### 2. Key Arcana Features

| Feature | Description |
|---------|-------------|
| Test Case Persistence | Database-backed test cases for reproducibility |
| Synthetic Generation | LLM generates questions from chunks |
| Manual Test Cases | User-defined ground truth |
| Multi-mode Evaluation | Compare semantic, fulltext, hybrid |
| Run History | Track evaluation over time |
| CI Integration | `--fail-under` threshold checking |
| Answer Evaluation | Optional faithfulness scoring |
| Per-case Results | Drill-down into failures |

---

## Portfolio Libraries Current State

### 1. PortfolioCore.Ports.Evaluation (Port Specification)

Defines the evaluation behavior contract with:

#### 1.1 RAG Triad Framework (TruLens-based)
Three evaluation dimensions scored 1-5:
- **Context Relevance**: Is retrieved context relevant to query?
- **Groundedness**: Is response supported by context?
- **Answer Relevance**: Does answer address the query?

#### 1.2 Hallucination Detection
- Binary detection with evidence explanation
- Strict mode option for safety-critical applications

#### 1.3 Callback Specifications
```elixir
@callback evaluate_rag_triad(generation(), opts) :: {:ok, triad_result()} | {:error, term()}
@callback evaluate_context_relevance(generation(), opts) :: {:ok, triad_score()} | {:error, term()}
@callback evaluate_groundedness(generation(), opts) :: {:ok, triad_score()} | {:error, term()}
@callback evaluate_answer_relevance(generation(), opts) :: {:ok, triad_score()} | {:error, term()}
@callback detect_hallucination(generation(), opts) :: {:ok, hallucination_result()} | {:error, term()}
```

### 2. PortfolioManager.Evaluation (Implementation)

#### 2.1 Full RAG Triad Implementation
- Implements all PortfolioCore.Ports.Evaluation callbacks
- Sequential evaluation of all three dimensions
- Overall score as average of three dimensions
- LLM-powered assessment via Router.complete

#### 2.2 Prompt Engineering
- Structured prompts for each evaluation type
- JSON response format with score and reasoning
- Robust JSON extraction from LLM responses

#### 2.3 Telemetry Integration
- Events for `:rag_triad` and `:hallucination` evaluations
- Duration tracking for performance monitoring
- Metadata includes generation_id and result

### 3. Key Portfolio Features

| Feature | Description |
|---------|-------------|
| RAG Triad Scoring | 1-5 scale with reasoning |
| Context Relevance | Query-context alignment |
| Groundedness | Response-context support |
| Answer Relevance | Response-query alignment |
| Hallucination Detection | Binary with evidence |
| Telemetry | Observable evaluation metrics |

---

## Identified Gaps

### Gap 1: Test Case Persistence System
- **Arcana Feature**: Complete Ecto-based test case storage with:
  - `arcana_evaluation_test_cases` table
  - `arcana_evaluation_test_case_chunks` join table
  - Source tracking (synthetic vs manual)
  - Question-to-chunk ground truth relationships
- **Missing From**: PortfolioCore (no port), PortfolioManager (no implementation)
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Requires Ecto schema definitions
  - Migration for test_cases and join table
  - API for CRUD operations on test cases
  - Integration with existing chunk schema from PortfolioIndex

### Gap 2: Synthetic Test Case Generation
- **Arcana Feature**: LLM-powered question generation from chunks with:
  - Random sampling of chunks
  - Customizable prompts with `{chunk_text}` placeholder
  - Filtering by source_id or collection
  - Automatic linking of source chunk as ground truth
- **Missing From**: PortfolioCore, PortfolioManager
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Requires LLM adapter integration
  - Chunk access/sampling logic
  - Prompt template system
  - Test case creation workflow

### Gap 3: Retrieval Quality Metrics (IR Metrics)
- **Arcana Feature**: Standard information retrieval metrics:
  - Recall@K (K=1,3,5,10)
  - Precision@K
  - Mean Reciprocal Rank (MRR)
  - Hit Rate@K
  - Aggregation across test cases
- **Missing From**: PortfolioCore (no port), PortfolioManager (no implementation)
- **Implementation Complexity**: Low-Medium
- **Technical Details**:
  - Pure computation functions (no external dependencies)
  - Requires test case with expected chunk IDs
  - Requires search results with retrieved chunk IDs
  - K-value configuration

### Gap 4: Evaluation Run Persistence
- **Arcana Feature**: Complete run tracking with:
  - `arcana_evaluation_runs` Ecto schema
  - Status tracking (running/completed/failed)
  - Stored configuration for reproducibility
  - Aggregate metrics storage
  - Per-case results for drill-down
  - Historical comparison capability
- **Missing From**: PortfolioCore, PortfolioManager
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Ecto schema with JSON fields for metrics/results/config
  - Run lifecycle management
  - Query APIs for historical runs
  - Integration with metrics computation

### Gap 5: CLI Tooling for Evaluation
- **Arcana Feature**: Mix tasks for evaluation workflow:
  - `mix arcana.eval.generate` for test case generation
  - `mix arcana.eval.run` for evaluation execution
  - Multiple output formats (table, JSON)
  - CI integration with `--fail-under` threshold
  - Auto-generation when no test cases exist
- **Missing From**: PortfolioCore, PortfolioManager, PortfolioIndex
- **Implementation Complexity**: Low
- **Technical Details**:
  - Mix.Task implementations
  - Option parsing with OptionParser
  - Output formatting functions
  - Integration with evaluation APIs

### Gap 6: Faithfulness Scoring Port
- **Arcana Feature**: `AnswerMetrics.evaluate_faithfulness/4` with:
  - 0-10 scale scoring
  - LLM-as-judge approach
  - Customizable prompt function
  - JSON response parsing with clamping
- **Missing From**: PortfolioCore (partial - groundedness is similar but not identical)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana uses 0-10 scale, Portfolio uses 1-5
  - Arcana evaluates full answer faithfulness
  - Portfolio's groundedness is response-to-context only
  - Different prompt structures and semantics

### Gap 7: Multi-Mode Evaluation Comparison
- **Arcana Feature**: Evaluation across search modes:
  - Semantic search evaluation
  - Fulltext search evaluation
  - Hybrid search evaluation
  - Same test cases across all modes for fair comparison
- **Missing From**: PortfolioCore, PortfolioManager
- **Implementation Complexity**: Low (if search modes exist)
- **Technical Details**:
  - Parameterized search mode in evaluation
  - Configuration storage per run
  - Cross-mode comparison reporting

### Gap 8: Ground Truth Management
- **Arcana Feature**: Multiple relevant chunks per test case:
  - Many-to-many relationship
  - Manual addition of relevant chunks
  - Source chunk tracking for synthetic cases
  - Bulk operations via insert_all
- **Missing From**: PortfolioCore, PortfolioManager
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Join table schema
  - API for managing ground truth
  - Preloading for evaluation

### Gap 9: Evaluation Port Definition
- **Arcana Feature**: (Implicit - no explicit port but cohesive API)
- **Missing From**: PortfolioCore lacks retrieval evaluation port
- **Implementation Complexity**: Low
- **Technical Details**:
  - Port for retrieval metrics computation
  - Port for test case generation
  - Port for evaluation run management
  - Integration with existing Evaluation port

### Gap 10: Per-Case Result Drill-Down
- **Arcana Feature**: Detailed per-test-case results including:
  - Expected chunk IDs
  - Retrieved chunk IDs
  - Per-K recall/precision values
  - Reciprocal rank
  - Hit indicators
- **Missing From**: PortfolioManager (evaluates single generations, no aggregation)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Return structure enhancement
  - Storage in run results map
  - Query APIs for analysis

### Gap 11: Threshold-Based CI Failure
- **Arcana Feature**: `--fail-under` option for CI pipelines:
  - Exit code 1 if recall@5 below threshold
  - Clear pass/fail output
  - Configurable threshold value
- **Missing From**: PortfolioCore, PortfolioManager, PortfolioIndex
- **Implementation Complexity**: Low
- **Technical Details**:
  - CLI flag parsing
  - Threshold comparison logic
  - Exit code management

### Gap 12: Collection-Based Filtering
- **Arcana Feature**: Filter evaluation by collection:
  - Generator supports `--collection` option
  - Joins to collection via document relationship
  - Scoped test case generation
- **Missing From**: PortfolioCore, PortfolioManager
- **Implementation Complexity**: Low
- **Technical Details**:
  - Requires collection association in schema
  - Query filtering enhancement
  - CLI option addition

---

## Reverse Gaps (Portfolio Features Missing from Arcana)

### Reverse Gap 1: Context Relevance Scoring
- **Portfolio Feature**: Query-to-context relevance evaluation
- **Arcana Status**: Not implemented - focuses on retrieval, not context quality
- **Notes**: Could be valuable addition to answer evaluation

### Reverse Gap 2: Answer Relevance Scoring
- **Portfolio Feature**: Response-to-query alignment evaluation
- **Arcana Status**: Not implemented - faithfulness only checks groundedness
- **Notes**: Complements faithfulness scoring

### Reverse Gap 3: Hallucination Detection with Evidence
- **Portfolio Feature**: Binary hallucination detection with explanation
- **Arcana Status**: Partial - faithfulness score implies but doesn't explicitly detect
- **Notes**: More actionable for content filtering

### Reverse Gap 4: Telemetry Integration
- **Portfolio Feature**: Observable evaluation with telemetry events
- **Arcana Status**: No telemetry integration
- **Notes**: Important for production monitoring

---

## Implementation Priority

### Priority 1: Critical (Needed for RAG Quality Assurance)
1. **Gap 3: Retrieval Quality Metrics** - Core IR metrics (Low-Medium complexity)
2. **Gap 9: Evaluation Port Definition** - Contract for retrieval evaluation (Low complexity)
3. **Gap 1: Test Case Persistence** - Foundation for reproducible evaluation (Medium complexity)

### Priority 2: High (Enable Automated Evaluation)
4. **Gap 2: Synthetic Test Case Generation** - Scalable test creation (Medium complexity)
5. **Gap 4: Evaluation Run Persistence** - Historical tracking (Medium complexity)
6. **Gap 8: Ground Truth Management** - Multiple relevant chunks (Medium complexity)

### Priority 3: Medium (CI/CD Integration)
7. **Gap 5: CLI Tooling** - Developer experience (Low complexity)
8. **Gap 11: Threshold-Based CI Failure** - Automated quality gates (Low complexity)
9. **Gap 7: Multi-Mode Evaluation** - Search mode comparison (Low complexity)

### Priority 4: Enhancement (Improved Analysis)
10. **Gap 10: Per-Case Drill-Down** - Failure analysis (Low complexity)
11. **Gap 6: Faithfulness Scoring Port** - Answer quality (Low complexity)
12. **Gap 12: Collection-Based Filtering** - Scoped evaluation (Low complexity)

---

## Technical Dependencies

### For PortfolioCore (Port Definitions)
| Gap | Dependencies |
|-----|--------------|
| Gap 9: Evaluation Port | None (pure specification) |
| Gap 3: Retrieval Metrics Port | Gap 9 |

### For PortfolioIndex (Adapters)
| Gap | Dependencies |
|-----|--------------|
| Gap 3: Retrieval Metrics Adapter | Gap 9 port, Chunk schema |
| Gap 1: Test Case Schema | Chunk schema, Ecto |
| Gap 8: Ground Truth | Gap 1 |
| Gap 4: Run Schema | Gap 1, Gap 3 |
| Gap 2: Generator | Gap 1, LLM adapter, Chunk access |

### For PortfolioManager (Integration)
| Gap | Dependencies |
|-----|--------------|
| Gap 7: Multi-Mode Eval | Search modes in manager |
| Gap 10: Per-Case Results | Gap 3, Gap 4 |
| Gap 5: CLI Tooling | All above gaps |
| Gap 11: CI Threshold | Gap 5 |

### External Dependencies
| Gap | External Dependency |
|-----|---------------------|
| Gap 2 | LLM provider (OpenAI, Anthropic, etc.) |
| Gap 6 | LLM provider for faithfulness |
| Gap 1, 4 | Ecto and PostgreSQL |

---

## Migration Strategy

### Phase 1: Foundation (PortfolioCore + PortfolioIndex)
1. Define retrieval evaluation port in PortfolioCore
2. Implement metrics computation adapter in PortfolioIndex
3. Add test case and run Ecto schemas to PortfolioIndex
4. Create migrations for evaluation tables

### Phase 2: Generation (PortfolioIndex)
1. Implement test case generator adapter
2. Add ground truth management APIs
3. Create synthetic test case generation workflow

### Phase 3: Integration (PortfolioManager)
1. Add evaluation orchestration to manager
2. Implement multi-mode evaluation
3. Add telemetry for evaluation events

### Phase 4: Tooling (All Libraries)
1. Create Mix tasks in appropriate library
2. Add CLI options for all features
3. Implement CI threshold checking
4. Add JSON output for automation

---

## Summary

The Arcana evaluation system provides a comprehensive **retrieval quality measurement framework** with test case persistence, synthetic generation, and standard IR metrics. The portfolio libraries currently focus on **generation quality assessment** via the RAG Triad framework.

**Key insight**: These are complementary systems. A complete RAG evaluation suite requires both:
- **Retrieval evaluation** (Arcana's strength): Did we find the right documents?
- **Generation evaluation** (Portfolio's strength): Did we produce a good answer?

The recommended approach is to:
1. Port Arcana's retrieval evaluation features to PortfolioIndex
2. Keep and enhance PortfolioManager's RAG Triad implementation
3. Create a unified evaluation workflow that assesses both retrieval and generation quality

Total gaps identified: **12 forward gaps** + **4 reverse gaps**
Implementation effort estimate: **3-4 weeks** for full feature parity
</file>

<file path="05_llm_integration.md">
# LLM Integration Gap Analysis

## Executive Summary

This analysis compares the LLM integration patterns between Arcana and the Portfolio libraries (portfolio_core, portfolio_index, portfolio_manager). Arcana uses a lightweight protocol-based approach with built-in RAG agent components, while Portfolio has a more formal port/adapter architecture with multi-provider support but lacks several RAG-specific query processing features.

---

## Arcana LLM Capabilities

### Core Protocol (`Arcana.LLM`)

Arcana defines a simple protocol for LLM integration with three implementations:

1. **Function-based** - Anonymous functions with 1-3 arity for testing/flexibility
2. **String-based** - Model strings via `ReqLLM` (e.g., `"openai:gpt-4o-mini"`, `"zai:glm-4.5-flash"`)
3. **Tuple-based** - `{model_string, opts}` for passing API keys and other options

**Key Features:**
- Single `complete/4` callback: `(llm, prompt, context, opts) -> {:ok, response} | {:error, reason}`
- Built-in telemetry via `Arcana.LLM.Helpers.with_telemetry/4`
- Context formatting with `format_context/1` for RAG scenarios
- Default system prompt generation with context injection

### Agent Pipeline Components (RAG-Specific)

Arcana provides a rich set of LLM-powered RAG components:

| Component | Module | Purpose |
|-----------|--------|---------|
| Query Rewriter | `Arcana.Agent.Rewriter.LLM` | Transforms conversational input into search queries |
| Query Expander | `Arcana.Agent.Expander.LLM` | Adds synonyms and related terms for better recall |
| Query Decomposer | `Arcana.Agent.Decomposer.LLM` | Breaks complex queries into sub-questions |
| Collection Selector | `Arcana.Agent.Selector.LLM` | Routes queries to relevant collections |
| Reranker | `Arcana.Agent.Reranker.LLM` | Scores chunk relevance with LLM judgment |
| Answerer | `Arcana.Agent.Answerer.LLM` | Generates final answers from context |

### Structured Output Patterns

Arcana uses JSON-based structured outputs for several components:

1. **Selector** - Returns `{"collections": [...], "reasoning": "..."}`
2. **Decomposer** - Returns `{"sub_questions": ["q1", "q2", ...]}`
3. **Reranker** - Returns `{"score": <0-10>, "reasoning": "..."}`

All components include robust JSON extraction with fallback handling.

### Pipeline Context (`Arcana.Agent.Context`)

A comprehensive struct that flows through the pipeline tracking:
- Input: `question`, `repo`, `llm`
- Query Processing: `rewritten_query`, `expanded_query`, `sub_questions`
- Routing: `collections`, `selection_reasoning`
- Retrieval: `results`, `rerank_scores`
- Output: `answer`, `context_used`, `correction_count`, `corrections`
- Error handling: `error`

---

## Portfolio Libraries Current State

### PortfolioCore.Ports.LLM (Behaviour Definition)

Formal port specification with comprehensive callbacks:

| Callback | Signature | Description |
|----------|-----------|-------------|
| `complete/2` | `([message], opts) -> {:ok, completion_result} | {:error, term}` | Synchronous completion |
| `stream/2` | `([message], opts) -> {:ok, Enumerable.t} | {:error, term}` | Streaming completion |
| `supported_models/0` | `() -> [String.t()]` | List available models |
| `model_info/1` | `(model) -> %{context_window, max_output, supports_tools}` | Model metadata |

**Structured Types:**
- `message` - `%{role: :system | :user | :assistant, content: String.t()}`
- `completion_result` - `%{content, model, usage, finish_reason}`
- `stream_chunk` - `%{delta, finish_reason}`

### PortfolioIndex LLM Adapters

Three provider implementations:

1. **Anthropic** (`PortfolioIndex.Adapters.LLM.Anthropic`)
   - Uses `claude_agent_sdk`
   - Supports SDK direct calls and `ClaudeAgentSDK.Streaming`
   - Dynamic SDK function detection

2. **Gemini** (`PortfolioIndex.Adapters.LLM.Gemini`)
   - Uses `gemini_ex`
   - Safety rating handling
   - Retry logic for empty responses

3. **OpenAI** (`PortfolioIndex.Adapters.LLM.OpenAI`)
   - Uses `codex_sdk`
   - Thread-based execution support
   - Stream event transformation

All adapters implement:
- Token usage normalization
- Finish reason mapping
- Telemetry emission
- Streaming support

### PortfolioIndex RAG Strategies

| Strategy | LLM Usage |
|----------|-----------|
| `SelfRAG` | Retrieval assessment, answer generation with self-critique, refinement |
| `Agentic` | Tool-based iterative retrieval with JSON action parsing |
| `Hybrid` | No direct LLM usage (RRF-based) |
| `GraphRAG` | Community summarization (LLM for synthesis) |

### PortfolioIndex Reranker

`PortfolioIndex.Adapters.Reranker.LLM`:
- JSON-based scoring prompt
- Score parsing with fallback to passthrough
- Normalized 0-1 score output

### PortfolioManager.Generation

State container for RAG pipeline tracking:
- `query`, `query_embedding`, `retrieval_results`
- `context`, `context_sources`, `prompt`, `response`
- `evaluations`, `metadata`, `halted?`, `errors`

Uses builder pattern: `new/2 |> with_embedding/2 |> with_retrieval/2 |> ...`

---

## Identified Gaps

### Gap 1: Query Rewriting

- **Arcana Feature**: `Arcana.Agent.Rewriter.LLM` transforms conversational input (e.g., "Hey, can you tell me about Elixir?") into clean search queries ("Elixir programming language")
- **Missing From**: portfolio_index, portfolio_core
- **Implementation Complexity**: Low
- **Technical Details**:
  - Requires single LLM call with simple prompt template
  - No structured output - just string response
  - Can use existing LLM adapters directly
  - Portfolio has no pre-processing of user queries before embedding

### Gap 2: Query Expansion

- **Arcana Feature**: `Arcana.Agent.Expander.LLM` adds synonyms and related terms to improve retrieval recall (e.g., "ML models" -> "ML machine learning models neural networks deep learning")
- **Missing From**: portfolio_index, portfolio_core
- **Implementation Complexity**: Low
- **Technical Details**:
  - Single LLM call with expansion prompt
  - Improves recall for abbreviations and technical terms
  - Could be integrated into `PortfolioIndex.RAG.Strategy` interface
  - Portfolio currently does direct query-to-embedding without term expansion

### Gap 3: Query Decomposition

- **Arcana Feature**: `Arcana.Agent.Decomposer.LLM` breaks complex questions into simpler sub-questions for parallel retrieval (e.g., "Compare Elixir and Go" -> ["Elixir features", "Go features", "performance comparison"])
- **Missing From**: portfolio_index, portfolio_core
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Returns JSON `{"sub_questions": [...]}` with fallback handling
  - Requires aggregation logic for multiple retrieval results
  - Portfolio's `Agentic` strategy does iterative retrieval but not query decomposition
  - Would need `Generation` struct extension or new preprocessing module

### Gap 4: Collection/Index Selection

- **Arcana Feature**: `Arcana.Agent.Selector.LLM` uses LLM to route queries to relevant collections based on descriptions
- **Missing From**: portfolio_index
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Returns JSON `{"collections": [...], "reasoning": "..."}`
  - Requires collection descriptions/metadata
  - Portfolio assumes single-index queries or manual specification
  - Could be added to `AdapterResolver` or as new strategy

### Gap 5: Lightweight Protocol-Based LLM Interface

- **Arcana Feature**: Simple `Arcana.LLM` protocol that accepts functions, strings, or tuples without formal behaviour callbacks
- **Missing From**: portfolio_core (has formal behaviour only)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana's approach enables easier testing with anonymous functions
  - Portfolio requires implementing full behaviour
  - Could add protocol defimpl for Portfolio's behaviour modules
  - Trade-off: Arcana is more flexible, Portfolio is more type-safe

### Gap 6: Context-Aware System Prompts

- **Arcana Feature**: `Arcana.LLM.Helpers.default_system_prompt/1` automatically generates system prompts with injected context
- **Missing From**: portfolio_index LLM adapters
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana auto-formats context chunks into system prompt
  - Portfolio adapters expect pre-formatted messages
  - RAG strategies manually build context but no standard helper
  - Could add to `PortfolioIndex.RAG.ContextBuilder` or similar

### Gap 7: Unified Pipeline Context Struct

- **Arcana Feature**: `Arcana.Agent.Context` tracks full pipeline state including query transformations, routing decisions, and correction history
- **Missing From**: Partial coverage in portfolio_manager
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Arcana tracks: `rewritten_query`, `expanded_query`, `sub_questions`, `collections`, `selection_reasoning`, `correction_count`, `corrections`
  - Portfolio's `Generation` tracks: `query_embedding`, `retrieval_results`, `context`, `response`, `evaluations`
  - Missing: query transformation tracking, routing decisions, self-correction history
  - Would need `Generation` struct extension

### Gap 8: Self-Correction with Critique History

- **Arcana Feature**: Context tracks `correction_count` and `corrections` list of `{answer, feedback}` tuples
- **Missing From**: portfolio_index strategies
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Arcana enables iterative answer refinement with feedback tracking
  - Portfolio's `SelfRAG` does single refinement but doesn't track history
  - Useful for debugging and answer quality analysis
  - Would need extension to `SelfRAG` strategy and `Generation` struct

### Gap 9: Customizable Prompt Functions

- **Arcana Feature**: All agent components accept `:prompt` option as custom function (e.g., `answer(ctx, prompt: fn question, chunks -> ... end)`)
- **Missing From**: portfolio_index adapters
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana: `prompt_fn.(question, chunks)` or `prompt_fn.(question)` patterns
  - Portfolio uses hardcoded prompt templates
  - Only `Reranker.LLM` has `:prompt_template` option
  - Easy addition to all LLM-using components

### Gap 10: Multi-Provider via ReqLLM

- **Arcana Feature**: Single `ReqLLM` dependency handles multiple providers via model string format ("openai:...", "zai:...", "anthropic:...")
- **Missing From**: portfolio_index (separate adapter per provider)
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Arcana: One protocol, one dependency, many providers
  - Portfolio: Separate Anthropic, Gemini, OpenAI adapters with different SDKs
  - Trade-off: Arcana is simpler, Portfolio offers provider-specific optimizations
  - Could add `ReqLLM` adapter as alternative implementation

---

## Implementation Priority

### High Priority (Essential for RAG Quality)

1. **Query Rewriting** - Immediate impact on search quality
2. **Query Expansion** - Significant recall improvement
3. **Customizable Prompt Functions** - Enables domain-specific tuning

### Medium Priority (Enhanced RAG Capabilities)

4. **Query Decomposition** - Complex query handling
5. **Collection Selection** - Multi-index routing
6. **Unified Pipeline Context** - Better observability

### Lower Priority (Nice-to-Have)

7. **Self-Correction History** - Debugging/analysis
8. **Context-Aware System Prompts** - Developer convenience
9. **Lightweight Protocol Interface** - Testing flexibility
10. **Multi-Provider via ReqLLM** - Simplification option

---

## Technical Dependencies

### For Query Processing Gaps (1-4)

- Existing LLM adapters (no new dependencies)
- JSON parsing (already available)
- `PortfolioIndex.RAG.Strategy` extension or new preprocessing module

### For Context/Pipeline Gaps (7-8)

- Extension of `PortfolioManager.Generation` struct
- Backward-compatible field additions

### For Interface Gaps (5, 9, 10)

- Optional: Protocol definitions for existing behaviours
- Optional: `req_llm` dependency for unified provider access

### Integration Points

| Gap | Integration Location |
|-----|---------------------|
| Query Rewriting | New `PortfolioIndex.RAG.QueryProcessor` module |
| Query Expansion | Same as above or inline in strategies |
| Query Decomposition | New module with `Generation` integration |
| Collection Selection | `PortfolioIndex.RAG.AdapterResolver` or new router |
| Prompt Functions | Options for existing LLM adapters and Reranker |
| Pipeline Context | `PortfolioManager.Generation` struct extension |

---

## Recommendations

1. **Create `PortfolioIndex.RAG.QueryProcessor`** - Unified module for rewriting, expansion, and decomposition with pluggable implementations

2. **Extend `PortfolioManager.Generation`** - Add fields for query transformations and routing decisions to enable full pipeline observability

3. **Add `:prompt` option to LLM-using components** - Low-effort change with high customization value

4. **Consider `ReqLLM` adapter** - As optional lightweight alternative to provider-specific adapters for simpler deployments

5. **Document structured output patterns** - Create standard for JSON response parsing with fallback handling across all components
</file>

<file path="06_telemetry.md">
# Telemetry System Gap Analysis

## Executive Summary

This document analyzes the telemetry instrumentation differences between Arcana's cohesive telemetry system and the portfolio libraries (PortfolioCore and PortfolioIndex). Arcana provides a unified, span-based telemetry approach with a built-in logger, while the portfolio libraries use a more fragmented, metrics-focused approach with less comprehensive event coverage.

---

## Arcana Telemetry Capabilities

### 1. Core Architecture

Arcana uses `:telemetry.span/3` consistently across all operations, automatically generating `:start`, `:stop`, and `:exception` event triplets. This provides:

- **Unified timing**: All operations are wrapped in spans with automatic duration tracking
- **Consistent metadata**: Start and stop events share metadata patterns
- **Exception handling**: Automatic exception event emission with stack traces

**Location**: `/home/home/p/g/n/portfolio_index/arcana/lib/arcana/telemetry.ex`

### 2. Event Categories

#### 2.1 Ingest Events (`[:arcana, :ingest, :*]`)
| Event | Measurements | Metadata |
|-------|-------------|----------|
| `:start` | `%{system_time: integer}` | `%{text: String.t(), repo: module(), collection: String.t()}` |
| `:stop` | `%{duration: integer}` | `%{document: Document.t(), chunk_count: integer}` |
| `:exception` | `%{duration: integer}` | `%{kind: atom(), reason: term(), stacktrace: list()}` |

#### 2.2 Search Events (`[:arcana, :search, :*]`)
| Event | Measurements | Metadata |
|-------|-------------|----------|
| `:start` | `%{system_time: integer}` | `%{query: String.t(), repo: module(), mode: atom(), limit: integer}` |
| `:stop` | `%{duration: integer}` | `%{results: list(), result_count: integer}` |
| `:exception` | `%{duration: integer}` | `%{kind: atom(), reason: term(), stacktrace: list()}` |

#### 2.3 Ask/RAG Events (`[:arcana, :ask, :*]`)
| Event | Measurements | Metadata |
|-------|-------------|----------|
| `:start` | `%{system_time: integer}` | `%{question: String.t(), repo: module()}` |
| `:stop` | `%{duration: integer}` | `%{answer: String.t(), context_count: integer}` |
| `:exception` | `%{duration: integer}` | `%{kind: atom(), reason: term(), stacktrace: list()}` |

#### 2.4 Embed Events (`[:arcana, :embed, :*]`)
| Event | Measurements | Metadata |
|-------|-------------|----------|
| `:start` | `%{system_time: integer}` | `%{text: String.t()}` |
| `:stop` | `%{duration: integer}` | `%{dimensions: integer}` |
| `:exception` | `%{duration: integer}` | `%{kind: atom(), reason: term(), stacktrace: list()}` |

#### 2.5 LLM Events (`[:arcana, :llm, :complete, :*]`)
| Event | Measurements | Metadata |
|-------|-------------|----------|
| `:start` | `%{system_time: integer}` | `%{model: String.t(), prompt_length: integer, context_count: integer}` |
| `:stop` | `%{duration: integer}` | `%{success: boolean, response_length: integer}` or `%{success: false, error: String.t()}` |
| `:exception` | `%{duration: integer}` | `%{kind: atom(), reason: term(), stacktrace: list()}` |

#### 2.6 Agent Pipeline Events

All agent steps emit `:start`, `:stop`, and `:exception` events:

| Pipeline Step | Event Prefix | Stop Metadata |
|---------------|--------------|---------------|
| Query Rewrite | `[:arcana, :agent, :rewrite, :*]` | `%{query: String.t(), rewritten_query: String.t()}` |
| Collection Select | `[:arcana, :agent, :select, :*]` | `%{selected_count: integer, selected_collections: [String.t()]}` |
| Query Expand | `[:arcana, :agent, :expand, :*]` | `%{expanded_query: String.t()}` |
| Question Decompose | `[:arcana, :agent, :decompose, :*]` | `%{sub_question_count: integer}` |
| Vector Search | `[:arcana, :agent, :search, :*]` | `%{total_chunks: integer}` |
| Chunk Rerank | `[:arcana, :agent, :rerank, :*]` | `%{kept: integer, original: integer}` |
| Answer Generate | `[:arcana, :agent, :answer, :*]` | `%{}` |
| Self Correction | `[:arcana, :agent, :self_correct, :*]` | `%{attempt: integer}` |

### 3. Built-in Logger (`Arcana.Telemetry.Logger`)

**Location**: `/home/home/p/g/n/portfolio_index/arcana/lib/arcana/telemetry/logger.ex`

Provides ready-to-use telemetry logging with:

- **One-line setup**: `Arcana.Telemetry.Logger.attach()`
- **Configurable log level**: Default `:info`, customizable
- **Custom handler ID**: For multiple handler instances
- **Human-readable output**: Duration formatting, event-specific details
- **Detach support**: Clean teardown with `detach/1`

Example output:
```
[info] [Arcana] search completed in 42ms (15 results)
[info] [Arcana] llm.complete completed in 1.23s [zai:glm-4.7] ok (156 chars) prompt=892chars
[info] [Arcana] agent.rerank completed in 312ms (10/25 kept)
```

---

## Portfolio Libraries Current State

### PortfolioCore Telemetry

**Location**: `/home/home/p/g/n/portfolio_core/lib/portfolio_core/telemetry.ex`

#### Capabilities:
1. **`with_span` macro**: Wraps operations with start/stop/exception events
2. **`emit/3` function**: Simple event emission helper
3. **`measure/3` function**: Duration measurement with status tracking
4. **`events/0` function**: Returns list of all defined events

#### Defined Event Categories:
- **Router events**: Route start/stop/exception, health check
- **Cache events**: Hit/miss/put/delete
- **Agent events**: Run start/stop, tool execute
- **Evaluation events**: RAG triad and hallucination detection
- **Graph events**: Traverse, vector search, community operations

#### Limitations:
- No built-in logger handler
- Limited metadata in events
- Manual event list maintenance
- No unified span approach across adapters

### PortfolioIndex Telemetry

**Location**: `/home/home/p/g/n/portfolio_index/lib/portfolio_index/telemetry.ex`

#### Capabilities:
1. **Supervisor-based**: Runs as part of application supervision tree
2. **Metrics definitions**: Pre-defined `telemetry_metrics` for dashboards
3. **Periodic polling**: Via `telemetry_poller` (currently empty)
4. **Basic logging handler**: `attach_default_handlers/0`

#### Defined Metrics:
- Vector store: store count, search count/duration, batch count
- Graph store: create node/edge count, query count/duration
- Embedder: embed count, batch count, tokens, duration
- LLM: complete count, input/output tokens, duration
- Pipeline: file processed count, chunks per file
- RAG: retrieve count/duration, items returned

#### Actual Telemetry Usage in Adapters:
Individual adapters emit events via `:telemetry.execute/3`:
- LLM adapters (Anthropic, Gemini, OpenAI): Basic count events
- Embedder adapters: Operation events with measurements
- Vector store (PgVector): Query events
- Graph store (Neo4j): Traversal and community events
- RAG strategies: Retrieve events

#### Limitations:
- **No span-based approach**: Uses discrete events, not spans
- **Inconsistent metadata**: Each adapter defines its own schema
- **No exception events**: Most adapters only emit success events
- **Minimal logging handler**: Only logs at debug level with limited formatting
- **No start events**: Only stop/completion events are typically emitted

---

## Identified Gaps

### Gap 1: Span-Based Telemetry Pattern

- **Arcana Feature**: Uses `:telemetry.span/3` for all operations, automatically generating `:start`, `:stop`, and `:exception` event triplets with consistent timing
- **Missing From**: PortfolioIndex (adapters use discrete events), partially in PortfolioCore (has `with_span` but not universally applied)
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Arcana wraps every operation (ingest, search, ask, embed, LLM, agent steps) in telemetry spans
  - PortfolioIndex adapters use `emit_telemetry/3` helper functions that call `:telemetry.execute/3` directly
  - No automatic exception tracking in portfolio adapters
  - Requires refactoring adapter implementations to use span pattern

### Gap 2: Built-in Telemetry Logger

- **Arcana Feature**: `Arcana.Telemetry.Logger` module with one-line attach, human-readable formatting, configurable log levels, and event-specific detail extraction
- **Missing From**: Both PortfolioCore and PortfolioIndex
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana's logger provides formatted output like `[Arcana] llm.complete completed in 1.23s [model] ok (chars)`
  - PortfolioIndex has basic `attach_default_handlers/0` but only logs at debug level with raw `inspect/1` output
  - PortfolioCore has no built-in logger handler at all
  - Should create `PortfolioIndex.Telemetry.Logger` following Arcana's pattern

### Gap 3: LLM Telemetry Enrichment

- **Arcana Feature**: LLM events include `model`, `prompt_length`, `context_count`, `success`, `response_length`, and `error` details
- **Missing From**: PortfolioIndex LLM adapters
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana's `[:arcana, :llm, :complete, :*]` events provide comprehensive LLM observability
  - PortfolioIndex Anthropic adapter only emits `%{count: 1}` measurement
  - No tracking of prompt length, response length, or success/failure status
  - No model identifier in telemetry events

### Gap 4: Agent Pipeline Telemetry

- **Arcana Feature**: Dedicated telemetry events for each agent pipeline step (rewrite, select, expand, decompose, search, rerank, answer, self_correct)
- **Missing From**: PortfolioIndex RAG strategies
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Arcana tracks 8 distinct pipeline steps with specific metadata for each
  - PortfolioIndex agentic RAG strategy has minimal telemetry (basic iterate events)
  - No visibility into individual pipeline step durations or outcomes
  - Critical for debugging and optimizing RAG pipelines

### Gap 5: Search Mode and Query Context Tracking

- **Arcana Feature**: Search events include `mode` (semantic, keyword, hybrid), `limit`, and full `query` string
- **Missing From**: PortfolioIndex vector store telemetry
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana's `[:arcana, :search, :*]` events capture search strategy used
  - PortfolioIndex only tracks generic search count/duration
  - No ability to compare performance across search modes
  - Missing query string makes debugging difficult

### Gap 6: Ingest/Chunking Telemetry

- **Arcana Feature**: Ingest events track `text`, `collection`, `document`, and `chunk_count`
- **Missing From**: PortfolioIndex ingestion pipeline
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana provides end-to-end ingest visibility
  - PortfolioIndex ingestion pipeline emits `file_processed` events but lacks document/chunk detail
  - No collection context in ingestion telemetry
  - Cannot correlate ingestion with downstream search performance

### Gap 7: Embedding Dimension Tracking

- **Arcana Feature**: Embed events include `dimensions` in stop metadata
- **Missing From**: PortfolioIndex embedder adapters
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana tracks embedding vector dimensions for observability
  - PortfolioIndex embedders track tokens but not output dimensions
  - Useful for validating embedding model configuration
  - Important when supporting multiple embedding models

### Gap 8: Exception Event Consistency

- **Arcana Feature**: All operations emit `:exception` events with `kind`, `reason`, and `stacktrace`
- **Missing From**: PortfolioIndex adapters (no exception events)
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Arcana's span pattern automatically captures exceptions
  - PortfolioIndex adapters typically return error tuples without telemetry
  - No visibility into failure rates or error patterns
  - Critical for production monitoring and alerting

### Gap 9: Reranker Telemetry

- **Arcana Feature**: `[:arcana, :agent, :rerank, :*]` events with `kept` and `original` chunk counts
- **Missing From**: PortfolioIndex reranker adapters
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana tracks reranking effectiveness (kept/original ratio)
  - PortfolioIndex LLM reranker emits basic events but no kept/original metrics
  - Cannot measure reranking impact on result quality
  - Important for tuning reranking thresholds

### Gap 10: Batch Embedding Telemetry

- **Arcana Feature**: `[:arcana, :embed_batch, :stop]` events for batch operations
- **Missing From**: PortfolioIndex embedder batch operations (partial)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana Logger handles `embed_batch` events with count extraction
  - PortfolioIndex defines `embed_batch.count` metric but implementation varies by adapter
  - Need consistent batch telemetry across all embedder implementations

### Gap 11: Self-Correction Iteration Tracking

- **Arcana Feature**: `[:arcana, :agent, :self_correct, :*]` events with `attempt` count
- **Missing From**: PortfolioIndex Self-RAG strategy
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana tracks each self-correction iteration for observability
  - PortfolioIndex Self-RAG has no iteration telemetry
  - Cannot measure correction loop depth or convergence
  - Important for understanding answer quality improvement

### Gap 12: Query Expansion/Decomposition Telemetry

- **Arcana Feature**: Separate events for query expansion (`expanded_query`) and decomposition (`sub_question_count`)
- **Missing From**: PortfolioIndex query processing
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana distinguishes between expansion (rephrasing) and decomposition (sub-questions)
  - PortfolioIndex has no dedicated query transformation telemetry
  - Cannot measure query transformation effectiveness
  - Impacts debugging of multi-step query processing

---

## Implementation Priority

### High Priority (Immediate Value)

1. **Gap 2: Built-in Telemetry Logger** - Quick win, improves debugging experience
2. **Gap 8: Exception Event Consistency** - Critical for production monitoring
3. **Gap 3: LLM Telemetry Enrichment** - High-value observability for expensive operations
4. **Gap 4: Agent Pipeline Telemetry** - Essential for RAG optimization

### Medium Priority (Significant Value)

5. **Gap 1: Span-Based Telemetry Pattern** - Architectural improvement, requires refactoring
6. **Gap 5: Search Mode Tracking** - Enables performance comparison
7. **Gap 9: Reranker Telemetry** - Measures reranking effectiveness
8. **Gap 11: Self-Correction Tracking** - Self-RAG optimization

### Lower Priority (Nice to Have)

9. **Gap 6: Ingest/Chunking Telemetry** - Improves ingestion visibility
10. **Gap 7: Embedding Dimension Tracking** - Minor observability enhancement
11. **Gap 10: Batch Embedding Telemetry** - Standardization
12. **Gap 12: Query Transformation Telemetry** - Advanced debugging

---

## Technical Dependencies

### Required Dependencies

| Gap | Dependencies |
|-----|--------------|
| Gap 1 (Spans) | Refactor of all adapter implementations |
| Gap 2 (Logger) | None - can be added independently |
| Gap 3 (LLM) | Modify LLM adapter implementations |
| Gap 4 (Pipeline) | Modify RAG strategy implementations |

### Cross-Cutting Concerns

1. **Event Naming Convention**: Adopt consistent `[:portfolio_index, :component, :operation, :phase]` pattern
2. **Metadata Schema**: Define standardized metadata structures per event type
3. **Duration Units**: Standardize on native time units with conversion helpers
4. **Error Format**: Consistent exception metadata structure

### Integration Points

- **PortfolioCore**: Leverage existing `with_span` macro, extend event definitions
- **Telemetry Libraries**: Compatible with `telemetry_metrics`, `prom_ex`, `opentelemetry`
- **Logging**: Integrate with Elixir Logger via handler module

---

## Recommendations

### Immediate Actions

1. Create `PortfolioIndex.Telemetry.Logger` module following Arcana's pattern
2. Add `:exception` events to critical paths (LLM, embedder, vector store)
3. Enrich LLM telemetry with model, prompt length, response metrics

### Short-Term (1-2 sprints)

4. Refactor RAG strategies to emit pipeline step events
5. Add reranker telemetry with kept/original metrics
6. Implement search mode tracking in vector store adapters

### Medium-Term (1-2 months)

7. Migrate adapters to span-based pattern
8. Standardize telemetry metadata schemas
9. Add comprehensive documentation with examples

---

## Appendix: File Locations

### Arcana Files
- `/home/home/p/g/n/portfolio_index/arcana/lib/arcana/telemetry.ex`
- `/home/home/p/g/n/portfolio_index/arcana/lib/arcana/telemetry/logger.ex`
- `/home/home/p/g/n/portfolio_index/arcana/lib/arcana/agent.ex` (telemetry usage)
- `/home/home/p/g/n/portfolio_index/arcana/lib/arcana/llm.ex` (telemetry usage)

### Portfolio Files
- `/home/home/p/g/n/portfolio_core/lib/portfolio_core/telemetry.ex`
- `/home/home/p/g/n/portfolio_index/lib/portfolio_index/telemetry.ex`
- `/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/llm/anthropic.ex` (example adapter)
- `/home/home/p/g/n/portfolio_index/lib/portfolio_index/rag/strategies/agentic.ex` (example strategy)
</file>

<file path="07_mix_tasks.md">
# Mix Tasks Gap Analysis

This document analyzes the mix tasks in Arcana and compares them against the portfolio libraries (portfolio_manager and portfolio_coder) to identify gaps in RAG-related functionality.

## Arcana Mix Tasks

### 1. `mix arcana.install`
**File:** `/home/home/p/g/n/portfolio_index/arcana/lib/mix/tasks/arcana.install.ex`

A comprehensive installation task that sets up Arcana in a Phoenix application.

**Features:**
- Generates migrations for `arcana_documents`, `arcana_chunks`, and `arcana_collections` tables
- Creates pgvector extension setup
- Sets up HNSW index for vector similarity search
- Creates evaluation tables (`arcana_evaluation_test_cases`, `arcana_evaluation_runs`)
- Generates Postgrex types module for pgvector support
- Configures repo types automatically
- Adds dashboard route to Phoenix router (optional)
- Supports Igniter for automatic project modification
- Falls back to basic migration generation if Igniter not available

**Options:**
- `--no-dashboard` - Skip adding dashboard route
- `--repo` - Specify custom repo module

### 2. `mix arcana.gen.embedding_migration`
**File:** `/home/home/p/g/n/portfolio_index/arcana/lib/mix/tasks/arcana.gen.embedding_migration.ex`

Generates a migration for updating vector column dimensions when switching embedding models.

**Features:**
- Auto-detects current embedding configuration dimensions
- Generates migration to drop and recreate HNSW index
- Updates vector column size
- Provides clear post-migration instructions

**Options:**
- `--dimensions` - Override auto-detected dimensions

### 3. `mix arcana.reembed`
**File:** `/home/home/p/g/n/portfolio_index/arcana/lib/mix/tasks/arcana.reembed.ex`

Re-embeds all documents with the current embedding configuration.

**Features:**
- Batch processing with configurable size
- Progress bar visualization
- Shows current embedding configuration (model, dimensions)
- Reports rechunked documents and total chunks
- Quiet mode for scripting

**Options:**
- `--batch-size` - Number of chunks per batch (default: 50)
- `--quiet` - Suppress progress output

### 4. `mix arcana.eval.generate`
**File:** `/home/home/p/g/n/portfolio_index/arcana/lib/mix/tasks/arcana.eval.generate.ex`

Generates synthetic test cases for retrieval evaluation.

**Features:**
- Samples chunks from existing data
- Uses LLM to generate evaluation questions
- Filters by source ID or collection
- Stores test cases in database

**Options:**
- `--sample-size` - Number of chunks to sample (default: 50)
- `--source-id` - Limit to specific source
- `--collection` - Limit to specific collection

### 5. `mix arcana.eval.run`
**File:** `/home/home/p/g/n/portfolio_index/arcana/lib/mix/tasks/arcana.eval.run.ex`

Runs retrieval evaluation and reports metrics.

**Features:**
- Multiple search modes: semantic, fulltext, hybrid
- Comprehensive metrics: Recall@1/3/5/10, Precision@1/5, MRR, Hit Rate@5
- Table and JSON output formats
- CI integration with `--fail-under` threshold
- Auto-generate test cases if none exist

**Options:**
- `--mode` - Search mode (semantic, fulltext, hybrid)
- `--source-id` - Limit to specific source
- `--generate` - Generate test cases first
- `--sample-size` - Sample size for generation
- `--format` - Output format (table, json)
- `--fail-under` - CI threshold for Recall@5

---

## Portfolio Libraries Mix Tasks

### PortfolioManager Tasks

#### 1. `mix portfolio.index`
**File:** `/home/home/p/g/n/portfolio_manager/lib/mix/tasks/portfolio.index.ex`

Indexes a repository for RAG queries.

**Features:**
- Path-based indexing
- Configurable file extensions
- Named index support

**Options:**
- `--index` - Index name (default: default)
- `--extensions` - File extensions to include (default: .ex,.exs,.md)

#### 2. `mix portfolio.search`
**File:** `/home/home/p/g/n/portfolio_manager/lib/mix/tasks/portfolio.search.ex`

Searches portfolio content using RAG retrieval.

**Features:**
- Query-based search
- Configurable result count
- Score display
- Content snippets

**Options:**
- `--index` - Vector index to query
- `--k` - Number of results (default: 10)

#### 3. `mix portfolio.graph`
**File:** `/home/home/p/g/n/portfolio_manager/lib/mix/tasks/portfolio.graph.ex`

Graph operations for portfolio analysis.

**Features:**
- Build dependency graphs
- Show graph statistics
- Language-specific analysis

**Options:**
- `--graph` - Graph ID
- `--language` - Dependency language

#### 4. `mix portfolio.ask`
**File:** `/home/home/p/g/n/portfolio_manager/lib/mix/tasks/portfolio.ask.ex`

Ask questions using RAG.

**Features:**
- Multiple RAG strategies (hybrid, self_rag, graph_rag)
- Streaming support
- Configurable result count

**Options:**
- `--strategy` - RAG strategy
- `--index` - Vector index
- `--k` - Number of results
- `--stream` - Stream response

### PortfolioCoder Tasks

#### 1. `mix code.index`
**File:** `/home/home/p/g/n/portfolio_coder/lib/mix/tasks/code.index.ex`

Indexes a code repository for semantic search.

**Features:**
- Multi-language support
- Exclude patterns
- Detailed indexing results

**Options:**
- `--index` - Index name
- `--languages` - Languages to index
- `--exclude` - Patterns to exclude

#### 2. `mix code.search`
**File:** `/home/home/p/g/n/portfolio_coder/lib/mix/tasks/code.search.ex`

Searches indexed code repositories.

**Features:**
- Language filtering
- File pattern filtering
- Score-based ranking

**Options:**
- `--index` - Index to search
- `--language` - Filter by language
- `--limit` - Max results
- `--file` - File pattern filter

#### 3. `mix code.ask`
**File:** `/home/home/p/g/n/portfolio_coder/lib/mix/tasks/code.ask.ex`

Ask questions about indexed code.

**Features:**
- RAG-based Q&A
- Streaming support

**Options:**
- `--index` - Index to use
- `--stream` - Stream response

#### 4. `mix code.deps`
**File:** `/home/home/p/g/n/portfolio_coder/lib/mix/tasks/code.deps.ex`

Analyze code dependencies.

**Features:**
- Build dependency graphs
- Show forward dependencies
- Show reverse dependencies
- Find circular dependencies

**Options:**
- `--graph` - Graph name
- `--language` - Project language
- `--depth` - Traversal depth

---

## Identified Gaps

### Gap 1: Installation/Setup Task
- **Arcana Task**: `mix arcana.install` - Comprehensive setup with migrations, pgvector config, dashboard routes, and Igniter integration
- **Missing From**: Both portfolio_manager and portfolio_coder
- **Implementation Complexity**: High
- **Technical Details**:
  - Portfolio libs lack any installation task
  - No migration generation for vector store setup
  - No pgvector types configuration automation
  - No dashboard route injection
  - Would need to create migrations for indexes, document schemas, and chunk tables
  - Consider Igniter integration for automatic project modification

### Gap 2: Embedding Migration Generator
- **Arcana Task**: `mix arcana.gen.embedding_migration` - Auto-detects dimensions, generates migration for dimension changes
- **Missing From**: Both portfolio_manager and portfolio_coder
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Portfolio libs have no mechanism for embedding model dimension changes
  - HNSW index recreation is not automated
  - Manual migrations would be required for each dimension change
  - Requires access to embedder configuration to detect dimensions
  - Depends on: Embedding adapter with `dimensions/1` callback

### Gap 3: Re-embedding Task
- **Arcana Task**: `mix arcana.reembed` - Batch re-embeds all documents with current config
- **Missing From**: Both portfolio_manager and portfolio_coder
- **Implementation Complexity**: Medium
- **Technical Details**:
  - No way to re-embed existing documents after model change
  - Would require batch processing infrastructure
  - Progress tracking needed for large datasets
  - Depends on: Maintenance module with `reembed/2` function
  - Should detect rechunked documents vs. re-embedded only

### Gap 4: Evaluation Test Case Generation
- **Arcana Task**: `mix arcana.eval.generate` - LLM-based synthetic test case generation
- **Missing From**: Both portfolio_manager and portfolio_coder
- **Implementation Complexity**: High
- **Technical Details**:
  - No RAG evaluation infrastructure in portfolio libs
  - Requires LLM integration for question generation
  - Needs evaluation database schema (test_cases, test_case_chunks)
  - Sampling strategy needed for chunk selection
  - Collection/source filtering for targeted evaluation
  - Depends on: LLM adapter, Evaluation module

### Gap 5: Evaluation Runner
- **Arcana Task**: `mix arcana.eval.run` - Comprehensive retrieval metrics
- **Missing From**: Both portfolio_manager and portfolio_coder
- **Implementation Complexity**: High
- **Technical Details**:
  - No retrieval quality metrics in portfolio libs
  - Missing metrics: Recall@k, Precision@k, MRR, Hit Rate
  - No CI integration for quality thresholds
  - Requires evaluation infrastructure from Gap 4
  - Multiple output formats needed (table, JSON)
  - Search mode comparison (semantic vs fulltext vs hybrid)
  - Depends on: Evaluation module, test cases from Gap 4

### Gap 6: Collection/Source Filtering
- **Arcana Task**: Multiple tasks support `--collection` and `--source-id` filtering
- **Missing From**: portfolio_manager (partial), portfolio_coder (missing collection concept)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana has first-class collection support
  - Portfolio libs use index_id but lack collection hierarchy
  - Source ID filtering useful for multi-repo setups

### Gap 7: Dashboard Integration
- **Arcana Task**: `mix arcana.install --dashboard` adds dashboard routes
- **Missing From**: Both portfolio_manager and portfolio_coder
- **Implementation Complexity**: Medium (if dashboard exists)
- **Technical Details**:
  - Portfolio libs have no web dashboard
  - No visualization of indexes, chunks, or search results
  - Would require Phoenix LiveView components
  - Router integration for mounting dashboard

### Gap 8: Quiet/Scripting Mode
- **Arcana Task**: `mix arcana.reembed --quiet` suppresses progress output
- **Missing From**: All portfolio_manager and portfolio_coder tasks
- **Implementation Complexity**: Low
- **Technical Details**:
  - Portfolio tasks always produce output
  - No scripting-friendly quiet mode
  - Impacts automation and CI pipelines

---

## Implementation Priority

### Priority 1 (Critical for RAG Operations)
1. **Re-embedding Task** - Essential for model changes, common operation
2. **Embedding Migration Generator** - Required when switching models
3. **Installation Task** - Streamlines adoption, reduces setup friction

### Priority 2 (Important for Quality Assurance)
4. **Evaluation Test Case Generation** - Enables quality measurement
5. **Evaluation Runner** - Critical for RAG quality validation in CI

### Priority 3 (Nice to Have)
6. **Collection/Source Filtering** - Improves multi-project support
7. **Quiet Mode** - Better CI/scripting support
8. **Dashboard Integration** - Visualization and debugging

---

## Technical Dependencies

### For portfolio_index
To implement these gaps, portfolio_index needs:
1. **Maintenance Module** - For reembed functionality
2. **Evaluation Module** - For test case generation and running
3. **Embedder Adapter with Dimensions** - For auto-detection
4. **Database Schema for Evaluation** - Test cases, runs, metrics

### For portfolio_manager
1. Depends on portfolio_index for underlying RAG functionality
2. Needs to expose maintenance operations via Mix tasks
3. Evaluation should wrap portfolio_index evaluation

### For portfolio_coder
1. Similar dependencies as portfolio_manager
2. Code-specific evaluation metrics may be needed
3. Language-aware test case generation could be valuable

### Cross-Cutting Concerns
1. **Igniter Integration** - Optional but valuable for all install tasks
2. **Progress Callbacks** - Standardize progress reporting across tasks
3. **Output Formatting** - JSON output for all tasks (CI integration)
4. **Threshold Validation** - `--fail-under` pattern for CI gates

---

## Summary Table

| Task | Arcana | portfolio_manager | portfolio_coder | Gap Severity |
|------|--------|-------------------|-----------------|--------------|
| Install/Setup | Yes | No | No | High |
| Index | Via API | Yes | Yes | None |
| Search | Via API | Yes | Yes | None |
| Ask/Query | Via API | Yes | Yes | None |
| Graph/Deps | Via API | Yes | Yes | None |
| Embedding Migration | Yes | No | No | Medium |
| Re-embed | Yes | No | No | High |
| Eval Generate | Yes | No | No | High |
| Eval Run | Yes | No | No | High |
| Dashboard | Yes | No | No | Medium |
| Quiet Mode | Partial | No | No | Low |
</file>

<file path="08_maintenance_and_documents.md">
# Maintenance, Documents, and Pipeline Features Gap Analysis

## Executive Summary

This document analyzes the gap between Arcana's maintenance, rewriter, parser, document/collection/chunk model systems and the equivalent functionality in the Portfolio libraries (portfolio_core, portfolio_index, portfolio_manager). The analysis focuses exclusively on RAG functionality.

Arcana provides a unified, Ecto-backed document management system with built-in maintenance operations, while Portfolio takes a more distributed approach with Broadway pipelines and adapter-based architecture. Key gaps include production maintenance utilities, Ecto schema-backed document models, unified collection management, and LLM-based query rewriting.

---

## Arcana Capabilities

### Maintenance Features

**File**: `/home/home/p/g/n/portfolio_index/arcana/lib/arcana/maintenance.ex`

Arcana provides production-ready maintenance functions callable from releases:

1. **Re-embedding System** (`reembed/2`)
   - Batch re-embeds all chunks when switching embedding models
   - Rechunks documents that have no chunks (status: pending or chunk_count: 0)
   - Configurable batch size and progress callbacks
   - Runs within Ecto transactions for consistency
   - Streaming support for large datasets via `repo.stream/2`

2. **Embedding Diagnostics**
   - `embedding_dimensions/0` - Returns configured embedding dimensions
   - `embedding_info/0` - Returns comprehensive embedding configuration including type, model, and dimensions

3. **Production Deployment Support**
   - Designed for release environments (no mix tasks required)
   - Callable via IEx remote shell or release eval commands
   - Example: `bin/my_app eval "Arcana.Maintenance.reembed(MyApp.Repo)"`

### Rewriters

**File**: `/home/home/p/g/n/portfolio_index/arcana/lib/arcana/rewriters.ex`

LLM-based query rewriting helpers:

1. **Query Expansion** (`expand/2`)
   - Expands queries with synonyms and related terms
   - Uses LLM protocol for provider-agnostic operation
   - Customizable prompt templates with `{query}` placeholder

2. **Keyword Extraction** (`keywords/2`)
   - Extracts key search terms from natural language queries
   - Reduces noise for better keyword-based retrieval

3. **Query Decomposition** (`decompose/2`)
   - Breaks complex multi-part questions into simpler sub-queries
   - Enables multi-hop retrieval strategies

4. **Dual API Pattern**
   - Direct use: `Rewriters.expand("ML models", llm: my_llm)`
   - Curried function: `Arcana.search("query", rewriter: Rewriters.expand(llm: my_llm))`

### Parser

**File**: `/home/home/p/g/n/portfolio_index/arcana/lib/arcana/parser.ex`

File parsing with format detection:

1. **Supported Formats**
   - Text formats: `.txt`, `.md`, `.markdown`
   - PDF parsing via `pdftotext` (poppler-utils)

2. **PDF Support**
   - Magic byte validation (`%PDF` header check)
   - External dependency detection (`pdf_support_available?/0`)
   - Layout-preserving extraction (`pdftotext -layout`)

3. **Error Handling**
   - `:file_not_found`, `:read_error`, `:unsupported_format`
   - `:invalid_pdf`, `:pdf_parse_error`, `:pdf_support_not_available`

### Document Model

**File**: `/home/home/p/g/n/portfolio_index/arcana/lib/arcana/document.ex`

Ecto schema for documents:

```elixir
schema "arcana_documents" do
  field(:content, :string)
  field(:content_type, :string, default: "text/plain")
  field(:source_id, :string)
  field(:file_path, :string)
  field(:metadata, :map, default: %{})
  field(:status, Ecto.Enum, values: [:pending, :processing, :completed, :failed])
  field(:error, :string)
  field(:chunk_count, :integer, default: 0)
  belongs_to(:collection, Arcana.Collection)
  has_many(:chunks, Arcana.Chunk)
  timestamps()
end
```

Key features:
- Binary UUID primary key
- Processing status tracking with Ecto.Enum
- Collection relationship for segmentation
- Chunk count for quick aggregation
- Error message storage for failed ingestions

### Collection Model

**File**: `/home/home/p/g/n/portfolio_index/arcana/lib/arcana/collection.ex`

Ecto schema for document collections:

```elixir
schema "arcana_collections" do
  field(:name, :string)
  field(:description, :string)
  has_many(:documents, Arcana.Document)
  timestamps()
end
```

Key features:
- Unique constraint on name
- Optional description (used for agent-based collection selection)
- `get_or_create/3` helper for upsert semantics
- Lazy description updates (only updates if existing is nil/empty)

### Chunk Model

**File**: `/home/home/p/g/n/portfolio_index/arcana/lib/arcana/chunk.ex`

Ecto schema for vector chunks:

```elixir
schema "arcana_chunks" do
  field(:text, :string)
  field(:embedding, Pgvector.Ecto.Vector)
  field(:chunk_index, :integer, default: 0)
  field(:token_count, :integer)
  field(:metadata, :map, default: %{})
  belongs_to(:document, Arcana.Document)
  timestamps()
end
```

Key features:
- Native pgvector integration (`Pgvector.Ecto.Vector`)
- Document relationship with foreign key constraint
- Token count for cost estimation
- Chunk ordering via `chunk_index`

### Main Module Integration

**File**: `/home/home/p/g/n/portfolio_index/arcana/lib/arcana.ex`

Unified API with:
- `ingest/2` and `ingest_file/2` for document ingestion
- `search/2` with semantic, fulltext, and hybrid modes
- `ask/2` for RAG question-answering
- `rewrite_query/2` for query preprocessing
- `delete/2` for document removal
- Telemetry spans for observability
- RRF (Reciprocal Rank Fusion) for hybrid search
- Collection-aware search with multi-collection support

---

## Portfolio Libraries Current State

### portfolio_core

**DocumentStore Port** (`/home/home/p/g/n/portfolio_core/lib/portfolio_core/ports/document_store.ex`):
- Behaviour definition only (no implementation)
- CRUD operations: `store`, `get`, `delete`, `list`
- Metadata search via `search_metadata`
- Type definitions for document structure

### portfolio_index

**DocumentStore Postgres Adapter** (`/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/document_store/postgres.ex`):
- Raw SQL-based implementation
- Content-addressable storage (SHA256 hashing)
- Store namespace isolation via `store_id`
- No Ecto schema (uses raw Postgrex queries)
- Additional helpers: `exists_by_hash?`, `get_by_hash`

**Ingestion Pipeline** (`/home/home/p/g/n/portfolio_index/lib/portfolio_index/pipelines/ingestion.ex`):
- Broadway-based file processing
- FileProducer for file discovery
- Basic file type detection
- Chunking integration with Recursive chunker
- No document status tracking

**Embedding Pipeline** (`/home/home/p/g/n/portfolio_index/lib/portfolio_index/pipelines/embedding.ex`):
- Broadway-based embedding generation
- ETS-based internal queuing
- Rate limiting via Hammer
- Batch storage to vector store

**Chunker** (`/home/home/p/g/n/portfolio_index/lib/portfolio_index/adapters/chunker/recursive.ex`):
- Format-aware splitting (17+ formats via Separators module)
- Configurable size function (character or token-based)
- Overlap support
- Offset tracking

### portfolio_manager

**RAG Module** (`/home/home/p/g/n/portfolio_manager/lib/portfolio_manager/rag.ex`):
- Strategy-based retrieval (hybrid, self_rag, graph_rag, agentic)
- Manifest-driven configuration
- Basic streaming support (`stream_query`, `stream_search`)
- Repository indexing (`index_repo`)
- No query rewriting

**Pipeline Module** (`/home/home/p/g/n/portfolio_manager/lib/portfolio_manager/pipeline.ex`):
- DAG-based orchestration
- Dependency resolution
- Parallel execution
- Caching via ETS
- Error policies (halt, continue, retry)

---

## Identified Gaps

### Gap 1: Production Maintenance Utilities

- **Arcana Feature**: `Arcana.Maintenance` module provides `reembed/2`, `embedding_dimensions/0`, and `embedding_info/0` for production operations without mix tasks.
- **Missing From**: portfolio_index, portfolio_manager
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Portfolio lacks any production maintenance tooling
  - No way to re-embed chunks when switching embedding models
  - No diagnostic functions for verifying embedding configuration
  - Would require adding chunk tracking to document store
  - Batch processing with streaming would need Ecto integration

### Gap 2: Ecto Schema-Backed Document Model

- **Arcana Feature**: `Arcana.Document` Ecto schema with status tracking, collection relationship, and chunk count.
- **Missing From**: portfolio_index (uses raw SQL)
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Current Postgres adapter uses raw Postgrex queries
  - No document lifecycle status (pending, processing, completed, failed)
  - No error message storage for failed ingestions
  - No chunk count tracking
  - Would require database migration and schema definition
  - Foreign key relationships to chunks not defined

### Gap 3: Collection/Namespace Management

- **Arcana Feature**: `Arcana.Collection` schema with `get_or_create/3`, unique constraints, and description for agent routing.
- **Missing From**: portfolio_index
- **Implementation Complexity**: Low
- **Technical Details**:
  - Portfolio uses `store_id` string for namespace isolation
  - No first-class Collection entity
  - No collection metadata (description, etc.)
  - Descriptions are used by Arcana's agent for collection selection
  - Would require new schema and migration

### Gap 4: Chunk Ecto Schema with Vector Type

- **Arcana Feature**: `Arcana.Chunk` with native `Pgvector.Ecto.Vector` type, document relationship, and metadata.
- **Missing From**: portfolio_index
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Portfolio stores vectors via VectorStore adapter (no Ecto schema)
  - No direct Ecto integration for pgvector
  - Chunk metadata is stored but without schema validation
  - Would require Pgvector.Ecto dependency

### Gap 5: LLM-Based Query Rewriters

- **Arcana Feature**: `Arcana.Rewriters` with `expand`, `keywords`, and `decompose` for query preprocessing.
- **Missing From**: portfolio_manager, portfolio_index
- **Implementation Complexity**: Low
- **Technical Details**:
  - Portfolio RAG has no query rewriting capability
  - Arcana integrates rewriters into search via `:rewriter` option
  - Uses LLM protocol for provider-agnostic operation
  - Customizable prompts with `{query}` placeholder
  - Easy to port using existing LLM adapter infrastructure

### Gap 6: File Parser with PDF Support

- **Arcana Feature**: `Arcana.Parser` with text/markdown/PDF parsing and magic byte validation.
- **Missing From**: portfolio_index (only basic file reading)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Ingestion pipeline does `File.read` without format parsing
  - No PDF text extraction
  - No content type detection
  - Would need poppler-utils dependency documentation
  - Error handling for various file issues not present

### Gap 7: Document Processing Status Tracking

- **Arcana Feature**: Document `status` field with Ecto.Enum (`:pending`, `:processing`, `:completed`, `:failed`) and `error` field.
- **Missing From**: portfolio_index
- **Implementation Complexity**: Low
- **Technical Details**:
  - No way to track document ingestion state
  - Failed ingestions are logged but not queryable
  - No mechanism to retry failed documents
  - Would enable building retry logic and progress monitoring

### Gap 8: Unified Ingestion API

- **Arcana Feature**: `Arcana.ingest/2` and `Arcana.ingest_file/2` with collection support, telemetry, and automatic chunking/embedding.
- **Missing From**: portfolio_index (Broadway pipelines only)
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Portfolio requires starting Broadway pipelines
  - No simple function-based ingestion API
  - Arcana provides synchronous ingestion for simpler use cases
  - Portfolio's async approach is better for large-scale, but lacks simple API

### Gap 9: Hybrid Search with RRF in Core Module

- **Arcana Feature**: `Arcana.search/2` with `:mode` option (`:semantic`, `:fulltext`, `:hybrid`) and built-in RRF.
- **Missing From**: portfolio_index (strategy-based only in portfolio_manager)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Portfolio uses RAG strategies for hybrid search
  - No simple search mode option in base API
  - RRF exists in PortfolioCore.VectorStore.RRF but not unified
  - Would simplify API surface

### Gap 10: Collection-Filtered Search

- **Arcana Feature**: Search with `:collection` or `:collections` option for multi-collection queries.
- **Missing From**: portfolio_index (uses index_id)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Portfolio uses `index_id` which is similar but not identical
  - No support for searching multiple collections in single query
  - Arcana merges results across collections with RRF

### Gap 11: Ask/QA API with Context

- **Arcana Feature**: `Arcana.ask/2` combines search + LLM completion with custom prompt support.
- **Missing From**: portfolio_index (exists in portfolio_manager.RAG but differently structured)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Arcana returns `{:ok, answer, context}` with context chunks
  - Portfolio's `ask` is more basic
  - Custom prompt function support missing in Portfolio
  - LLM protocol integration provides flexibility

### Gap 12: Telemetry Spans for Ingestion/Search

- **Arcana Feature**: `:telemetry.span/3` calls for `[:arcana, :ingest]`, `[:arcana, :search]`, `[:arcana, :ask]`.
- **Missing From**: portfolio_index (has some telemetry but not comprehensive spans)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Portfolio has telemetry for pipeline stages
  - No unified ingestion/search span telemetry
  - Would improve observability and debugging

### Gap 13: Memory Vector Store Backend

- **Arcana Feature**: `Arcana.VectorStore.Memory` for testing and small-scale RAG without pgvector.
- **Missing From**: portfolio_index (hardcoded to Pgvector)
- **Implementation Complexity**: Medium
- **Technical Details**:
  - Portfolio's VectorStore adapters are Pgvector-centric
  - No in-memory option for testing
  - Arcana uses HNSWLib for memory backend
  - Useful for local development without Postgres

### Gap 14: Configurable Embedder Resolution

- **Arcana Feature**: `Arcana.embedder/0` with `:local`, `:openai`, function, or custom module support.
- **Missing From**: portfolio_index (fixed adapter pattern)
- **Implementation Complexity**: Low
- **Technical Details**:
  - Portfolio uses manifest-driven adapter resolution
  - Arcana provides simpler config-based approach
  - Anonymous function support for testing
  - Both approaches valid but Arcana's is simpler for single-app use

---

## Implementation Priority

### High Priority (Core Functionality Gaps)

1. **Gap 2: Ecto Schema-Backed Document Model** - Foundation for other features
2. **Gap 7: Document Processing Status Tracking** - Enables retry logic
3. **Gap 5: LLM-Based Query Rewriters** - Low effort, high value for retrieval quality
4. **Gap 1: Production Maintenance Utilities** - Essential for production operations

### Medium Priority (Enhanced Features)

5. **Gap 4: Chunk Ecto Schema with Vector Type** - Better data integrity
6. **Gap 3: Collection/Namespace Management** - Improves organization
7. **Gap 8: Unified Ingestion API** - Better developer experience
8. **Gap 6: File Parser with PDF Support** - Common use case

### Lower Priority (Nice-to-Have)

9. **Gap 9: Hybrid Search with RRF in Core Module** - Already available via strategies
10. **Gap 11: Ask/QA API with Context** - Partially exists in portfolio_manager
11. **Gap 12: Telemetry Spans** - Incremental improvement
12. **Gap 10: Collection-Filtered Search** - Similar to existing index_id
13. **Gap 13: Memory Vector Store Backend** - Testing convenience
14. **Gap 14: Configurable Embedder Resolution** - Different but equivalent approach

---

## Technical Dependencies

### Required for Document/Chunk Schemas

- **Ecto**: Already available in portfolio_index
- **Pgvector.Ecto**: Add `{:pgvector, "~> 0.3"}` to deps
- **Database migrations**: New tables for documents, collections, chunks

### Required for Parser

- **Poppler-utils**: System dependency for PDF parsing
- **External command execution**: Already using System.cmd in other areas

### Required for Memory Backend

- **HNSWLib**: `{:hnswlib, "~> 0.1"}` for in-memory vector search
- **GenServer**: For Memory process management

### Required for Query Rewriters

- **LLM Adapter**: Already available in portfolio_index
- **Protocol definition**: May want to extract Arcana.LLM protocol pattern

---

## Migration Path

### Phase 1: Schema Foundation

1. Add Pgvector.Ecto dependency
2. Create Document Ecto schema with status tracking
3. Create Collection Ecto schema
4. Create Chunk Ecto schema with vector type
5. Write database migrations

### Phase 2: Maintenance and Rewriters

1. Port Arcana.Maintenance functions
2. Implement query rewriters using existing LLM adapters
3. Add production maintenance documentation

### Phase 3: Parser and Ingestion

1. Port file parser with PDF support
2. Create unified ingestion API
3. Integrate status tracking with Broadway pipelines

### Phase 4: Search Enhancements

1. Add search mode option to base API
2. Implement collection-filtered search
3. Enhance telemetry coverage

---

## Recommendations

1. **Prioritize Document/Chunk Schemas**: These provide the foundation for maintenance, status tracking, and better data integrity.

2. **Add Query Rewriters Early**: Low implementation cost with significant retrieval quality improvements.

3. **Keep Broadway Pipelines**: Portfolio's async pipeline approach is better for large-scale operations; add simple API alongside, not instead of.

4. **Consider Hybrid Approach**: Use Ecto schemas for document/chunk management while keeping adapter pattern for embedder/vector store flexibility.

5. **Maintain Compatibility**: New features should be additive, not breaking changes to existing API.
</file>

</files>
